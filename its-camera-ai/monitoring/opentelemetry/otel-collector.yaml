# OpenTelemetry Collector Configuration for ITS Camera AI
# Provides distributed tracing, metrics collection, and log correlation
# Supports high-throughput processing for 1000+ camera streams

receivers:
  # OTLP receiver for gRPC and HTTP
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
        max_recv_msg_size: 67108864  # 64MB
        max_concurrent_streams: 16
      http:
        endpoint: 0.0.0.0:4318
        cors:
          allowed_origins:
            - "http://localhost:3000"
            - "https://grafana.its-camera-ai.com"
          allowed_headers:
            - "*"

  # Jaeger receiver for legacy support
  jaeger:
    protocols:
      grpc:
        endpoint: 0.0.0.0:14250
      thrift_http:
        endpoint: 0.0.0.0:14268
      thrift_compact:
        endpoint: 0.0.0.0:6831
      thrift_binary:
        endpoint: 0.0.0.0:6832

  # Zipkin receiver
  zipkin:
    endpoint: 0.0.0.0:9411

  # Prometheus metrics receiver
  prometheus:
    config:
      global:
        scrape_interval: 30s
      scrape_configs:
        - job_name: 'otel-collector'
          static_configs:
            - targets: ['localhost:8888']

  # Host metrics receiver
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      disk:
      filesystem:
      load:
      memory:
      network:
      paging:
      processes:

  # Application-specific receivers
  filelog:
    include:
      - /var/log/its-camera-ai/*.log
      - /var/log/containers/*its-camera-ai*.log
    exclude:
      - /var/log/its-camera-ai/*debug*.log
    start_at: end
    include_file_path: true
    include_file_name: false
    operators:
      - type: json_parser
        id: parser-json
        output: extract_timestamp_parser
        timestamp:
          parse_from: attributes.timestamp
          layout: '%Y-%m-%d %H:%M:%S'
      - type: move
        from: attributes.message
        to: body
      - type: move
        from: attributes.severity
        to: severity_text
      - type: add
        field: attributes.service_name
        value: "its-camera-ai"

processors:
  # Batch processor for performance
  batch:
    timeout: 1s
    send_batch_size: 1024
    send_batch_max_size: 2048

  # Memory limiter to prevent OOM
  memory_limiter:
    limit_mib: 2048
    spike_limit_mib: 512
    check_interval: 5s

  # Resource processor to add service information
  resource:
    attributes:
      - key: service.name
        value: "its-camera-ai"
        action: upsert
      - key: service.version
        value: "2.0.0"
        action: upsert
      - key: deployment.environment
        value: "production"
        action: upsert
      - key: cluster.name
        value: "its-camera-ai-prod"
        action: upsert

  # Span processor for trace enhancement
  span:
    name:
      to_attributes:
        rules:
          - pattern: '^/api/v1/(.*)$'
            name_template: 'api_v1_$1'
      from_attributes:
        - db.statement

  # Attributes processor for data enrichment
  attributes:
    actions:
      - key: http.user_agent
        action: delete
      - key: camera_id
        action: upsert
        from_attribute: camera.id
      - key: model_version
        action: upsert
        from_attribute: ml.model.version
      - key: business_impact
        action: upsert
        value: "high"
        condition: 'attributes["service.name"] == "vision-engine"'

  # Probabilistic sampler for high-volume traces
  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 1.0  # Sample 1% of traces in production

  # Tail sampling for intelligent sampling
  tail_sampling:
    decision_wait: 30s
    num_traces: 50000
    expected_new_traces_per_sec: 100
    policies:
      # Always sample error traces
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      # Always sample slow traces
      - name: slow_requests
        type: latency
        latency:
          threshold_ms: 1000
      # Always sample traces with specific business context
      - name: business_critical
        type: string_attribute
        string_attribute:
          key: business_impact
          values: [critical, high]
      # Sample a percentage of normal traces
      - name: randomized
        type: probabilistic
        probabilistic:
          sampling_percentage: 0.1

  # Filter processor to reduce noise
  filter:
    traces:
      span:
        - 'attributes["http.route"] == "/health"'
        - 'attributes["http.route"] == "/metrics"'
        - 'attributes["http.route"] == "/ready"'
    metrics:
      metric:
        - 'name == "runtime.go.gc.duration"'

  # Transform processor for custom metrics
  transform:
    metric_statements:
      - context: metric
        statements:
          # Add custom business metrics
          - set(attributes["business_critical"], "true") where name == "inference_duration_seconds"
          - set(attributes["sla_tier"], "tier1") where name == "http_request_duration_seconds" and attributes["service.name"] == "vision-engine"

exporters:
  # Jaeger exporter for distributed tracing
  jaeger:
    endpoint: jaeger-collector:14250
    tls:
      insecure: true
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

  # Prometheus metrics exporter
  prometheus:
    endpoint: "0.0.0.0:8889"
    send_timestamps: true
    metric_expiration: 180m
    enable_open_metrics: true
    resource_to_telemetry_conversion:
      enabled: true

  # OTLP exporter for metrics (to Prometheus)
  otlp/prometheus:
    endpoint: http://prometheus:9090/api/v1/otlp
    tls:
      insecure: true
    headers:
      "X-Prometheus-Remote-Write-Version": "0.1.0"

  # Loki exporter for logs
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
    tenant_id: "its-camera-ai"
    labels:
      attributes:
        service.name: "service_name"
        severity_text: "level"
        container.name: "container"
      resource:
        service.name: "service_name"
        service.version: "service_version"
    format: json

  # File exporter for debugging
  file:
    path: /tmp/otel-traces.json
    rotation:
      max_megabytes: 100
      max_days: 7
      max_backups: 10

  # Logging exporter for debugging
  logging:
    loglevel: info
    sampling_initial: 5
    sampling_thereafter: 200

  # Custom webhook exporter for alerts
  webhook:
    endpoint: http://alertmanager:9093/api/v1/alerts
    headers:
      "Content-Type": "application/json"
      "Authorization": "Bearer ${ALERTMANAGER_TOKEN}"
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s

extensions:
  # Health check extension
  health_check:
    endpoint: 0.0.0.0:13133
    path: "/health"

  # Performance profiler extension
  pprof:
    endpoint: 0.0.0.0:1777

  # Memory ballast extension for stability
  memory_ballast:
    size_mib: 512

  # Kubernetes observer
  k8s_observer:
    auth_type: serviceAccount
    node: ${env:K8S_NODE_NAME}
    observe_pods: true
    observe_nodes: true

service:
  extensions: [health_check, pprof, memory_ballast, k8s_observer]
  
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp, jaeger, zipkin]
      processors: [
        memory_limiter,
        resource,
        span,
        attributes,
        tail_sampling,
        filter,
        batch
      ]
      exporters: [jaeger, logging]

    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus, hostmetrics]
      processors: [
        memory_limiter,
        resource,
        attributes,
        transform,
        filter,
        batch
      ]
      exporters: [prometheus, otlp/prometheus]

    # Logs pipeline
    logs:
      receivers: [otlp, filelog]
      processors: [
        memory_limiter,
        resource,
        attributes,
        batch
      ]
      exporters: [loki, logging]

  telemetry:
    logs:
      level: "info"
      development: false
      encoding: "json"
      disable_caller: false
      disable_stacktrace: false
      output_paths: ["stdout"]
      error_output_paths: ["stderr"]
    
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
    traces:
      processors: [batch]

# Environment-specific configuration
environments:
  production:
    processors:
      probabilistic_sampler:
        sampling_percentage: 0.1  # Reduce sampling in production
      tail_sampling:
        policies:
          - name: randomized
            type: probabilistic
            probabilistic:
              sampling_percentage: 0.01  # Very low sampling for normal traces
    
  staging:
    processors:
      probabilistic_sampler:
        sampling_percentage: 1.0  # Sample more in staging
        
  development:
    processors:
      probabilistic_sampler:
        sampling_percentage: 10.0  # Sample everything in development
    exporters:
      logging:
        loglevel: debug