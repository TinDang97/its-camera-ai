# Default values for ITS Camera AI
# Cross-platform deployment with inference optimization

# Global platform detection
global:
  platform: "linux"  # linux, macos
  architecture: "amd64"  # amd64, arm64
  gpu:
    enabled: false
    vendor: "nvidia"  # nvidia, amd, intel
  registry: "ghcr.io/your-org"
  imageTag: "latest"
  pullPolicy: IfNotPresent

# Application configuration
app:
  name: its-camera-ai
  replicas: 2
  image:
    repository: its-camera-ai
    tag: latest
    pullPolicy: IfNotPresent

  # Platform-specific configurations
  platformConfig:
    linux:
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
        limits:
          cpu: "4000m"
          memory: "8Gi"
      # Linux-specific optimizations
      sysctls:
        - name: net.core.somaxconn
          value: "1024"
        - name: vm.swappiness
          value: "10"
    macos:
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "4Gi"

  env:
    - name: PLATFORM
      value: "linux"
    - name: ARCHITECTURE
      value: "amd64"
    - name: INFERENCE_ENGINE
      value: "pytorch"
    - name: LOG_LEVEL
      value: "INFO"

  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

  ingress:
    enabled: true
    className: nginx
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "100m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    hosts:
      - host: its-camera-ai.local
        paths:
          - path: /
            pathType: Prefix

# Inference services configuration with gRPC optimization
inference:
  # gRPC configuration for high-performance serialization
  grpc:
    enabled: true
    port: 50051
    compression:
      enabled: true
      format: "jpeg"  # jpeg, png, webp
      quality: 85
      level: 6
    maxMessageSize: "100MB"
    keepalive:
      time: "60s"
      timeout: "10s"
      permitWithoutCalls: true
    
  # Triton Inference Server (Linux/NVIDIA)
  triton:
    enabled: false
    replicas: 1
    image:
      repository: nvcr.io/nvidia/tritonserver
      tag: "24.02-py3"
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
        nvidia.com/gpu: 1
      limits:
        cpu: "8000m"
        memory: "16Gi"
        nvidia.com/gpu: 1
    models:
      repository: "/models"
      storageClass: "fast-ssd"
      size: "50Gi"
    service:
      http: 8000
      grpc: 8001
      metrics: 8002

  # BentoML Service (Cross-platform) with Redis Streams integration
  bentoml:
    enabled: true
    replicas: 2
    # Redis Streams integration
    redisStreams:
      enabled: true
      inputQueue: "camera_frames"
      outputQueue: "processed_frames"
      batchSize: 20
      consumerGroup: "ml_inference"
    image:
      repository: its-camera-ai-bentoml
      tag: latest
    resources:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
    service:
      port: 3000

    # Platform-specific optimizations
    platformOptimizations:
      linux:
        workers: 4
        backend: "pytorch"
      macos:
        workers: 2
        backend: "coreml"

  # Horizontal Pod Autoscaler with Redis Streams metrics
  hpa:
    enabled: true
    minReplicas: 2
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    # Custom metrics for inference workloads
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: inference_queue_length
          target:
            type: AverageValue
            averageValue: "10"
      # Redis Streams metrics
      - type: External
        external:
          metric:
            name: redis_stream_length
            selector:
              matchLabels:
                stream_name: "camera_frames"
          target:
            type: AverageValue
            averageValue: "100"

# Model management
models:
  storage:
    enabled: true
    storageClass: "standard"
    size: "100Gi"
    accessMode: ReadWriteMany

  # Model optimization per platform
  optimization:
    linux:
      formats: ["pytorch", "onnx", "tensorrt"]
      tensorrt:
        enabled: true
        precision: "fp16"
    macos:
      formats: ["pytorch", "onnx", "coreml"]
      coreml:
        enabled: true
        computeUnits: "cpuAndGPU"

# Monitoring and observability
monitoring:
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s

  grafana:
    enabled: true
    dashboards:
      inference: true
      platform: true

  # Platform-specific metrics
  platformMetrics:
    linux:
      gpu: true
      numa: true
    macos:
      metal: true
      thermal: true

# Database dependencies
postgresql:
  enabled: true
  auth:
    username: its_user
    password: its_password
    database: its_camera_ai
  primary:
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"

redis:
  enabled: true
  auth:
    enabled: true
    password: redis_password
  master:
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
  # Cache configuration (separate from streams)
  cache:
    databases:
      inference_cache: 0
      session_cache: 1
      analytics_cache: 2
      config_cache: 3

# Kafka removed - replaced with Redis Streams
kafka:
  enabled: false
  
# Redis Streams for high-performance queue processing
redisStreams:
  enabled: true
  replicas: 3
  image:
    repository: redis
    tag: "7.2.0-alpine"
  resources:
    requests:
      memory: "8Gi"
      cpu: "2"
    limits:
      memory: "16Gi"
      cpu: "4"
  persistence:
    enabled: true
    storageClass: "fast-ssd"
    size: "200Gi"
  auth:
    enabled: true
    password: "ItsCarneraAIRedisPassword2024"
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: "10s"
  # Queue-specific configuration
  queueConfig:
    maxLength: 50000
    consumerGroups:
      - name: "stream_processor"
        consumers: 10
      - name: "ml_inference"
        consumers: 6
      - name: "output_consumers"
        consumers: 5
    streams:
      - name: "camera_frames"
        maxLength: 10000
      - name: "processed_frames"
        maxLength: 50000
      - name: "inference_queue"
        maxLength: 20000

# Security configuration
security:
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  networkPolicy:
    enabled: true

# Platform-specific node selection with workload types
nodeSelector:
  enabled: true
  linux:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  macos:
    kubernetes.io/os: darwin
    kubernetes.io/arch: amd64
  # Workload-specific node selection
  workloadTypes:
    streaming:
      workload-type: streaming
    ml:
      workload-type: ml-inference
    database:
      workload-type: database

# Tolerations for specialized nodes
tolerations:
  gpu:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  streaming:
    - key: workload-type
      operator: Equal
      value: streaming
      effect: NoSchedule
  database:
    - key: workload-type
      operator: Equal
      value: database
      effect: NoSchedule