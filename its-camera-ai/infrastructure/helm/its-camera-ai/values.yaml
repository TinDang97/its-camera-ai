# Default values for ITS Camera AI
# Cross-platform deployment with inference optimization

# Global platform detection
global:
  platform: "linux"  # linux, macos
  architecture: "amd64"  # amd64, arm64
  gpu:
    enabled: false
    vendor: "nvidia"  # nvidia, amd, intel
  registry: "ghcr.io/your-org"
  imageTag: "latest"
  pullPolicy: IfNotPresent

# Application configuration
app:
  name: its-camera-ai
  replicas: 2
  image:
    repository: its-camera-ai
    tag: latest
    pullPolicy: IfNotPresent

  # Platform-specific configurations
  platformConfig:
    linux:
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
        limits:
          cpu: "4000m"
          memory: "8Gi"
      # Linux-specific optimizations
      sysctls:
        - name: net.core.somaxconn
          value: "1024"
        - name: vm.swappiness
          value: "10"
    macos:
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "4Gi"

  env:
    - name: PLATFORM
      value: "linux"
    - name: ARCHITECTURE
      value: "amd64"
    - name: INFERENCE_ENGINE
      value: "pytorch"
    - name: LOG_LEVEL
      value: "INFO"

  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

  ingress:
    enabled: true
    className: nginx
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "100m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    hosts:
      - host: its-camera-ai.local
        paths:
          - path: /
            pathType: Prefix

# Inference services configuration
inference:
  # Triton Inference Server (Linux/NVIDIA)
  triton:
    enabled: false
    replicas: 1
    image:
      repository: nvcr.io/nvidia/tritonserver
      tag: "24.02-py3"
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
        nvidia.com/gpu: 1
      limits:
        cpu: "8000m"
        memory: "16Gi"
        nvidia.com/gpu: 1
    models:
      repository: "/models"
      storageClass: "fast-ssd"
      size: "50Gi"
    service:
      http: 8000
      grpc: 8001
      metrics: 8002

  # BentoML Service (Cross-platform)
  bentoml:
    enabled: true
    replicas: 2
    image:
      repository: its-camera-ai-bentoml
      tag: latest
    resources:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
    service:
      port: 3000

    # Platform-specific optimizations
    platformOptimizations:
      linux:
        workers: 4
        backend: "pytorch"
      macos:
        workers: 2
        backend: "coreml"

  # Horizontal Pod Autoscaler
  hpa:
    enabled: true
    minReplicas: 2
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    # Custom metrics for inference workloads
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: inference_queue_length
          target:
            type: AverageValue
            averageValue: "10"

# Model management
models:
  storage:
    enabled: true
    storageClass: "standard"
    size: "100Gi"
    accessMode: ReadWriteMany

  # Model optimization per platform
  optimization:
    linux:
      formats: ["pytorch", "onnx", "tensorrt"]
      tensorrt:
        enabled: true
        precision: "fp16"
    macos:
      formats: ["pytorch", "onnx", "coreml"]
      coreml:
        enabled: true
        computeUnits: "cpuAndGPU"

# Monitoring and observability
monitoring:
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s

  grafana:
    enabled: true
    dashboards:
      inference: true
      platform: true

  # Platform-specific metrics
  platformMetrics:
    linux:
      gpu: true
      numa: true
    macos:
      metal: true
      thermal: true

# Database dependencies
postgresql:
  enabled: true
  auth:
    username: its_user
    password: its_password
    database: its_camera_ai
  primary:
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"

redis:
  enabled: true
  auth:
    enabled: true
    password: redis_password
  master:
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"

kafka:
  enabled: true
  replicaCount: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# Security configuration
security:
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  networkPolicy:
    enabled: true

# Platform-specific node selection
nodeSelector:
  enabled: true
  linux:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  macos:
    kubernetes.io/os: darwin
    kubernetes.io/arch: amd64

# Tolerations for GPU nodes
tolerations:
  gpu:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule