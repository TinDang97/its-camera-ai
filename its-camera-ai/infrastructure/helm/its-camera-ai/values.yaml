# Default values for ITS Camera AI
# Cross-platform deployment with inference optimization

# Global platform detection
global:
  platform: "linux"  # linux, macos
  architecture: "amd64"  # amd64, arm64
  gpu:
    enabled: false
    vendor: "nvidia"  # nvidia, amd, intel
  registry: "ghcr.io/your-org"
  imageTag: "latest"
  pullPolicy: IfNotPresent

# Application configuration
app:
  name: its-camera-ai
  replicas: 2
  image:
    repository: its-camera-ai
    tag: latest
    pullPolicy: IfNotPresent

  # Platform-specific configurations
  platformConfig:
    linux:
      resources:
        requests:
          cpu: "1000m"
          memory: "2Gi"
        limits:
          cpu: "4000m"
          memory: "8Gi"
      # Linux-specific optimizations
      sysctls:
        - name: net.core.somaxconn
          value: "1024"
        - name: vm.swappiness
          value: "10"
    macos:
      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "4Gi"

  env:
    - name: PLATFORM
      value: "linux"
    - name: ARCHITECTURE
      value: "amd64"
    - name: INFERENCE_ENGINE
      value: "pytorch"
    - name: LOG_LEVEL
      value: "INFO"

  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000

  ingress:
    enabled: true
    className: nginx
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "100m"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    hosts:
      - host: its-camera-ai.local
        paths:
          - path: /
            pathType: Prefix

# Inference services configuration with gRPC optimization
inference:
  # gRPC configuration for high-performance serialization
  grpc:
    enabled: true
    port: 50051
    compression:
      enabled: true
      format: "jpeg"  # jpeg, png, webp
      quality: 85
      level: 6
    maxMessageSize: "100MB"
    keepalive:
      time: "60s"
      timeout: "10s"
      permitWithoutCalls: true
    
  # Triton Inference Server (Linux/NVIDIA)
  triton:
    enabled: false
    replicas: 1
    image:
      repository: nvcr.io/nvidia/tritonserver
      tag: "24.02-py3"
    resources:
      requests:
        cpu: "2000m"
        memory: "4Gi"
        nvidia.com/gpu: 1
      limits:
        cpu: "8000m"
        memory: "16Gi"
        nvidia.com/gpu: 1
    models:
      repository: "/models"
      storageClass: "fast-ssd"
      size: "50Gi"
    service:
      http: 8000
      grpc: 8001
      metrics: 8002

  # BentoML Service (Cross-platform) with Redis Streams integration
  bentoml:
    enabled: true
    replicas: 2
    # Redis Streams integration
    redisStreams:
      enabled: true
      inputQueue: "camera_frames"
      outputQueue: "processed_frames"
      batchSize: 20
      consumerGroup: "ml_inference"
    image:
      repository: its-camera-ai-bentoml
      tag: latest
    resources:
      requests:
        cpu: "1000m"
        memory: "2Gi"
      limits:
        cpu: "4000m"
        memory: "8Gi"
    service:
      port: 3000

    # Platform-specific optimizations
    platformOptimizations:
      linux:
        workers: 4
        backend: "pytorch"
      macos:
        workers: 2
        backend: "coreml"

  # Horizontal Pod Autoscaler with Redis Streams metrics
  hpa:
    enabled: true
    minReplicas: 2
    maxReplicas: 20
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    # Custom metrics for inference workloads
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: inference_queue_length
          target:
            type: AverageValue
            averageValue: "10"
      # Redis Streams metrics
      - type: External
        external:
          metric:
            name: redis_stream_length
            selector:
              matchLabels:
                stream_name: "camera_frames"
          target:
            type: AverageValue
            averageValue: "100"

# Model management
models:
  storage:
    enabled: true
    storageClass: "standard"
    size: "100Gi"
    accessMode: ReadWriteMany

  # Model optimization per platform
  optimization:
    linux:
      formats: ["pytorch", "onnx", "tensorrt"]
      tensorrt:
        enabled: true
        precision: "fp16"
    macos:
      formats: ["pytorch", "onnx", "coreml"]
      coreml:
        enabled: true
        computeUnits: "cpuAndGPU"

# Monitoring and observability
monitoring:
  prometheus:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 10s

  grafana:
    enabled: true
    dashboards:
      inference: true
      platform: true

  # Platform-specific metrics
  platformMetrics:
    linux:
      gpu: true
      numa: true
    macos:
      metal: true
      thermal: true

# Database dependencies
postgresql:
  enabled: true
  auth:
    username: its_user
    password: its_password
    database: its_camera_ai
  primary:
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"

redis:
  enabled: true
  auth:
    enabled: true
    password: redis_password
  master:
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1Gi"
        cpu: "500m"
  # Cache configuration (separate from streams)
  cache:
    databases:
      inference_cache: 0
      session_cache: 1
      analytics_cache: 2
      config_cache: 3

# Kafka removed - replaced with Redis Streams
kafka:
  enabled: false

# MinIO Object Storage Configuration
minio:
  enabled: true
  
  # Deployment mode
  mode: distributed
  
  # Authentication
  auth:
    rootUser: "its_camera_ai_admin"
    rootPassword: "ItsCAI2024SecureMinIOPass!"
    
  # Distributed mode configuration
  statefulset:
    replicaCount: 4
    zones: 1
    drivesPerNode: 4
    
  # Resource allocation
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
      
  # Persistence
  persistence:
    enabled: true
    storageClass: "fast-ssd"
    size: "500Gi"
    
  # Service configuration
  service:
    type: LoadBalancer
    ports:
      api: 9000
      console: 9001
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      
  # MinIO Console (Web UI)
  console:
    enabled: true
    ingress:
      enabled: true
      hostname: "minio-console.its-camera-ai.local"
      annotations:
        nginx.ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
        nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
        
  # API Ingress
  ingress:
    enabled: true
    hostname: "minio-api.its-camera-ai.local"
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "Access-Control-Allow-Origin: *";
        more_set_headers "Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS";
        more_set_headers "Access-Control-Allow-Headers: authorization, content-type";
        
  # Security
  networkPolicy:
    enabled: true
    allowExternal: true
    ingressRules:
      primaryAccessOnlyFrom:
        enabled: false
      readAccessOnlyFrom:
        enabled: false
        
  # Metrics and monitoring
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 30s
      scrapeTimeout: 15s
      
  # Default buckets to create
  defaultBuckets: "camera-streams,ml-models,analytics,backups,temp"
  
  # Bucket lifecycle policies
  bucketLifecycle:
    enabled: true
    policies:
      temp:
        expirationDays: 7
      camera-streams:
        expirationDays: 30
      analytics:
        expirationDays: 90
        
  # Performance optimizations
  extraEnvVars:
    - name: MINIO_STORAGE_CLASS_STANDARD
      value: "EC:2"
    - name: MINIO_CACHE_QUOTA
      value: "80"
    - name: MINIO_CACHE_AFTER
      value: "3"
    - name: MINIO_CACHE_WATERMARK_LOW
      value: "70"
    - name: MINIO_CACHE_WATERMARK_HIGH
      value: "90"
    - name: MINIO_API_REQUESTS_MAX
      value: "10000"
    - name: MINIO_API_REQUESTS_DEADLINE
      value: "10s"
    - name: MINIO_AUDIT_WEBHOOK_ENABLE_target1
      value: "on"
    - name: MINIO_AUDIT_WEBHOOK_ENDPOINT_target1
      value: "http://api-service:8000/api/v1/audit/minio"
      
  # Pod Disruption Budget
  pdb:
    create: true
    minAvailable: 2
    
  # Horizontal Pod Autoscaler
  autoscaling:
    enabled: true
    minReplicas: 4
    maxReplicas: 12
    targetCPU: 70
    targetMemory: 80
    
  # Affinity and tolerations
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: workload-type
            operator: In
            values:
            - storage
          - key: storage-type
            operator: In
            values:
            - ssd
            - nvme
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: minio
        topologyKey: kubernetes.io/hostname
        
  tolerations:
  - key: workload-type
    operator: Equal
    value: storage
    effect: NoSchedule
  - key: storage-tier
    operator: Equal
    value: high-performance
    effect: NoSchedule
  
# Redis Streams for high-performance queue processing
redisStreams:
  enabled: true
  replicas: 3
  image:
    repository: redis
    tag: "7.2.0-alpine"
  resources:
    requests:
      memory: "8Gi"
      cpu: "2"
    limits:
      memory: "16Gi"
      cpu: "4"
  persistence:
    enabled: true
    storageClass: "fast-ssd"
    size: "200Gi"
  auth:
    enabled: true
    password: "ItsCarneraAIRedisPassword2024"
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: "10s"
  # Queue-specific configuration
  queueConfig:
    maxLength: 50000
    consumerGroups:
      - name: "stream_processor"
        consumers: 10
      - name: "ml_inference"
        consumers: 6
      - name: "output_consumers"
        consumers: 5
    streams:
      - name: "camera_frames"
        maxLength: 10000
      - name: "processed_frames"
        maxLength: 50000
      - name: "inference_queue"
        maxLength: 20000

# Security configuration
security:
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  networkPolicy:
    enabled: true

# Platform-specific node selection with workload types
nodeSelector:
  enabled: true
  linux:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  macos:
    kubernetes.io/os: darwin
    kubernetes.io/arch: amd64
  # Workload-specific node selection
  workloadTypes:
    streaming:
      workload-type: streaming
    ml:
      workload-type: ml-inference
    database:
      workload-type: database
    storage:
      workload-type: storage
      storage-type: ssd

# Tolerations for specialized nodes
tolerations:
  gpu:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
  streaming:
    - key: workload-type
      operator: Equal
      value: streaming
      effect: NoSchedule
  database:
    - key: workload-type
      operator: Equal
      value: database
      effect: NoSchedule