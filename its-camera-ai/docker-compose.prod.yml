# Production Docker Compose configuration for ITS Camera AI
# High-availability production deployment with clustering, load balancing, and monitoring
# Usage: docker-compose -f docker-compose.prod.yml up

version: '3.8'

services:
  # =============================================================================
  # Load Balancer - Nginx reverse proxy with SSL termination
  # =============================================================================
  nginx:
    image: nginx:1.25-alpine
    container_name: its-camera-ai-nginx
    ports:
      - "80:80"
      - "443:443"
    environment:
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=1024
      - NGINX_KEEPALIVE_TIMEOUT=65
      - NGINX_CLIENT_MAX_BODY_SIZE=50m
    volumes:
      - ./infrastructure/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./infrastructure/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./infrastructure/nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
      - nginx_cache:/var/cache/nginx
    depends_on:
      - api-primary
      - api-secondary
      - frontend
    networks:
      - its-network
      - external-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # =============================================================================
  # Frontend - Production Next.js Web Application
  # =============================================================================
  frontend:
    build:
      context: ./web
      dockerfile: Dockerfile
      target: production
      args:
        BUILD_DATE: "${BUILD_DATE:-$(date -u +%Y-%m-%dT%H:%M:%SZ)}"
        VCS_REF: "${VCS_REF:-$(git rev-parse --short HEAD)}"
        VERSION: "${VERSION:-1.0.0}"
    image: its-camera-ai-frontend:production
    container_name: its-camera-ai-frontend
    environment:
      # Next.js Production Environment
      - NODE_ENV=production
      - PORT=3000
      - HOSTNAME=0.0.0.0
      # API Configuration
      - NEXT_PUBLIC_API_URL=${FRONTEND_API_URL:-https://api.its-camera-ai.com}
      - NEXT_PUBLIC_WS_URL=${FRONTEND_WS_URL:-wss://api.its-camera-ai.com/ws}
      - NEXT_PUBLIC_API_BASE_URL=http://nginx:80
      # Authentication
      - NEXTAUTH_URL=${NEXTAUTH_URL:-https://its-camera-ai.com}
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-change-this-in-production}
      # External Services
      - NEXT_PUBLIC_SENTRY_DSN=${SENTRY_DSN:-}
      - NEXT_PUBLIC_ANALYTICS_ID=${ANALYTICS_ID:-}
      # Production Settings
      - NEXT_TELEMETRY_DISABLED=1
      # Security
      - SECURE_COOKIES=true
      - CSP_ENABLED=true
    volumes:
      - frontend_logs:/app/logs
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1GB
          cpus: '0.5'
        reservations:
          memory: 512MB
          cpus: '0.25'

  # =============================================================================
  # API Services - Load balanced FastAPI instances
  # =============================================================================
  api-primary:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
      args:
        BUILD_DATE: "${BUILD_DATE:-$(date -u +%Y-%m-%dT%H:%M:%SZ)}"
        VCS_REF: "${VCS_REF:-$(git rev-parse --short HEAD)}"
        VERSION: "${VERSION:-1.0.0}"
    image: its-camera-ai:production
    container_name: its-camera-ai-api-primary
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=info
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - WORKERS=4
      - RELOAD=false
      # Database configuration with connection pooling
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-primary:5432/its_camera_ai
      - DATABASE_POOL_SIZE=20
      - DATABASE_MAX_OVERFLOW=30
      - DATABASE_POOL_TIMEOUT=30
      - DATABASE_POOL_RECYCLE=3600
      # Redis configuration with clustering
      - REDIS_URL=redis://redis-primary:6379/0
      - REDIS_CACHE_URL=redis://redis-primary:6379/1
      - REDIS_SESSION_URL=redis://redis-primary:6379/2
      - REDIS_CLUSTER_ENABLED=true
      - REDIS_SENTINEL_ENABLED=true
      # Kafka configuration
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - KAFKA_TOPIC_PREFIX=its_prod
      - KAFKA_COMPRESSION_TYPE=snappy
      - KAFKA_ACKS=all
      - KAFKA_RETRIES=5
      # MinIO configuration
      - MINIO_ENDPOINT=minio-1:9000,minio-2:9000,minio-3:9000,minio-4:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_SECURE=true
      # Security configuration
      - SECRET_KEY=${SECRET_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      # Monitoring configuration
      - PROMETHEUS_METRICS_ENABLED=true
      - OPENTELEMETRY_ENABLED=true
      - OPENTELEMETRY_ENDPOINT=http://otel-collector:4317
      - SENTRY_DSN=${SENTRY_DSN}
      # Performance configuration
      - GUNICORN_WORKERS=4
      - GUNICORN_WORKER_CLASS=uvicorn.workers.UvicornWorker
      - GUNICORN_WORKER_CONNECTIONS=1000
      - GUNICORN_MAX_REQUESTS=1000
      - GUNICORN_MAX_REQUESTS_JITTER=100
      - GUNICORN_TIMEOUT=30
      - GUNICORN_KEEPALIVE=2
    volumes:
      - api_logs:/app/logs
      - api_data:/app/data
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-primary:
        condition: service_healthy
      kafka-1:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  api-secondary:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: its-camera-ai:production
    container_name: its-camera-ai-api-secondary
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=info
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - WORKERS=4
      - RELOAD=false
      # Same configuration as primary but with backup endpoints
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-replica:5432/its_camera_ai
      - REDIS_URL=redis://redis-replica:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - MINIO_ENDPOINT=minio-1:9000,minio-2:9000,minio-3:9000,minio-4:9000
      - SECRET_KEY=${SECRET_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - PROMETHEUS_METRICS_ENABLED=true
      - OPENTELEMETRY_ENABLED=true
      - OPENTELEMETRY_ENDPOINT=http://otel-collector:4317
    volumes:
      - api_logs:/app/logs
      - api_data:/app/data
    depends_on:
      postgres-replica:
        condition: service_healthy
      redis-replica:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # =============================================================================
  # ML Inference Services - GPU-enabled with clustering
  # =============================================================================
  ml-inference-primary:
    build:
      context: .
      dockerfile: Dockerfile
      target: gpu-production
      args:
        BUILD_DATE: "${BUILD_DATE:-$(date -u +%Y-%m-%dT%H:%M:%SZ)}"
        VCS_REF: "${VCS_REF:-$(git rev-parse --short HEAD)}"
        VERSION: "${VERSION:-1.0.0}"
    image: its-camera-ai:gpu-production
    container_name: its-camera-ai-ml-primary
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - ENVIRONMENT=production
      - SERVICE_TYPE=ml_inference
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - ML_WORKERS=4
      - ML_BATCH_SIZE=16
      - ML_MAX_BATCH_DELAY=10
      - ML_DEVICE=cuda
      - ML_PRECISION=fp16
      - ML_TENSORRT_ENABLED=true
      - ML_MEMORY_FRACTION=0.9
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-primary:5432/its_camera_ai
      - REDIS_URL=redis://redis-primary:6379/5
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - PROMETHEUS_METRICS_ENABLED=true
    volumes:
      - ml_models:/app/models:ro
      - ml_inference_logs:/app/logs
      - tensorrt_cache:/app/.tensorrt_cache
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import grpc; grpc.insecure_channel('localhost:50051').get_state()"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s

  ml-inference-secondary:
    build:
      context: .
      dockerfile: Dockerfile
      target: gpu-production
    image: its-camera-ai:gpu-production
    container_name: its-camera-ai-ml-secondary
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=1
      - ENVIRONMENT=production
      - SERVICE_TYPE=ml_inference
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50051
      - ML_WORKERS=4
      - ML_DEVICE=cuda
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-replica:5432/its_camera_ai
      - REDIS_URL=redis://redis-replica:6379/5
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
    volumes:
      - ml_models:/app/models:ro
      - ml_inference_logs:/app/logs
      - tensorrt_cache:/app/.tensorrt_cache
    depends_on:
      postgres-replica:
        condition: service_healthy
      redis-replica:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped

  # =============================================================================
  # Stream Processing Services - Camera processing cluster
  # =============================================================================
  stream-processor-primary:
    build:
      context: .
      dockerfile: Dockerfile
      target: stream-processor-production
    image: its-camera-ai:stream-processor-production
    container_name: its-camera-ai-stream-primary
    environment:
      - ENVIRONMENT=production
      - SERVICE_TYPE=stream_processor
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50052
      - STREAM_WORKERS=8
      - MAX_CONCURRENT_STREAMS=200
      - STREAM_BUFFER_SIZE=60
      - QUALITY_CHECK_ENABLED=true
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-primary:5432/its_camera_ai
      - REDIS_URL=redis://redis-primary:6379/6
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - ML_INFERENCE_ENDPOINTS=ml-inference-primary:50051,ml-inference-secondary:50051
    volumes:
      - stream_logs:/app/logs
      - stream_data:/app/data
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-primary:
        condition: service_healthy
      ml-inference-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 8G
        reservations:
          cpus: '1.5'
          memory: 4G

  stream-processor-secondary:
    build:
      context: .
      dockerfile: Dockerfile
      target: stream-processor-production
    image: its-camera-ai:stream-processor-production
    container_name: its-camera-ai-stream-secondary
    environment:
      - ENVIRONMENT=production
      - SERVICE_TYPE=stream_processor
      - GRPC_HOST=0.0.0.0
      - GRPC_PORT=50052
      - STREAM_WORKERS=8
      - MAX_CONCURRENT_STREAMS=200
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-replica:5432/its_camera_ai
      - REDIS_URL=redis://redis-replica:6379/6
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
      - ML_INFERENCE_ENDPOINTS=ml-inference-primary:50051,ml-inference-secondary:50051
    volumes:
      - stream_logs:/app/logs
      - stream_data:/app/data
    depends_on:
      postgres-replica:
        condition: service_healthy
      redis-replica:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '3.0'
          memory: 8G
        reservations:
          cpus: '1.5'
          memory: 4G

  # =============================================================================
  # Background Workers - Clustered for high availability
  # =============================================================================
  worker-analytics-cluster:
    build:
      context: .
      dockerfile: Dockerfile
      target: worker
    image: its-camera-ai:worker
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    environment:
      - ENVIRONMENT=production
      - WORKER_TYPE=analytics
      - WORKER_CONCURRENCY=4
      - WORKER_QUEUE=analytics
      - CELERY_BROKER_URL=redis://redis-primary:6379/3
      - CELERY_RESULT_BACKEND=redis://redis-primary:6379/4
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-primary:5432/its_camera_ai
      - REDIS_URL=redis://redis-primary:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092
    volumes:
      - worker_logs:/app/logs
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped

  worker-aggregation-cluster:
    build:
      context: .
      dockerfile: Dockerfile
      target: worker
    image: its-camera-ai:worker
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    environment:
      - ENVIRONMENT=production
      - WORKER_TYPE=aggregation
      - WORKER_CONCURRENCY=2
      - WORKER_QUEUE=aggregation
      - CELERY_BROKER_URL=redis://redis-primary:6379/3
      - CELERY_RESULT_BACKEND=redis://redis-primary:6379/4
      - DATABASE_URL=postgresql+asyncpg://its_user:${POSTGRES_PASSWORD}@postgres-primary:5432/its_camera_ai
      - REDIS_URL=redis://redis-primary:6379/0
    volumes:
      - worker_logs:/app/logs
    depends_on:
      postgres-primary:
        condition: service_healthy
      redis-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped

  # =============================================================================
  # Database Cluster - PostgreSQL with replication
  # =============================================================================
  postgres-primary:
    image: timescale/timescaledb:2.13-pg15
    container_name: its-camera-ai-postgres-primary
    environment:
      - POSTGRES_DB=its_camera_ai
      - POSTGRES_USER=its_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - TIMESCALEDB_TELEMETRY=off
      # Production performance tuning
      - POSTGRES_SHARED_BUFFERS=2GB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=6GB
      - POSTGRES_WORK_MEM=64MB
      - POSTGRES_MAINTENANCE_WORK_MEM=512MB
      - POSTGRES_WAL_BUFFERS=64MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_RANDOM_PAGE_COST=1.1
      - POSTGRES_MAX_CONNECTIONS=1000
      - POSTGRES_MAX_WAL_SIZE=4GB
      - POSTGRES_MIN_WAL_SIZE=1GB
      # Replication configuration
      - POSTGRES_ARCHIVE_MODE=on
      - POSTGRES_ARCHIVE_COMMAND='test ! -f /var/lib/postgresql/archive/%f && cp %p /var/lib/postgresql/archive/%f'
      - POSTGRES_WAL_LEVEL=replica
      - POSTGRES_MAX_WAL_SENDERS=3
      - POSTGRES_WAL_KEEP_SEGMENTS=32
    ports:
      - "5432:5432"
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - postgres_archive:/var/lib/postgresql/archive
      - ./database/timescaledb_setup.sql:/docker-entrypoint-initdb.d/01-timescaledb-setup.sql:ro
      - ./database/timescaledb_continuous_aggregates.sql:/docker-entrypoint-initdb.d/02-continuous-aggregates.sql:ro
      - ./infrastructure/database/postgresql-primary.conf:/etc/postgresql/postgresql.conf:ro
      - ./infrastructure/database/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U its_user -d its_camera_ai"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G

  postgres-replica:
    image: timescale/timescaledb:2.13-pg15
    container_name: its-camera-ai-postgres-replica
    environment:
      - POSTGRES_DB=its_camera_ai
      - POSTGRES_USER=its_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - POSTGRES_PRIMARY_HOST=postgres-primary
      - POSTGRES_PRIMARY_PORT=5432
      - TIMESCALEDB_TELEMETRY=off
      # Read replica configuration
      - POSTGRES_HOT_STANDBY=on
      - POSTGRES_MAX_STANDBY_STREAMING_DELAY=30s
      - POSTGRES_WAL_RECEIVER_STATUS_INTERVAL=10s
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
      - ./infrastructure/database/postgresql-replica.conf:/etc/postgresql/postgresql.conf:ro
      - ./infrastructure/database/recovery.conf:/var/lib/postgresql/data/recovery.conf:ro
    depends_on:
      postgres-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U its_user -d its_camera_ai"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 120s

  # =============================================================================
  # Redis Cluster - High availability caching
  # =============================================================================
  redis-primary:
    image: redis:7.2-alpine
    container_name: its-camera-ai-redis-primary
    command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes --save 900 1 --save 300 10 --save 60 10000
    ports:
      - "6379:6379"
    environment:
      - REDIS_REPLICATION_MODE=master
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_primary_data:/data
      - ./infrastructure/redis/redis-primary.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  redis-replica:
    image: redis:7.2-alpine
    container_name: its-camera-ai-redis-replica
    command: redis-server /usr/local/etc/redis/redis.conf --appendonly yes --replicaof redis-primary 6379
    environment:
      - REDIS_REPLICATION_MODE=slave
      - REDIS_MASTER_PASSWORD=${REDIS_PASSWORD}
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_replica_data:/data
      - ./infrastructure/redis/redis-replica.conf:/usr/local/etc/redis/redis.conf:ro
    depends_on:
      redis-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 30s

  # =============================================================================
  # Kafka Cluster - Event streaming
  # =============================================================================
  kafka-1:
    image: confluentinc/cp-kafka:7.6.1
    container_name: its-camera-ai-kafka-1
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
      KAFKA_NUM_PARTITIONS: 12
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      # Production performance tuning
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_COMPRESSION_TYPE: snappy
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 16
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
    volumes:
      - kafka_1_data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  kafka-2:
    image: confluentinc/cp-kafka:7.6.1
    container_name: its-camera-ai-kafka-2
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
    volumes:
      - kafka_2_data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - its-network
    restart: unless-stopped

  kafka-3:
    image: confluentinc/cp-kafka:7.6.1
    container_name: its-camera-ai-kafka-3
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:29094
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29094
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
    volumes:
      - kafka_3_data:/var/lib/kafka/data
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - its-network
    restart: unless-stopped

  # Zookeeper cluster for Kafka
  zookeeper-1:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: its-camera-ai-zookeeper-1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    volumes:
      - zookeeper_1_data:/var/lib/zookeeper/data
      - zookeeper_1_logs:/var/lib/zookeeper/log
    networks:
      - its-network
    restart: unless-stopped

  zookeeper-2:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: its-camera-ai-zookeeper-2
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    volumes:
      - zookeeper_2_data:/var/lib/zookeeper/data
      - zookeeper_2_logs:/var/lib/zookeeper/log
    networks:
      - its-network
    restart: unless-stopped

  zookeeper-3:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: its-camera-ai-zookeeper-3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    volumes:
      - zookeeper_3_data:/var/lib/zookeeper/data
      - zookeeper_3_logs:/var/lib/zookeeper/log
    networks:
      - its-network
    restart: unless-stopped

  # =============================================================================
  # MinIO Cluster - Distributed object storage
  # =============================================================================
  minio-1:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: its-camera-ai-minio-1
    command: server http://minio-{1...4}:9000/data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
      MINIO_REGION_NAME: us-east-1
    volumes:
      - minio_1_data:/data
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 30s

  minio-2:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: its-camera-ai-minio-2
    command: server http://minio-{1...4}:9000/data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
      MINIO_REGION_NAME: us-east-1
    volumes:
      - minio_2_data:/data
    networks:
      - its-network
    restart: unless-stopped

  minio-3:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: its-camera-ai-minio-3
    command: server http://minio-{1...4}:9000/data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
      MINIO_REGION_NAME: us-east-1
    volumes:
      - minio_3_data:/data
    networks:
      - its-network
    restart: unless-stopped

  minio-4:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: its-camera-ai-minio-4
    command: server http://minio-{1...4}:9000/data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
      MINIO_REGION_NAME: us-east-1
    volumes:
      - minio_4_data:/data
    networks:
      - its-network
    restart: unless-stopped

  # =============================================================================
  # Monitoring Stack - Production observability
  # =============================================================================
  prometheus:
    image: prom/prometheus:v2.48.1
    container_name: its-camera-ai-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=50GB'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.external-url=https://monitoring.its-camera-ai.com/prometheus'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus-production.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/ml-pipeline-rules.yaml:/etc/prometheus/ml-pipeline-rules.yaml:ro
      - ./monitoring/prometheus/business-rules.yaml:/etc/prometheus/business-rules.yaml:ro
      - prometheus_data:/prometheus
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  grafana:
    image: grafana/grafana:10.2.3
    container_name: its-camera-ai-grafana
    ports:
      - "3001:3000"  # External port changed to avoid conflict with frontend
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SECURITY_SECRET_KEY=${GRAFANA_SECRET_KEY}
      - GF_SECURITY_ALLOW_EMBEDDING=false
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
      - GF_SERVER_ROOT_URL=https://monitoring.its-camera-ai.com/grafana
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres-primary:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana_user
      - GF_DATABASE_PASSWORD=${GRAFANA_DB_PASSWORD}
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=${SMTP_HOST}
      - GF_SMTP_USER=${SMTP_USER}
      - GF_SMTP_PASSWORD=${SMTP_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - postgres-primary
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: its-camera-ai-alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://monitoring.its-camera-ai.com/alertmanager'
      - '--cluster.listen-address=0.0.0.0:9094'
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager-production.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    networks:
      - its-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: its-camera-ai-otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics
    volumes:
      - ./monitoring/opentelemetry/otel-collector.yaml:/etc/otel-collector-config.yml:ro
    networks:
      - its-network
    restart: unless-stopped
    depends_on:
      - prometheus

  # =============================================================================
  # Backup Services
  # =============================================================================
  backup-service:
    image: postgres:15-alpine
    container_name: its-camera-ai-backup
    environment:
      - PGPASSWORD=${POSTGRES_PASSWORD}
      - BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
      - POSTGRES_HOST=postgres-primary
      - POSTGRES_USER=its_user
      - POSTGRES_DB=its_camera_ai
      - BACKUP_RETENTION_DAYS=30
    volumes:
      - backup_data:/backups
      - ./scripts/backup.sh:/backup.sh:ro
    command: |
      sh -c "
      apk add --no-cache dcron aws-cli &&
      echo '0 2 * * * /backup.sh' | crontab - &&
      crond -f -d 8
      "
    depends_on:
      postgres-primary:
        condition: service_healthy
    networks:
      - its-network
    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
networks:
  its-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
  external-network:
    driver: bridge

# =============================================================================
# Production Volumes
# =============================================================================
volumes:
  # Load balancer
  nginx_logs:
    driver: local
  nginx_cache:
    driver: local

  # Frontend
  frontend_logs:
    driver: local

  # Application services
  api_logs:
    driver: local
  api_data:
    driver: local
  worker_logs:
    driver: local

  # ML services
  ml_models:
    driver: local
  ml_inference_logs:
    driver: local
  tensorrt_cache:
    driver: local

  # Stream processing
  stream_logs:
    driver: local
  stream_data:
    driver: local

  # Database cluster
  postgres_primary_data:
    driver: local
  postgres_replica_data:
    driver: local
  postgres_archive:
    driver: local

  # Redis cluster
  redis_primary_data:
    driver: local
  redis_replica_data:
    driver: local

  # Kafka cluster
  kafka_1_data:
    driver: local
  kafka_2_data:
    driver: local
  kafka_3_data:
    driver: local
  zookeeper_1_data:
    driver: local
  zookeeper_1_logs:
    driver: local
  zookeeper_2_data:
    driver: local
  zookeeper_2_logs:
    driver: local
  zookeeper_3_data:
    driver: local
  zookeeper_3_logs:
    driver: local

  # MinIO cluster
  minio_1_data:
    driver: local
  minio_2_data:
    driver: local
  minio_3_data:
    driver: local
  minio_4_data:
    driver: local

  # Monitoring
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local

  # Backup
  backup_data:
    driver: local
