"""Performance Integration Tests for 1000+ Camera Simulation.

This test suite validates system performance under high load conditions:
- 1000+ concurrent camera streams
- Real-time processing under load
- Database performance with high throughput
- Memory and resource management
- System stability under stress
- Performance regression detection
"""

import asyncio
import gc
import psutil
import random
import statistics
import time
from collections import defaultdict
from datetime import UTC, datetime, timedelta
from typing import Dict, List, Any
from unittest.mock import AsyncMock, patch
from uuid import uuid4

import pytest
import pytest_asyncio
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession

from its_camera_ai.core.logging import get_logger
from its_camera_ai.services.analytics_dtos import (
    DetectionData,
    DetectionResultDTO,
    BoundingBoxDTO,
    ProcessingResult,
)
from its_camera_ai.services.unified_analytics_service import UnifiedAnalyticsService
from its_camera_ai.performance.latency_monitor import LatencyMonitor
from its_camera_ai.performance.performance_optimizer import PerformanceOptimizer

logger = get_logger(__name__)


@pytest.mark.performance
@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.asyncio
class TestPerformance1000Cameras:
    """Performance tests for 1000+ camera simulation."""

    @pytest_asyncio.fixture
    async def performance_monitor(self):
        """Performance monitoring setup."""
        return LatencyMonitor()

    @pytest_asyncio.fixture
    async def camera_simulator(self):
        """Camera stream simulator for load testing."""
        
        class CameraSimulator:
            def __init__(self):
                self.camera_configs = {}
                self.frame_generators = {}
            
            def setup_cameras(self, count: int) -> List[str]:
                """Setup camera configurations."""
                camera_ids = []
                for i in range(count):
                    camera_id = f"load_test_camera_{i:04d}"
                    camera_ids.append(camera_id)
                    
                    # Simulate different camera characteristics
                    self.camera_configs[camera_id] = {
                        "location": f"intersection_{i % 100}",
                        "resolution": random.choice([(1920, 1080), (1280, 720), (640, 480)]),
                        "fps": random.choice([15, 25, 30]),
                        "traffic_density": random.choice(["low", "medium", "high"]),
                        "quality": random.uniform(0.7, 0.95),
                    }
                
                return camera_ids
            
            def generate_frame_data(self, camera_id: str, frame_num: int) -> Dict[str, Any]:
                """Generate realistic frame data for a camera."""
                config = self.camera_configs[camera_id]\n                
                # Simulate varying traffic based on time and camera type\n                base_vehicles = {\n                    \"low\": random.randint(0, 5),\n                    \"medium\": random.randint(3, 15),\n                    \"high\": random.randint(10, 30)\n                }[config[\"traffic_density\"]]\n                \n                # Add some randomness and patterns\n                vehicle_count = max(0, base_vehicles + random.randint(-3, 3))\n                \n                detections = []\n                vehicle_types = [\"car\", \"truck\", \"bus\", \"motorcycle\", \"van\"]\n                \n                for i in range(vehicle_count):\n                    vehicle_type = random.choice(vehicle_types)\n                    detection = {\n                        \"detection_id\": str(uuid4()),\n                        \"class_name\": vehicle_type,\n                        \"confidence\": random.uniform(0.7, 0.95),\n                        \"bbox\": {\n                            \"x1\": random.randint(0, config[\"resolution\"][0] - 200),\n                            \"y1\": random.randint(0, config[\"resolution\"][1] - 150),\n                            \"x2\": random.randint(100, 200),\n                            \"y2\": random.randint(100, 150)\n                        },\n                        \"track_id\": f\"track_{camera_id}_{i}_{frame_num}\",\n                        \"vehicle_type\": vehicle_type,\n                        \"speed\": random.uniform(20.0, 80.0),\n                        \"direction\": random.choice([\"north\", \"south\", \"east\", \"west\"]),\n                        \"attributes\": {\n                            \"color\": random.choice([\"red\", \"blue\", \"white\", \"black\", \"gray\"]),\n                            \"size\": random.choice([\"small\", \"medium\", \"large\"])\n                        }\n                    }\n                    detections.append(detection)\n                \n                # Adjust bbox coordinates\n                for detection in detections:\n                    bbox = detection[\"bbox\"]\n                    bbox[\"x2\"] = bbox[\"x1\"] + bbox[\"x2\"]\n                    bbox[\"y2\"] = bbox[\"y1\"] + bbox[\"y2\"]\n                \n                return {\n                    \"camera_id\": camera_id,\n                    \"frame_id\": f\"frame_{frame_num:06d}\",\n                    \"timestamp\": datetime.now(UTC),\n                    \"inference_time_ms\": random.uniform(30.0, 80.0),\n                    \"model_version\": \"yolo11n_v1.2\",\n                    \"confidence_threshold\": 0.5,\n                    \"detections\": detections,\n                    \"frame_metadata\": {\n                        \"resolution\": config[\"resolution\"],\n                        \"fps\": config[\"fps\"],\n                        \"quality_score\": config[\"quality\"],\n                        \"lighting_conditions\": random.choice([\"good\", \"fair\", \"poor\"]),\n                        \"weather\": random.choice([\"clear\", \"cloudy\", \"rainy\"])\n                    }\n                }\n        \n        return CameraSimulator()\n\n    @pytest_asyncio.fixture\n    async def mock_ml_pipeline_optimized(self):\n        \"\"\"Optimized ML pipeline mock for performance testing.\"\"\"\n        pipeline = AsyncMock()\n        \n        async def process_frame_batch(frame_data_list: List[Dict[str, Any]]) -> List[DetectionData]:\n            \"\"\"Process multiple frames in batch for better performance.\"\"\"\n            results = []\n            \n            # Simulate batch processing efficiency\n            batch_size = len(frame_data_list)\n            base_time = 0.030  # 30ms base processing time\n            batch_efficiency = max(0.5, 1.0 - (batch_size * 0.01))  # Efficiency decreases with batch size\n            processing_time = base_time * batch_efficiency\n            \n            await asyncio.sleep(processing_time)\n            \n            for frame_data in frame_data_list:\n                detection_data = DetectionData(\n                    camera_id=frame_data[\"camera_id\"],\n                    timestamp=frame_data[\"timestamp\"],\n                    frame_id=frame_data[\"frame_id\"],\n                    vehicle_count=len(frame_data[\"detections\"]),\n                    detections=[\n                        DetectionResultDTO(\n                            detection_id=det[\"detection_id\"],\n                            class_name=det[\"class_name\"],\n                            confidence=det[\"confidence\"],\n                            bbox=BoundingBoxDTO(**det[\"bbox\"]),\n                            track_id=det[\"track_id\"],\n                            timestamp=frame_data[\"timestamp\"],\n                            vehicle_type=det[\"vehicle_type\"],\n                            speed=det[\"speed\"],\n                            direction=det[\"direction\"],\n                            attributes=det[\"attributes\"]\n                        )\n                        for det in frame_data[\"detections\"]\n                    ],\n                    processing_metadata={\n                        \"model_version\": frame_data[\"model_version\"],\n                        \"inference_time_ms\": frame_data[\"inference_time_ms\"],\n                        \"batch_size\": batch_size,\n                        \"batch_processing_time_ms\": processing_time * 1000\n                    }\n                )\n                results.append(detection_data)\n            \n            return results\n        \n        pipeline.process_frame_batch = process_frame_batch\n        return pipeline\n\n    async def test_1000_cameras_concurrent_processing(\n        self,\n        camera_simulator,\n        mock_ml_pipeline_optimized,\n        unified_analytics_service: UnifiedAnalyticsService,\n        performance_monitor,\n        redis_client: redis.Redis\n    ):\n        \"\"\"Test concurrent processing of 1000 camera streams.\"\"\"\n        \n        camera_count = 1000\n        frames_per_camera = 5\n        batch_size = 50  # Process cameras in batches\n        \n        logger.info(f\"Starting 1000 camera test: {camera_count} cameras, {frames_per_camera} frames each\")\n        \n        # Setup cameras\n        camera_ids = camera_simulator.setup_cameras(camera_count)\n        \n        # Track performance metrics\n        start_memory = psutil.virtual_memory().used / 1024 / 1024  # MB\n        start_time = time.time()\n        processing_times = []\n        throughput_metrics = []\n        error_count = 0\n        \n        async def process_camera_batch(camera_batch: List[str], batch_num: int):\n            \"\"\"Process a batch of cameras concurrently.\"\"\"\n            nonlocal error_count\n            batch_results = []\n            batch_start = time.time()\n            \n            try:\n                # Generate frames for all cameras in batch\n                frame_tasks = []\n                for camera_id in camera_batch:\n                    for frame_num in range(frames_per_camera):\n                        frame_data = camera_simulator.generate_frame_data(camera_id, frame_num)\n                        frame_tasks.append(frame_data)\n                \n                # Process frames in smaller sub-batches for ML pipeline\n                ml_batch_size = 10\n                detection_results = []\n                \n                for i in range(0, len(frame_tasks), ml_batch_size):\n                    sub_batch = frame_tasks[i:i + ml_batch_size]\n                    ml_results = await mock_ml_pipeline_optimized.process_frame_batch(sub_batch)\n                    detection_results.extend(ml_results)\n                \n                # Process analytics for all detections\n                analytics_tasks = []\n                for detection_data in detection_results:\n                    task = unified_analytics_service.process_realtime_analytics(\n                        detection_data=detection_data,\n                        include_anomaly_detection=False,  # Disable for performance\n                        include_incident_detection=False,\n                        include_rule_evaluation=True,\n                        include_speed_calculation=False\n                    )\n                    analytics_tasks.append(task)\n                \n                # Execute analytics processing concurrently\n                analytics_results = await asyncio.gather(*analytics_tasks, return_exceptions=True)\n                \n                # Count successful results\n                successful_results = [\n                    result for result in analytics_results \n                    if not isinstance(result, Exception)\n                ]\n                batch_results.extend(successful_results)\n                \n                # Count errors\n                batch_errors = len(analytics_results) - len(successful_results)\n                error_count += batch_errors\n                \n                batch_time = time.time() - batch_start\n                batch_throughput = len(successful_results) / batch_time\n                \n                processing_times.append(batch_time)\n                throughput_metrics.append(batch_throughput)\n                \n                logger.info(\n                    f\"Batch {batch_num}: {len(camera_batch)} cameras, \"\n                    f\"{len(successful_results)} frames processed, \"\n                    f\"{batch_time:.2f}s, {batch_throughput:.1f} frames/s, \"\n                    f\"{batch_errors} errors\"\n                )\n                \n                return batch_results\n                \n            except Exception as e:\n                logger.error(f\"Batch {batch_num} failed: {e}\")\n                error_count += len(camera_batch) * frames_per_camera\n                return []\n        \n        # Process cameras in batches\n        batch_tasks = []\n        for i in range(0, camera_count, batch_size):\n            camera_batch = camera_ids[i:i + batch_size]\n            batch_num = i // batch_size + 1\n            task = process_camera_batch(camera_batch, batch_num)\n            batch_tasks.append(task)\n        \n        # Execute all batches concurrently with limited concurrency\n        max_concurrent_batches = 10\n        all_results = []\n        \n        for i in range(0, len(batch_tasks), max_concurrent_batches):\n            concurrent_batch = batch_tasks[i:i + max_concurrent_batches]\n            batch_results = await asyncio.gather(*concurrent_batch, return_exceptions=True)\n            \n            for result in batch_results:\n                if isinstance(result, list):\n                    all_results.extend(result)\n                else:\n                    logger.error(f\"Batch execution error: {result}\")\n        \n        total_time = time.time() - start_time\n        end_memory = psutil.virtual_memory().used / 1024 / 1024  # MB\n        memory_increase = end_memory - start_memory\n        \n        # Calculate performance metrics\n        total_frames_expected = camera_count * frames_per_camera\n        total_frames_processed = len(all_results)\n        success_rate = total_frames_processed / total_frames_expected\n        overall_throughput = total_frames_processed / total_time\n        avg_processing_time = statistics.mean(processing_times) if processing_times else 0\n        avg_batch_throughput = statistics.mean(throughput_metrics) if throughput_metrics else 0\n        \n        # Performance assertions\n        logger.info(\n            f\"1000 Camera Test Results:\\n\"\n            f\"  Total Time: {total_time:.2f}s\\n\"\n            f\"  Frames Processed: {total_frames_processed}/{total_frames_expected}\\n\"\n            f\"  Success Rate: {success_rate:.1%}\\n\"\n            f\"  Overall Throughput: {overall_throughput:.1f} frames/s\\n\"\n            f\"  Avg Batch Throughput: {avg_batch_throughput:.1f} frames/s\\n\"\n            f\"  Avg Batch Time: {avg_processing_time:.2f}s\\n\"\n            f\"  Memory Increase: {memory_increase:.1f} MB\\n\"\n            f\"  Error Count: {error_count}\"\n        )\n        \n        # Performance requirements\n        assert success_rate >= 0.95, f\"Success rate {success_rate:.1%} below 95% threshold\"\n        assert overall_throughput >= 500, f\"Throughput {overall_throughput:.1f} below 500 frames/s\"\n        assert total_time <= 60, f\"Total time {total_time:.1f}s exceeds 60s limit\"\n        assert memory_increase <= 2000, f\"Memory increase {memory_increase:.1f}MB exceeds 2GB limit\"\n        assert error_count <= total_frames_expected * 0.05, \"Error rate exceeds 5%\"\n        \n        # Cleanup and force garbage collection\n        gc.collect()\n\n    async def test_sustained_high_load_performance(\n        self,\n        camera_simulator,\n        mock_ml_pipeline_optimized,\n        unified_analytics_service: UnifiedAnalyticsService,\n        redis_client: redis.Redis\n    ):\n        \"\"\"Test sustained high load over extended period.\"\"\"\n        \n        camera_count = 500  # Smaller count for sustained test\n        duration_minutes = 2  # 2 minutes sustained test\n        target_fps = 1  # 1 frame per second per camera\n        \n        logger.info(f\"Starting sustained load test: {camera_count} cameras for {duration_minutes} minutes\")\n        \n        camera_ids = camera_simulator.setup_cameras(camera_count)\n        \n        # Performance tracking\n        start_time = time.time()\n        end_time = start_time + (duration_minutes * 60)\n        \n        frame_counter = 0\n        error_counter = 0\n        processing_times = []\n        memory_samples = []\n        \n        async def camera_stream_worker(camera_id: str):\n            \"\"\"Simulate continuous camera stream.\"\"\"\n            nonlocal frame_counter, error_counter\n            \n            frame_num = 0\n            last_frame_time = time.time()\n            \n            while time.time() < end_time:\n                try:\n                    # Generate frame data\n                    frame_data = camera_simulator.generate_frame_data(camera_id, frame_num)\n                    \n                    # Process through ML pipeline\n                    ml_start = time.time()\n                    detection_results = await mock_ml_pipeline_optimized.process_frame_batch([frame_data])\n                    \n                    # Process analytics\n                    if detection_results:\n                        result = await unified_analytics_service.process_realtime_analytics(\n                            detection_data=detection_results[0],\n                            include_anomaly_detection=False,\n                            include_incident_detection=False\n                        )\n                        \n                        processing_time = time.time() - ml_start\n                        processing_times.append(processing_time)\n                        frame_counter += 1\n                    \n                    frame_num += 1\n                    \n                    # Maintain target FPS\n                    elapsed = time.time() - last_frame_time\n                    sleep_time = max(0, (1.0 / target_fps) - elapsed)\n                    if sleep_time > 0:\n                        await asyncio.sleep(sleep_time)\n                    \n                    last_frame_time = time.time()\n                    \n                except Exception as e:\n                    error_counter += 1\n                    logger.debug(f\"Stream error for {camera_id}: {e}\")\n                    await asyncio.sleep(0.1)  # Brief pause on error\n        \n        # Start all camera streams\n        stream_tasks = [camera_stream_worker(camera_id) for camera_id in camera_ids]\n        \n        # Monitor system resources\n        async def resource_monitor():\n            while time.time() < end_time:\n                memory_mb = psutil.virtual_memory().used / 1024 / 1024\n                memory_samples.append(memory_mb)\n                await asyncio.sleep(5)  # Sample every 5 seconds\n        \n        monitor_task = asyncio.create_task(resource_monitor())\n        \n        # Run sustained load test\n        await asyncio.gather(*stream_tasks, monitor_task, return_exceptions=True)\n        \n        total_duration = time.time() - start_time\n        expected_frames = camera_count * duration_minutes * 60 * target_fps\n        success_rate = frame_counter / expected_frames\n        avg_processing_time = statistics.mean(processing_times) if processing_times else 0\n        \n        # Memory analysis\n        if memory_samples:\n            memory_growth = max(memory_samples) - min(memory_samples)\n            avg_memory = statistics.mean(memory_samples)\n        else:\n            memory_growth = 0\n            avg_memory = 0\n        \n        logger.info(\n            f\"Sustained Load Test Results:\\n\"\n            f\"  Duration: {total_duration:.1f}s\\n\"\n            f\"  Frames Processed: {frame_counter}/{expected_frames:.0f}\\n\"\n            f\"  Success Rate: {success_rate:.1%}\\n\"\n            f\"  Avg Processing Time: {avg_processing_time:.3f}s\\n\"\n            f\"  Error Count: {error_counter}\\n\"\n            f\"  Memory Growth: {memory_growth:.1f}MB\\n\"\n            f\"  Avg Memory Usage: {avg_memory:.1f}MB\"\n        )\n        \n        # Sustained performance requirements\n        assert success_rate >= 0.90, f\"Sustained success rate {success_rate:.1%} below 90%\"\n        assert avg_processing_time <= 0.2, f\"Avg processing time {avg_processing_time:.3f}s too high\"\n        assert memory_growth <= 1000, f\"Memory growth {memory_growth:.1f}MB excessive\"\n        assert error_counter <= expected_frames * 0.1, \"Error rate exceeds 10% in sustained test\"\n\n    async def test_database_performance_under_load(\n        self,\n        camera_simulator,\n        unified_analytics_service: UnifiedAnalyticsService,\n        db_session: AsyncSession\n    ):\n        \"\"\"Test database performance under high insert load.\"\"\"\n        \n        camera_count = 100\n        records_per_camera = 100\n        total_records = camera_count * records_per_camera\n        \n        logger.info(f\"Testing database performance: {total_records} records\")\n        \n        camera_ids = camera_simulator.setup_cameras(camera_count)\n        \n        # Generate test data\n        test_records = []\n        for camera_id in camera_ids:\n            for i in range(records_per_camera):\n                timestamp = datetime.now(UTC) - timedelta(minutes=i)\n                record = {\n                    \"camera_id\": camera_id,\n                    \"timestamp\": timestamp,\n                    \"vehicle_count\": random.randint(0, 30),\n                    \"average_speed\": random.uniform(20.0, 80.0),\n                    \"traffic_density\": random.uniform(0.0, 1.0),\n                    \"congestion_level\": random.choice([\"free_flow\", \"light\", \"moderate\", \"heavy\", \"severe\"]),\n                    \"processing_time_ms\": random.uniform(30.0, 100.0)\n                }\n                test_records.append(record)\n        \n        # Test batch insert performance\n        batch_size = 100\n        insert_times = []\n        \n        start_time = time.time()\n        \n        for i in range(0, len(test_records), batch_size):\n            batch = test_records[i:i + batch_size]\n            \n            batch_start = time.time()\n            \n            # Simulate database insert operations\n            # In a real implementation, this would use the analytics repository\n            await asyncio.sleep(0.01 * len(batch))  # Simulate insert time\n            \n            batch_time = time.time() - batch_start\n            insert_times.append(batch_time)\n            \n            if (i // batch_size + 1) % 10 == 0:\n                logger.info(f\"Inserted {i + len(batch)} records...\")\n        \n        total_time = time.time() - start_time\n        records_per_second = total_records / total_time\n        avg_batch_time = statistics.mean(insert_times)\n        \n        logger.info(\n            f\"Database Performance Results:\\n\"\n            f\"  Total Records: {total_records}\\n\"\n            f\"  Total Time: {total_time:.2f}s\\n\"\n            f\"  Records/Second: {records_per_second:.1f}\\n\"\n            f\"  Avg Batch Time: {avg_batch_time:.3f}s\\n\"\n            f\"  Batch Size: {batch_size}\"\n        )\n        \n        # Database performance requirements\n        assert records_per_second >= 1000, f\"Insert rate {records_per_second:.1f} below 1000 records/s\"\n        assert avg_batch_time <= 5.0, f\"Avg batch time {avg_batch_time:.3f}s too high\"\n        assert total_time <= 60, f\"Total insert time {total_time:.2f}s exceeds 60s\"\n\n    async def test_cache_performance_under_load(\n        self,\n        redis_client: redis.Redis,\n        camera_simulator\n    ):\n        \"\"\"Test Redis cache performance under high load.\"\"\"\n        \n        camera_count = 1000\n        operations_per_camera = 50\n        total_operations = camera_count * operations_per_camera\n        \n        logger.info(f\"Testing cache performance: {total_operations} operations\")\n        \n        camera_ids = camera_simulator.setup_cameras(camera_count)\n        \n        # Test data\n        cache_operations = []\n        operation_times = []\n        \n        async def cache_worker(camera_id: str):\n            \"\"\"Perform cache operations for a camera.\"\"\"\n            worker_times = []\n            \n            for i in range(operations_per_camera):\n                # Generate cache data\n                cache_key = f\"realtime_analytics:{camera_id}:{i}\"\n                cache_data = {\n                    \"camera_id\": camera_id,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                    \"vehicle_count\": random.randint(0, 30),\n                    \"processing_time_ms\": random.uniform(30.0, 100.0),\n                    \"frame_id\": f\"frame_{i}\"\n                }\n                \n                # Test SET operation\n                start_time = time.time()\n                await redis_client.setex(cache_key, 300, json.dumps(cache_data))\n                set_time = time.time() - start_time\n                \n                # Test GET operation\n                start_time = time.time()\n                retrieved_data = await redis_client.get(cache_key)\n                get_time = time.time() - start_time\n                \n                # Verify data\n                assert retrieved_data is not None\n                \n                worker_times.extend([set_time, get_time])\n            \n            return worker_times\n        \n        # Execute cache operations concurrently\n        start_time = time.time()\n        \n        # Process in batches to avoid overwhelming Redis\n        batch_size = 100\n        all_times = []\n        \n        for i in range(0, camera_count, batch_size):\n            batch_cameras = camera_ids[i:i + batch_size]\n            batch_tasks = [cache_worker(camera_id) for camera_id in batch_cameras]\n            \n            batch_results = await asyncio.gather(*batch_tasks)\n            \n            for worker_times in batch_results:\n                all_times.extend(worker_times)\n            \n            logger.info(f\"Completed cache operations for {i + len(batch_cameras)} cameras\")\n        \n        total_time = time.time() - start_time\n        operations_per_second = len(all_times) / total_time\n        avg_operation_time = statistics.mean(all_times)\n        p95_operation_time = statistics.quantiles(all_times, n=20)[18]  # 95th percentile\n        \n        logger.info(\n            f\"Cache Performance Results:\\n\"\n            f\"  Total Operations: {len(all_times)}\\n\"\n            f\"  Total Time: {total_time:.2f}s\\n\"\n            f\"  Operations/Second: {operations_per_second:.1f}\\n\"\n            f\"  Avg Operation Time: {avg_operation_time:.4f}s\\n\"\n            f\"  P95 Operation Time: {p95_operation_time:.4f}s\"\n        )\n        \n        # Cache performance requirements\n        assert operations_per_second >= 5000, f\"Cache ops/s {operations_per_second:.1f} below 5000\"\n        assert avg_operation_time <= 0.01, f\"Avg operation time {avg_operation_time:.4f}s too high\"\n        assert p95_operation_time <= 0.05, f\"P95 operation time {p95_operation_time:.4f}s too high\"\n\n    @pytest.mark.benchmark\n    async def test_performance_regression_detection(\n        self,\n        camera_simulator,\n        mock_ml_pipeline_optimized,\n        unified_analytics_service: UnifiedAnalyticsService,\n        performance_monitor\n    ):\n        \"\"\"Test for performance regressions using baseline benchmarks.\"\"\"\n        \n        # Performance baselines (these would be stored and updated)\n        performance_baselines = {\n            \"processing_time_p95_ms\": 150.0,  # 95th percentile processing time\n            \"throughput_fps_min\": 100.0,     # Minimum frames per second\n            \"memory_growth_mb_max\": 500.0,   # Maximum memory growth\n            \"error_rate_max\": 0.02,          # Maximum error rate (2%)\n        }\n        \n        camera_count = 200\n        frames_per_camera = 10\n        \n        logger.info(f\"Performance regression test: {camera_count} cameras, {frames_per_camera} frames\")\n        \n        camera_ids = camera_simulator.setup_cameras(camera_count)\n        \n        # Collect performance metrics\n        processing_times = []\n        error_count = 0\n        start_memory = psutil.virtual_memory().used / 1024 / 1024\n        start_time = time.time()\n        \n        # Process frames and collect metrics\n        for camera_id in camera_ids:\n            for frame_num in range(frames_per_camera):\n                try:\n                    frame_data = camera_simulator.generate_frame_data(camera_id, frame_num)\n                    \n                    frame_start = time.time()\n                    \n                    # Process through pipeline\n                    detection_results = await mock_ml_pipeline_optimized.process_frame_batch([frame_data])\n                    \n                    if detection_results:\n                        result = await unified_analytics_service.process_realtime_analytics(\n                            detection_data=detection_results[0]\n                        )\n                    \n                    frame_time = (time.time() - frame_start) * 1000  # Convert to ms\n                    processing_times.append(frame_time)\n                    \n                except Exception as e:\n                    error_count += 1\n                    logger.debug(f\"Processing error: {e}\")\n        \n        total_time = time.time() - start_time\n        end_memory = psutil.virtual_memory().used / 1024 / 1024\n        \n        # Calculate metrics\n        total_frames = camera_count * frames_per_camera\n        throughput_fps = total_frames / total_time\n        memory_growth_mb = end_memory - start_memory\n        error_rate = error_count / total_frames\n        p95_processing_time = statistics.quantiles(processing_times, n=20)[18] if processing_times else 0\n        \n        current_metrics = {\n            \"processing_time_p95_ms\": p95_processing_time,\n            \"throughput_fps_min\": throughput_fps,\n            \"memory_growth_mb_max\": memory_growth_mb,\n            \"error_rate_max\": error_rate,\n        }\n        \n        logger.info(\n            f\"Performance Regression Test Results:\\n\"\n            f\"  P95 Processing Time: {p95_processing_time:.1f}ms (baseline: {performance_baselines['processing_time_p95_ms']})\\n\"\n            f\"  Throughput: {throughput_fps:.1f} fps (baseline: {performance_baselines['throughput_fps_min']})\\n\"\n            f\"  Memory Growth: {memory_growth_mb:.1f}MB (baseline: {performance_baselines['memory_growth_mb_max']})\\n\"\n            f\"  Error Rate: {error_rate:.3%} (baseline: {performance_baselines['error_rate_max']:.1%})\"\n        )\n        \n        # Check for regressions\n        regressions = []\n        \n        if p95_processing_time > performance_baselines[\"processing_time_p95_ms\"]:\n            regressions.append(f\"Processing time regression: {p95_processing_time:.1f}ms > {performance_baselines['processing_time_p95_ms']}ms\")\n        \n        if throughput_fps < performance_baselines[\"throughput_fps_min\"]:\n            regressions.append(f\"Throughput regression: {throughput_fps:.1f} fps < {performance_baselines['throughput_fps_min']} fps\")\n        \n        if memory_growth_mb > performance_baselines[\"memory_growth_mb_max\"]:\n            regressions.append(f\"Memory regression: {memory_growth_mb:.1f}MB > {performance_baselines['memory_growth_mb_max']}MB\")\n        \n        if error_rate > performance_baselines[\"error_rate_max\"]:\n            regressions.append(f\"Error rate regression: {error_rate:.3%} > {performance_baselines['error_rate_max']:.1%}\")\n        \n        # Assert no regressions\n        if regressions:\n            regression_message = \"\\n\".join(regressions)\n            pytest.fail(f\"Performance regressions detected:\\n{regression_message}\")\n        \n        logger.info(\"No performance regressions detected - all metrics within baselines\")