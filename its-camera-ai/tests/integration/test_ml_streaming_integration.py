"""Real-time ML Streaming Pipeline Integration Tests.

This test suite validates the ML pipeline streaming integration:
- YOLO11 model inference streaming
- Real-time frame processing
- GPU optimization and batching
- Stream synchronization and ordering
- Memory management under load
- Error recovery and resilience
- Background worker integration
"""

import asyncio
import json
import random
import time
from collections import deque
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, MagicMock, patch
from uuid import uuid4

import pytest
import pytest_asyncio
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession

from its_camera_ai.core.logging import get_logger
from its_camera_ai.ml.core_vision_engine import CoreVisionEngine
from its_camera_ai.ml.ultra_fast_streaming_pipeline import UltraFastStreamingPipeline
from its_camera_ai.services.ml_streaming_integration_service import MLStreamingIntegrationService
from its_camera_ai.services.analytics_dtos import (
    DetectionData,
    DetectionResultDTO,
    BoundingBoxDTO,
    ProcessingResult,
)
from its_camera_ai.workers.analytics_worker import AnalyticsWorker

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.ml
@pytest.mark.asyncio
class TestMLStreamingIntegration:
    """Integration tests for ML streaming pipeline."""

    @pytest_asyncio.fixture
    async def mock_vision_engine(self):
        \"\"\"Mock YOLO11 vision engine with realistic performance characteristics.\"\"\"\n        engine = AsyncMock(spec=CoreVisionEngine)\n        \n        # Simulate GPU processing characteristics\n        engine.gpu_available = True\n        engine.model_loaded = True\n        engine.batch_size = 8\n        engine.input_resolution = (640, 640)\n        \n        async def process_frame(frame_data: bytes, metadata: Dict[str, Any]) -> Dict[str, Any]:\n            \"\"\"Simulate YOLO11 inference with realistic timing.\"\"\"\n            # Simulate inference time based on image complexity\n            base_inference_time = 0.045  # 45ms base time\n            complexity_factor = len(frame_data) / (1920 * 1080 * 3)  # Normalized complexity\n            inference_time = base_inference_time * (0.8 + 0.4 * complexity_factor)\n            \n            await asyncio.sleep(inference_time)\n            \n            # Generate realistic detection results\n            num_detections = random.randint(0, 15)  # 0-15 vehicles per frame\n            detections = []\n            \n            for i in range(num_detections):\n                detection = {\n                    \"class_id\": random.choice([0, 1, 2, 3, 5, 7]),  # COCO vehicle classes\n                    \"class_name\": random.choice([\"car\", \"truck\", \"bus\", \"motorcycle\"]),\n                    \"confidence\": random.uniform(0.5, 0.95),\n                    \"bbox\": {\n                        \"x1\": random.randint(0, 1280),\n                        \"y1\": random.randint(0, 720),\n                        \"x2\": random.randint(100, 300),\n                        \"y2\": random.randint(80, 200)\n                    },\n                    \"track_id\": f\"track_{uuid4().hex[:8]}\",\n                    \"speed\": random.uniform(0, 80),\n                    \"direction\": random.choice([\"north\", \"south\", \"east\", \"west\"]),\n                }\n                \n                # Ensure bbox coordinates are valid\n                detection[\"bbox\"][\"x2\"] = detection[\"bbox\"][\"x1\"] + detection[\"bbox\"][\"x2\"]\n                detection[\"bbox\"][\"y2\"] = detection[\"bbox\"][\"y1\"] + detection[\"bbox\"][\"y2\"]\n                \n                detections.append(detection)\n            \n            return {\n                \"detections\": detections,\n                \"inference_time_ms\": inference_time * 1000,\n                \"model_version\": \"yolo11n_v1.2\",\n                \"gpu_utilization\": random.uniform(0.6, 0.9),\n                \"memory_usage_mb\": random.uniform(800, 1200),\n                \"timestamp\": datetime.now(UTC).isoformat()\n            }\n        \n        async def process_batch(frames: List[bytes], metadata_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n            \"\"\"Simulate batch processing for better GPU utilization.\"\"\"\n            batch_size = len(frames)\n            \n            # Batch processing efficiency\n            base_time = 0.045\n            batch_efficiency = 0.7 + (0.3 / batch_size)  # Efficiency improves with batch size\n            total_inference_time = base_time * batch_size * batch_efficiency\n            \n            await asyncio.sleep(total_inference_time)\n            \n            results = []\n            for i, (frame_data, metadata) in enumerate(zip(frames, metadata_list)):\n                result = await process_frame(frame_data, metadata)\n                result[\"batch_id\"] = metadata.get(\"batch_id\", str(uuid4()))\n                result[\"batch_position\"] = i\n                result[\"batch_size\"] = batch_size\n                results.append(result)\n            \n            return results\n        \n        engine.process_frame = process_frame\n        engine.process_batch = process_batch\n        \n        return engine\n\n    @pytest_asyncio.fixture\n    async def streaming_pipeline(self, mock_vision_engine):\n        \"\"\"Ultra-fast streaming pipeline with mocked vision engine.\"\"\"\n        pipeline = UltraFastStreamingPipeline(\n            vision_engine=mock_vision_engine,\n            max_batch_size=8,\n            batch_timeout_ms=50,\n            max_queue_size=100\n        )\n        return pipeline\n\n    @pytest_asyncio.fixture\n    async def ml_integration_service(self, streaming_pipeline, redis_client):\n        \"\"\"ML streaming integration service.\"\"\"\n        service = MLStreamingIntegrationService(\n            streaming_pipeline=streaming_pipeline,\n            redis_client=redis_client,\n            max_concurrent_streams=50\n        )\n        return service\n\n    @pytest_asyncio.fixture\n    async def frame_generator(self):\n        \"\"\"Generate simulated camera frames.\"\"\"\n        \n        class FrameGenerator:\n            def __init__(self):\n                self.frame_counter = 0\n            \n            def generate_frame(self, camera_id: str, resolution: tuple = (1920, 1080)) -> Dict[str, Any]:\n                \"\"\"Generate a simulated camera frame.\"\"\"\n                self.frame_counter += 1\n                \n                # Simulate frame data (simplified)\n                frame_size = resolution[0] * resolution[1] * 3  # RGB\n                frame_data = b'\\x00' * min(frame_size, 1000)  # Simplified frame data\n                \n                return {\n                    \"camera_id\": camera_id,\n                    \"frame_id\": f\"frame_{self.frame_counter:06d}\",\n                    \"timestamp\": datetime.now(UTC),\n                    \"frame_data\": frame_data,\n                    \"resolution\": resolution,\n                    \"fps\": 30,\n                    \"format\": \"RGB\",\n                    \"metadata\": {\n                        \"sequence_number\": self.frame_counter,\n                        \"quality\": random.uniform(0.8, 1.0),\n                        \"lighting\": random.choice([\"good\", \"fair\", \"poor\"]),\n                        \"weather\": random.choice([\"clear\", \"cloudy\", \"rainy\"])\n                    }\n                }\n            \n            async def stream_frames(self, camera_id: str, duration_seconds: int, fps: int = 30):\n                \"\"\"Generate continuous stream of frames.\"\"\"\n                frames = []\n                frame_interval = 1.0 / fps\n                end_time = time.time() + duration_seconds\n                \n                while time.time() < end_time:\n                    frame = self.generate_frame(camera_id)\n                    frames.append(frame)\n                    \n                    yield frame\n                    \n                    await asyncio.sleep(frame_interval)\n                \n                return frames\n        \n        return FrameGenerator()\n\n    async def test_single_camera_streaming(self, ml_integration_service, frame_generator):\n        \"\"\"Test real-time streaming from a single camera.\"\"\"\n        camera_id = \"test_camera_streaming_001\"\n        stream_duration = 5  # 5 seconds\n        target_fps = 30\n        \n        logger.info(f\"Testing single camera streaming: {stream_duration}s at {target_fps} FPS\")\n        \n        processed_frames = []\n        processing_times = []\n        frame_count = 0\n        \n        async def process_stream():\n            \"\"\"Process camera stream in real-time.\"\"\"\n            nonlocal frame_count\n            \n            async for frame in frame_generator.stream_frames(camera_id, stream_duration, target_fps):\n                start_time = time.time()\n                \n                # Process frame through ML pipeline\n                result = await ml_integration_service.process_frame(\n                    camera_id=frame[\"camera_id\"],\n                    frame_data=frame[\"frame_data\"],\n                    metadata=frame[\"metadata\"]\n                )\n                \n                processing_time = time.time() - start_time\n                processing_times.append(processing_time)\n                processed_frames.append(result)\n                frame_count += 1\n        \n        # Execute streaming test\n        start_time = time.time()\n        await process_stream()\n        total_time = time.time() - start_time\n        \n        # Analyze results\n        expected_frames = stream_duration * target_fps\n        actual_fps = frame_count / total_time\n        avg_processing_time = sum(processing_times) / len(processing_times)\n        max_processing_time = max(processing_times)\n        \n        logger.info(\n            f\"Single Camera Streaming Results:\\n\"\n            f\"  Frames Processed: {frame_count}/{expected_frames}\\n\"\n            f\"  Actual FPS: {actual_fps:.1f}\\n\"\n            f\"  Avg Processing Time: {avg_processing_time:.3f}s\\n\"\n            f\"  Max Processing Time: {max_processing_time:.3f}s\\n\"\n            f\"  Total Duration: {total_time:.1f}s\"\n        )\n        \n        # Performance assertions\n        assert frame_count >= expected_frames * 0.95, f\"Dropped frames: {frame_count}/{expected_frames}\"\n        assert actual_fps >= target_fps * 0.9, f\"FPS too low: {actual_fps:.1f} < {target_fps * 0.9}\"\n        assert avg_processing_time <= 0.1, f\"Avg processing time too high: {avg_processing_time:.3f}s\"\n        assert max_processing_time <= 0.2, f\"Max processing time too high: {max_processing_time:.3f}s\"\n        \n        # Validate detection results\n        for result in processed_frames:\n            assert \"detections\" in result\n            assert \"inference_time_ms\" in result\n            assert \"model_version\" in result\n            assert result[\"inference_time_ms\"] > 0\n\n    async def test_multi_camera_concurrent_streaming(\n        self, ml_integration_service, frame_generator\n    ):\n        \"\"\"Test concurrent streaming from multiple cameras.\"\"\"\n        camera_count = 10\n        stream_duration = 3  # 3 seconds\n        target_fps = 15  # Reduced FPS for multiple cameras\n        \n        camera_ids = [f\"multi_camera_{i:02d}\" for i in range(camera_count)]\n        \n        logger.info(f\"Testing multi-camera streaming: {camera_count} cameras, {stream_duration}s\")\n        \n        results_per_camera = {camera_id: [] for camera_id in camera_ids}\n        processing_times = []\n        \n        async def process_camera_stream(camera_id: str):\n            \"\"\"Process stream for a single camera.\"\"\"\n            camera_results = []\n            \n            async for frame in frame_generator.stream_frames(camera_id, stream_duration, target_fps):\n                start_time = time.time()\n                \n                try:\n                    result = await ml_integration_service.process_frame(\n                        camera_id=frame[\"camera_id\"],\n                        frame_data=frame[\"frame_data\"],\n                        metadata=frame[\"metadata\"]\n                    )\n                    \n                    processing_time = time.time() - start_time\n                    processing_times.append(processing_time)\n                    camera_results.append(result)\n                    \n                except Exception as e:\n                    logger.error(f\"Frame processing error for {camera_id}: {e}\")\n            \n            results_per_camera[camera_id] = camera_results\n            return len(camera_results)\n        \n        # Execute concurrent streaming\n        start_time = time.time()\n        tasks = [process_camera_stream(camera_id) for camera_id in camera_ids]\n        frame_counts = await asyncio.gather(*tasks, return_exceptions=True)\n        total_time = time.time() - start_time\n        \n        # Analyze results\n        total_frames_processed = sum(\n            count for count in frame_counts if isinstance(count, int)\n        )\n        expected_total_frames = camera_count * stream_duration * target_fps\n        overall_fps = total_frames_processed / total_time\n        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0\n        \n        # Per-camera analysis\n        frames_per_camera = [len(results) for results in results_per_camera.values()]\n        min_frames = min(frames_per_camera) if frames_per_camera else 0\n        max_frames = max(frames_per_camera) if frames_per_camera else 0\n        \n        logger.info(\n            f\"Multi-Camera Streaming Results:\\n\"\n            f\"  Total Frames: {total_frames_processed}/{expected_total_frames}\\n\"\n            f\"  Overall FPS: {overall_fps:.1f}\\n\"\n            f\"  Avg Processing Time: {avg_processing_time:.3f}s\\n\"\n            f\"  Frames per Camera: {min_frames}-{max_frames}\\n\"\n            f\"  Total Duration: {total_time:.1f}s\"\n        )\n        \n        # Performance assertions for concurrent streaming\n        assert total_frames_processed >= expected_total_frames * 0.85, \"Too many dropped frames\"\n        assert overall_fps >= (camera_count * target_fps) * 0.8, \"Overall FPS too low\"\n        assert avg_processing_time <= 0.15, \"Processing time too high under load\"\n        \n        # Verify all cameras processed frames\n        active_cameras = sum(1 for count in frame_counts if isinstance(count, int) and count > 0)\n        assert active_cameras >= camera_count * 0.9, \"Some cameras failed to process frames\"\n\n    async def test_gpu_batch_optimization(\n        self, ml_integration_service, frame_generator, mock_vision_engine\n    ):\n        \"\"\"Test GPU batching optimization for improved throughput.\"\"\"\n        camera_count = 8  # Match typical batch size\n        frames_per_camera = 20\n        \n        camera_ids = [f\"batch_camera_{i:02d}\" for i in range(camera_count)]\n        \n        logger.info(f\"Testing GPU batch optimization: {camera_count} cameras, {frames_per_camera} frames\")\n        \n        # Generate frames for all cameras\n        all_frames = []\n        for camera_id in camera_ids:\n            for i in range(frames_per_camera):\n                frame = frame_generator.generate_frame(camera_id)\n                frame[\"batch_test\"] = True\n                all_frames.append(frame)\n        \n        # Test individual processing (no batching)\n        individual_start = time.time()\n        individual_results = []\n        \n        for frame in all_frames[:10]:  # Test subset for comparison\n            result = await mock_vision_engine.process_frame(\n                frame[\"frame_data\"], frame[\"metadata\"]\n            )\n            individual_results.append(result)\n        \n        individual_time = time.time() - individual_start\n        individual_fps = len(individual_results) / individual_time\n        \n        # Test batch processing\n        batch_start = time.time()\n        batch_frames = all_frames[:8]  # Full batch\n        \n        frame_data_list = [frame[\"frame_data\"] for frame in batch_frames]\n        metadata_list = [frame[\"metadata\"] for frame in batch_frames]\n        \n        batch_results = await mock_vision_engine.process_batch(frame_data_list, metadata_list)\n        batch_time = time.time() - batch_start\n        batch_fps = len(batch_results) / batch_time\n        \n        logger.info(\n            f\"GPU Batch Optimization Results:\\n\"\n            f\"  Individual Processing: {individual_fps:.1f} FPS\\n\"\n            f\"  Batch Processing: {batch_fps:.1f} FPS\\n\"\n            f\"  Speedup: {batch_fps / individual_fps:.1f}x\\n\"\n            f\"  Batch Efficiency: {(batch_fps / individual_fps) / len(batch_frames):.1%}\"\n        )\n        \n        # Validate batch optimization\n        assert len(batch_results) == len(batch_frames), \"Batch size mismatch\"\n        assert batch_fps > individual_fps, \"Batch processing should be faster\"\n        assert batch_fps / individual_fps >= 2.0, \"Insufficient batch speedup\"\n        \n        # Validate batch result consistency\n        for i, result in enumerate(batch_results):\n            assert result[\"batch_size\"] == len(batch_frames)\n            assert result[\"batch_position\"] == i\n            assert \"batch_id\" in result\n\n    async def test_stream_synchronization_and_ordering(\n        self, ml_integration_service, frame_generator, redis_client\n    ):\n        \"\"\"Test frame ordering and synchronization across streams.\"\"\"\n        camera_id = \"sync_test_camera\"\n        frame_count = 50\n        \n        logger.info(f\"Testing stream synchronization: {frame_count} frames\")\n        \n        # Generate frames with sequence numbers\n        frames = []\n        for i in range(frame_count):\n            frame = frame_generator.generate_frame(camera_id)\n            frame[\"sequence_number\"] = i\n            frame[\"metadata\"][\"sequence_number\"] = i\n            frames.append(frame)\n        \n        # Process frames with artificial delays to test ordering\n        processed_results = []\n        processing_tasks = []\n        \n        async def process_with_delay(frame: Dict[str, Any], delay: float):\n            \"\"\"Process frame with artificial delay.\"\"\"\n            await asyncio.sleep(delay)\n            \n            result = await ml_integration_service.process_frame(\n                camera_id=frame[\"camera_id\"],\n                frame_data=frame[\"frame_data\"],\n                metadata=frame[\"metadata\"]\n            )\n            \n            result[\"sequence_number\"] = frame[\"sequence_number\"]\n            return result\n        \n        # Submit frames with varying delays to simulate real-world conditions\n        for i, frame in enumerate(frames):\n            # Add random delay to simulate network/processing variations\n            delay = random.uniform(0.01, 0.05)\n            task = process_with_delay(frame, delay)\n            processing_tasks.append(task)\n        \n        # Execute processing\n        start_time = time.time()\n        results = await asyncio.gather(*processing_tasks)\n        total_time = time.time() - start_time\n        \n        # Check sequence ordering\n        result_sequences = [result[\"sequence_number\"] for result in results]\n        expected_sequences = list(range(frame_count))\n        \n        # Verify all frames processed\n        assert len(results) == frame_count, f\"Frame count mismatch: {len(results)}/{frame_count}\"\n        \n        # Check for missing frames\n        missing_frames = set(expected_sequences) - set(result_sequences)\n        assert not missing_frames, f\"Missing frames: {missing_frames}\"\n        \n        # Performance validation\n        fps = frame_count / total_time\n        assert fps >= 50, f\"Processing too slow: {fps:.1f} FPS\"\n        \n        logger.info(\n            f\"Stream Synchronization Results:\\n\"\n            f\"  Frames Processed: {len(results)}\\n\"\n            f\"  Processing FPS: {fps:.1f}\\n\"\n            f\"  Total Time: {total_time:.2f}s\\n\"\n            f\"  Missing Frames: {len(missing_frames)}\"\n        )\n        \n        # Test Redis stream ordering\n        stream_key = f\"camera_stream:{camera_id}\"\n        \n        # Verify frames are stored in order\n        stream_length = await redis_client.xlen(stream_key)\n        if stream_length > 0:\n            stream_data = await redis_client.xrange(stream_key, count=10)\n            \n            # Validate stream data structure\n            assert len(stream_data) > 0, \"No stream data found\"\n            \n            for entry_id, fields in stream_data:\n                assert b\"frame_id\" in fields\n                assert b\"timestamp\" in fields\n\n    async def test_memory_management_under_load(\n        self, ml_integration_service, frame_generator\n    ):\n        \"\"\"Test memory management during extended streaming.\"\"\"\n        import psutil\n        import gc\n        \n        camera_count = 5\n        stream_duration = 10  # 10 seconds\n        target_fps = 20\n        \n        camera_ids = [f\"memory_test_camera_{i:02d}\" for i in range(camera_count)]\n        \n        logger.info(f\"Testing memory management: {camera_count} cameras, {stream_duration}s\")\n        \n        # Monitor memory usage\n        initial_memory = psutil.virtual_memory().used / 1024 / 1024  # MB\n        memory_samples = [initial_memory]\n        \n        async def memory_monitor():\n            \"\"\"Monitor memory usage during test.\"\"\"\n            while True:\n                current_memory = psutil.virtual_memory().used / 1024 / 1024\n                memory_samples.append(current_memory)\n                await asyncio.sleep(1)  # Sample every second\n        \n        async def process_camera_with_memory_tracking(camera_id: str):\n            \"\"\"Process camera stream while tracking memory.\"\"\"\n            frame_count = 0\n            \n            async for frame in frame_generator.stream_frames(camera_id, stream_duration, target_fps):\n                try:\n                    result = await ml_integration_service.process_frame(\n                        camera_id=frame[\"camera_id\"],\n                        frame_data=frame[\"frame_data\"],\n                        metadata=frame[\"metadata\"]\n                    )\n                    \n                    frame_count += 1\n                    \n                    # Periodic garbage collection\n                    if frame_count % 50 == 0:\n                        gc.collect()\n                \n                except Exception as e:\n                    logger.error(f\"Memory test processing error: {e}\")\n            \n            return frame_count\n        \n        # Start memory monitoring\n        monitor_task = asyncio.create_task(memory_monitor())\n        \n        # Execute streaming with memory tracking\n        start_time = time.time()\n        \n        stream_tasks = [\n            process_camera_with_memory_tracking(camera_id) \n            for camera_id in camera_ids\n        ]\n        \n        frame_counts = await asyncio.gather(*stream_tasks, return_exceptions=True)\n        \n        # Stop memory monitoring\n        monitor_task.cancel()\n        \n        total_time = time.time() - start_time\n        final_memory = psutil.virtual_memory().used / 1024 / 1024\n        \n        # Memory analysis\n        memory_growth = final_memory - initial_memory\n        max_memory = max(memory_samples)\n        avg_memory = sum(memory_samples) / len(memory_samples)\n        \n        total_frames = sum(count for count in frame_counts if isinstance(count, int))\n        memory_per_frame = memory_growth / total_frames if total_frames > 0 else 0\n        \n        logger.info(\n            f\"Memory Management Results:\\n\"\n            f\"  Initial Memory: {initial_memory:.1f} MB\\n\"\n            f\"  Final Memory: {final_memory:.1f} MB\\n\"\n            f\"  Memory Growth: {memory_growth:.1f} MB\\n\"\n            f\"  Max Memory: {max_memory:.1f} MB\\n\"\n            f\"  Avg Memory: {avg_memory:.1f} MB\\n\"\n            f\"  Total Frames: {total_frames}\\n\"\n            f\"  Memory/Frame: {memory_per_frame:.3f} MB\"\n        )\n        \n        # Memory management assertions\n        assert memory_growth <= 500, f\"Excessive memory growth: {memory_growth:.1f} MB\"\n        assert memory_per_frame <= 1.0, f\"High memory per frame: {memory_per_frame:.3f} MB\"\n        assert max_memory - initial_memory <= 1000, \"Peak memory usage too high\"\n        \n        # Force cleanup\n        gc.collect()\n\n    async def test_error_recovery_and_resilience(\n        self, ml_integration_service, frame_generator, mock_vision_engine\n    ):\n        \"\"\"Test system resilience and error recovery during streaming.\"\"\"\n        camera_id = \"error_test_camera\"\n        frame_count = 30\n        error_injection_rate = 0.2  # 20% of frames will have errors\n        \n        logger.info(f\"Testing error recovery: {frame_count} frames, {error_injection_rate:.0%} error rate\")\n        \n        # Inject errors into vision engine\n        original_process = mock_vision_engine.process_frame\n        \n        async def error_prone_process(frame_data: bytes, metadata: Dict[str, Any]):\n            \"\"\"Process with random errors.\"\"\"\n            if random.random() < error_injection_rate:\n                if random.choice([True, False]):\n                    raise RuntimeError(\"Simulated GPU memory error\")\n                else:\n                    raise TimeoutError(\"Simulated inference timeout\")\n            \n            return await original_process(frame_data, metadata)\n        \n        mock_vision_engine.process_frame = error_prone_process\n        \n        # Process frames with error injection\n        successful_frames = 0\n        error_count = 0\n        recovery_times = []\n        \n        for i in range(frame_count):\n            frame = frame_generator.generate_frame(camera_id)\n            \n            try:\n                start_time = time.time()\n                \n                result = await ml_integration_service.process_frame(\n                    camera_id=frame[\"camera_id\"],\n                    frame_data=frame[\"frame_data\"],\n                    metadata=frame[\"metadata\"]\n                )\n                \n                processing_time = time.time() - start_time\n                recovery_times.append(processing_time)\n                successful_frames += 1\n                \n            except Exception as e:\n                error_count += 1\n                logger.debug(f\"Expected error during resilience test: {e}\")\n        \n        # Restore original function\n        mock_vision_engine.process_frame = original_process\n        \n        # Analyze resilience results\n        success_rate = successful_frames / frame_count\n        expected_errors = frame_count * error_injection_rate\n        actual_error_rate = error_count / frame_count\n        avg_recovery_time = sum(recovery_times) / len(recovery_times) if recovery_times else 0\n        \n        logger.info(\n            f\"Error Recovery Results:\\n\"\n            f\"  Successful Frames: {successful_frames}/{frame_count}\\n\"\n            f\"  Success Rate: {success_rate:.1%}\\n\"\n            f\"  Error Count: {error_count}\\n\"\n            f\"  Expected Errors: {expected_errors:.0f}\\n\"\n            f\"  Actual Error Rate: {actual_error_rate:.1%}\\n\"\n            f\"  Avg Recovery Time: {avg_recovery_time:.3f}s\"\n        )\n        \n        # Resilience assertions\n        assert success_rate >= 0.7, f\"Success rate too low: {success_rate:.1%}\"\n        assert abs(actual_error_rate - error_injection_rate) <= 0.1, \"Error rate unexpected\"\n        assert avg_recovery_time <= 0.2, f\"Recovery time too high: {avg_recovery_time:.3f}s\"\n        \n        # Test continued operation after errors\n        post_error_frames = 5\n        post_error_success = 0\n        \n        for i in range(post_error_frames):\n            frame = frame_generator.generate_frame(camera_id)\n            try:\n                await ml_integration_service.process_frame(\n                    camera_id=frame[\"camera_id\"],\n                    frame_data=frame[\"frame_data\"],\n                    metadata=frame[\"metadata\"]\n                )\n                post_error_success += 1\n            except Exception:\n                pass\n        \n        post_error_rate = post_error_success / post_error_frames\n        assert post_error_rate >= 0.8, f\"Poor recovery: {post_error_rate:.1%}\"\n\n    async def test_background_worker_integration(\n        self, ml_integration_service, frame_generator, redis_client\n    ):\n        \"\"\"Test integration with background workers for analytics processing.\"\"\"\n        camera_id = \"worker_test_camera\"\n        frame_count = 20\n        \n        logger.info(f\"Testing background worker integration: {frame_count} frames\")\n        \n        # Mock analytics worker\n        analytics_results = []\n        \n        async def mock_analytics_worker(detection_data: DetectionData):\n            \"\"\"Mock analytics processing worker.\"\"\"\n            await asyncio.sleep(0.02)  # Simulate analytics processing\n            \n            analytics_result = {\n                \"camera_id\": detection_data.camera_id,\n                \"timestamp\": detection_data.timestamp.isoformat(),\n                \"vehicle_count\": detection_data.vehicle_count,\n                \"processing_time_ms\": 20.0,\n                \"violations\": [],\n                \"anomalies\": [],\n                \"worker_processed\": True\n            }\n            \n            analytics_results.append(analytics_result)\n            return analytics_result\n        \n        # Process frames and trigger worker processing\n        detection_results = []\n        \n        for i in range(frame_count):\n            frame = frame_generator.generate_frame(camera_id)\n            \n            # ML processing\n            ml_result = await ml_integration_service.process_frame(\n                camera_id=frame[\"camera_id\"],\n                frame_data=frame[\"frame_data\"],\n                metadata=frame[\"metadata\"]\n            )\n            \n            # Convert to DetectionData for worker\n            detection_data = DetectionData(\n                camera_id=camera_id,\n                timestamp=frame[\"timestamp\"],\n                frame_id=frame[\"frame_id\"],\n                vehicle_count=len(ml_result.get(\"detections\", [])),\n                detections=[],  # Simplified for test\n                processing_metadata={\n                    \"model_version\": ml_result.get(\"model_version\", \"test\"),\n                    \"inference_time_ms\": ml_result.get(\"inference_time_ms\", 0)\n                }\n            )\n            \n            detection_results.append(detection_data)\n            \n            # Trigger analytics worker\n            analytics_result = await mock_analytics_worker(detection_data)\n        \n        # Validate worker integration\n        assert len(analytics_results) == frame_count, \"Worker processing count mismatch\"\n        assert len(detection_results) == frame_count, \"Detection results count mismatch\"\n        \n        # Verify all results have worker processing flag\n        worker_processed_count = sum(\n            1 for result in analytics_results \n            if result.get(\"worker_processed\", False)\n        )\n        \n        assert worker_processed_count == frame_count, \"Not all frames processed by worker\"\n        \n        # Verify Redis queue operations\n        queue_key = f\"analytics_queue:{camera_id}\"\n        queue_length = await redis_client.llen(queue_key)\n        \n        logger.info(\n            f\"Background Worker Integration Results:\\n\"\n            f\"  ML Results: {len(detection_results)}\\n\"\n            f\"  Analytics Results: {len(analytics_results)}\\n\"\n            f\"  Worker Processed: {worker_processed_count}\\n\"\n            f\"  Queue Length: {queue_length}\"\n        )\n        \n        # Test queue processing\n        if queue_length > 0:\n            queued_item = await redis_client.lpop(queue_key)\n            assert queued_item is not None, \"Queue should contain items\"\n            \n            try:\n                queue_data = json.loads(queued_item)\n                assert \"camera_id\" in queue_data\n                assert \"timestamp\" in queue_data\n            except json.JSONDecodeError:\n                pytest.fail(\"Invalid JSON in queue\")