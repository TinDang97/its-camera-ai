"""Cache Integration Tests for Multi-Level Caching.

This test suite validates the multi-level caching system:
- Redis L2 cache performance and reliability
- In-memory L1 cache efficiency
- Cache hierarchy and fallback mechanisms
- Cache invalidation and consistency
- High-throughput cache operations
- Memory management and eviction policies
- Cache warming and preloading strategies
"""

import asyncio
import json
import random
import statistics
import time
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, patch
from uuid import uuid4

import pytest
import pytest_asyncio
import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession

from its_camera_ai.core.logging import get_logger
from its_camera_ai.services.cache import CacheService
from its_camera_ai.services.analytics_dtos import (
    ProcessingResult,
    RealtimeTrafficMetrics,
    CongestionLevel,
)
from its_camera_ai.performance.streaming_cache_manager import StreamingCacheManager

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.performance
@pytest.mark.asyncio
class TestCacheIntegration:
    """Integration tests for multi-level caching system."""

    @pytest_asyncio.fixture
    async def cache_service(self, redis_client: redis.Redis, test_settings):
        """Cache service with Redis backend."""
        service = CacheService(redis_client=redis_client, settings=test_settings)
        return service

    @pytest_asyncio.fixture
    async def streaming_cache_manager(self, redis_client: redis.Redis):
        """Streaming cache manager for real-time data."""
        manager = StreamingCacheManager(
            redis_client=redis_client,
            l1_cache_size=1000,
            l2_cache_ttl=300,
            preload_enabled=True
        )
        return manager

    @pytest_asyncio.fixture
    async def sample_cache_data(self):
        """Generate sample data for cache testing."""
        camera_ids = [f\"cache_test_camera_{i:02d}\" for i in range(20)]
        
        cache_data = {}
        for camera_id in camera_ids:
            # Real-time analytics data
            analytics_data = {
                \"camera_id\": camera_id,\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"total_vehicles\": random.randint(0, 30),\n                \"vehicle_breakdown\": {\n                    \"car\": random.randint(0, 20),\n                    \"truck\": random.randint(0, 5),\n                    \"bus\": random.randint(0, 3),\n                    \"motorcycle\": random.randint(0, 8)\n                },\n                \"average_speed\": random.uniform(20.0, 80.0),\n                \"traffic_density\": random.uniform(0.0, 1.0),\n                \"congestion_level\": random.choice([\"free_flow\", \"light\", \"moderate\", \"heavy\", \"severe\"]),\n                \"processing_time_ms\": random.uniform(30.0, 100.0),\n                \"quality_score\": random.uniform(0.7, 1.0)\n            }\n            \n            # Historical aggregated data\n            historical_data = {\n                \"camera_id\": camera_id,\n                \"time_range\": \"last_24h\",\n                \"hourly_averages\": [\n                    {\n                        \"hour\": hour,\n                        \"avg_vehicles\": random.randint(5, 25),\n                        \"avg_speed\": random.uniform(30.0, 70.0),\n                        \"peak_vehicles\": random.randint(10, 40)\n                    }\n                    for hour in range(24)\n                ],\n                \"daily_summary\": {\n                    \"total_vehicles\": random.randint(200, 800),\n                    \"avg_speed\": random.uniform(35.0, 65.0),\n                    \"peak_hour\": random.randint(7, 19),\n                    \"congestion_events\": random.randint(0, 15)\n                }\n            }\n            \n            cache_data[camera_id] = {\n                \"realtime\": analytics_data,\n                \"historical\": historical_data\n            }\n        \n        return cache_data\n\n    async def test_redis_l2_cache_basic_operations(\n        self, cache_service: CacheService\n    ):\n        \"\"\"Test basic Redis L2 cache operations.\"\"\"\n        \n        logger.info(\"Testing Redis L2 cache basic operations\")\n        \n        test_key = \"test_basic_operations\"\n        test_data = {\n            \"camera_id\": \"test_camera_001\",\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"data\": \"test_value\",\n            \"number\": 42,\n            \"nested\": {\"key\": \"value\", \"list\": [1, 2, 3]}\n        }\n        \n        # Test SET operation\n        set_start = time.time()\n        await cache_service.set_json(test_key, test_data, ttl=300)\n        set_time = time.time() - set_start\n        \n        # Test GET operation\n        get_start = time.time()\n        retrieved_data = await cache_service.get_json(test_key)\n        get_time = time.time() - get_start\n        \n        # Validate data integrity\n        assert retrieved_data is not None, \"Data should be retrieved from cache\"\n        assert retrieved_data[\"camera_id\"] == test_data[\"camera_id\"]\n        assert retrieved_data[\"data\"] == test_data[\"data\"]\n        assert retrieved_data[\"number\"] == test_data[\"number\"]\n        assert retrieved_data[\"nested\"][\"key\"] == test_data[\"nested\"][\"key\"]\n        \n        # Test TTL\n        ttl = await cache_service.get_ttl(test_key)\n        assert 290 <= ttl <= 300, f\"TTL should be around 300s, got {ttl}s\"\n        \n        # Test EXISTS\n        exists = await cache_service.exists(test_key)\n        assert exists, \"Key should exist in cache\"\n        \n        # Test DELETE\n        deleted = await cache_service.delete(test_key)\n        assert deleted, \"Key should be deleted\"\n        \n        # Verify deletion\n        retrieved_after_delete = await cache_service.get_json(test_key)\n        assert retrieved_after_delete is None, \"Data should not exist after deletion\"\n        \n        logger.info(\n            f\"Basic Cache Operations Results:\\n\"\n            f\"  SET Time: {set_time:.4f}s\\n\"\n            f\"  GET Time: {get_time:.4f}s\\n\"\n            f\"  Data Integrity: Passed\\n\"\n            f\"  TTL Management: Passed\"\n        )\n        \n        # Performance assertions\n        assert set_time < 0.01, f\"SET operation too slow: {set_time:.4f}s\"\n        assert get_time < 0.005, f\"GET operation too slow: {get_time:.4f}s\"\n\n    async def test_high_throughput_cache_operations(\n        self, cache_service: CacheService, sample_cache_data: Dict[str, Any]\n    ):\n        \"\"\"Test cache performance under high throughput.\"\"\"\n        \n        logger.info(\"Testing high throughput cache operations\")\n        \n        operations_count = 1000\n        concurrent_operations = 50\n        \n        # Prepare test data\n        test_keys = [f\"throughput_test_{i:04d}\" for i in range(operations_count)]\n        test_data_list = [\n            {\n                \"key\": key,\n                \"data\": {\n                    \"id\": i,\n                    \"timestamp\": datetime.now(UTC).isoformat(),\n                    \"payload\": f\"test_data_{i}\",\n                    \"metrics\": random.choice(list(sample_cache_data.values()))[\"realtime\"]\n                }\n            }\n            for i, key in enumerate(test_keys)\n        ]\n        \n        # Test concurrent SET operations\n        async def perform_set_operations(batch_data: List[Dict[str, Any]]):\n            \"\"\"Perform batch SET operations.\"\"\"\n            set_times = []\n            \n            for item in batch_data:\n                start_time = time.time()\n                await cache_service.set_json(item[\"key\"], item[\"data\"], ttl=300)\n                operation_time = time.time() - start_time\n                set_times.append(operation_time)\n            \n            return set_times\n        \n        # Split data into batches for concurrent processing\n        batch_size = operations_count // concurrent_operations\n        batches = [\n            test_data_list[i:i + batch_size]\n            for i in range(0, operations_count, batch_size)\n        ]\n        \n        # Execute concurrent SET operations\n        set_start = time.time()\n        set_tasks = [perform_set_operations(batch) for batch in batches]\n        set_results = await asyncio.gather(*set_tasks)\n        total_set_time = time.time() - set_start\n        \n        # Flatten results\n        all_set_times = [time for batch_times in set_results for time in batch_times]\n        \n        # Test concurrent GET operations\n        async def perform_get_operations(keys: List[str]):\n            \"\"\"Perform batch GET operations.\"\"\"\n            get_times = []\n            successful_gets = 0\n            \n            for key in keys:\n                start_time = time.time()\n                data = await cache_service.get_json(key)\n                operation_time = time.time() - start_time\n                get_times.append(operation_time)\n                \n                if data is not None:\n                    successful_gets += 1\n            \n            return get_times, successful_gets\n        \n        # Split keys into batches\n        key_batches = [\n            test_keys[i:i + batch_size]\n            for i in range(0, len(test_keys), batch_size)\n        ]\n        \n        # Execute concurrent GET operations\n        get_start = time.time()\n        get_tasks = [perform_get_operations(batch) for batch in key_batches]\n        get_results = await asyncio.gather(*get_tasks)\n        total_get_time = time.time() - get_start\n        \n        # Process GET results\n        all_get_times = []\n        total_successful_gets = 0\n        \n        for get_times, successful_gets in get_results:\n            all_get_times.extend(get_times)\n            total_successful_gets += successful_gets\n        \n        # Calculate performance metrics\n        set_ops_per_second = len(all_set_times) / total_set_time\n        get_ops_per_second = len(all_get_times) / total_get_time\n        \n        avg_set_time = statistics.mean(all_set_times)\n        avg_get_time = statistics.mean(all_get_times)\n        \n        p95_set_time = statistics.quantiles(all_set_times, n=20)[18]\n        p95_get_time = statistics.quantiles(all_get_times, n=20)[18]\n        \n        success_rate = total_successful_gets / len(test_keys)\n        \n        logger.info(\n            f\"High Throughput Cache Results:\\n\"\n            f\"  SET Operations: {len(all_set_times)} in {total_set_time:.2f}s\\n\"\n            f\"  SET Ops/Second: {set_ops_per_second:.1f}\\n\"\n            f\"  Avg SET Time: {avg_set_time:.4f}s\\n\"\n            f\"  P95 SET Time: {p95_set_time:.4f}s\\n\"\n            f\"  GET Operations: {len(all_get_times)} in {total_get_time:.2f}s\\n\"\n            f\"  GET Ops/Second: {get_ops_per_second:.1f}\\n\"\n            f\"  Avg GET Time: {avg_get_time:.4f}s\\n\"\n            f\"  P95 GET Time: {p95_get_time:.4f}s\\n\"\n            f\"  Success Rate: {success_rate:.1%}\"\n        )\n        \n        # Performance assertions\n        assert set_ops_per_second >= 1000, f\"SET throughput too low: {set_ops_per_second:.1f} ops/s\"\n        assert get_ops_per_second >= 2000, f\"GET throughput too low: {get_ops_per_second:.1f} ops/s\"\n        assert avg_set_time <= 0.05, f\"Average SET time too high: {avg_set_time:.4f}s\"\n        assert avg_get_time <= 0.01, f\"Average GET time too high: {avg_get_time:.4f}s\"\n        assert p95_set_time <= 0.1, f\"P95 SET time too high: {p95_set_time:.4f}s\"\n        assert p95_get_time <= 0.02, f\"P95 GET time too high: {p95_get_time:.4f}s\"\n        assert success_rate >= 0.99, f\"Success rate too low: {success_rate:.1%}\"\n        \n        # Cleanup\n        await asyncio.gather(*[cache_service.delete(key) for key in test_keys[:100]])  # Cleanup subset\n\n    async def test_multi_level_cache_hierarchy(\n        self, streaming_cache_manager: StreamingCacheManager, sample_cache_data: Dict[str, Any]\n    ):\n        \"\"\"Test multi-level cache hierarchy (L1 memory + L2 Redis).\"\"\"\n        \n        logger.info(\"Testing multi-level cache hierarchy\")\n        \n        camera_ids = list(sample_cache_data.keys())[:10]  # Test with 10 cameras\n        \n        # Test L1 cache (in-memory) operations\n        l1_set_times = []\n        l1_get_times = []\n        \n        for camera_id in camera_ids:\n            realtime_data = sample_cache_data[camera_id][\"realtime\"]\n            \n            # L1 SET\n            start_time = time.time()\n            streaming_cache_manager.set_l1_cache(f\"realtime:{camera_id}\", realtime_data)\n            l1_set_time = time.time() - start_time\n            l1_set_times.append(l1_set_time)\n            \n            # L1 GET\n            start_time = time.time()\n            l1_data = streaming_cache_manager.get_l1_cache(f\"realtime:{camera_id}\")\n            l1_get_time = time.time() - start_time\n            l1_get_times.append(l1_get_time)\n            \n            assert l1_data is not None, f\"L1 cache miss for {camera_id}\"\n            assert l1_data[\"camera_id\"] == camera_id, \"L1 data integrity issue\"\n        \n        # Test L2 cache (Redis) operations\n        l2_set_times = []\n        l2_get_times = []\n        \n        for camera_id in camera_ids:\n            historical_data = sample_cache_data[camera_id][\"historical\"]\n            \n            # L2 SET\n            start_time = time.time()\n            await streaming_cache_manager.set_l2_cache(\n                f\"historical:{camera_id}\", historical_data, ttl=600\n            )\n            l2_set_time = time.time() - start_time\n            l2_set_times.append(l2_set_time)\n            \n            # L2 GET\n            start_time = time.time()\n            l2_data = await streaming_cache_manager.get_l2_cache(f\"historical:{camera_id}\")\n            l2_get_time = time.time() - start_time\n            l2_get_times.append(l2_get_time)\n            \n            assert l2_data is not None, f\"L2 cache miss for {camera_id}\"\n            assert l2_data[\"camera_id\"] == camera_id, \"L2 data integrity issue\"\n        \n        # Test cache hierarchy fallback\n        test_key = \"hierarchy_test_key\"\n        test_data = {\"test\": \"hierarchy_data\", \"timestamp\": datetime.now(UTC).isoformat()}\n        \n        # Set in L2 only\n        await streaming_cache_manager.set_l2_cache(test_key, test_data, ttl=300)\n        \n        # Get with hierarchy (should fallback to L2 and promote to L1)\n        hierarchy_start = time.time()\n        hierarchy_data = await streaming_cache_manager.get_with_fallback(test_key)\n        hierarchy_time = time.time() - hierarchy_start\n        \n        assert hierarchy_data is not None, \"Hierarchy fallback failed\"\n        assert hierarchy_data[\"test\"] == \"hierarchy_data\", \"Hierarchy data integrity issue\"\n        \n        # Verify data was promoted to L1\n        l1_promoted_data = streaming_cache_manager.get_l1_cache(test_key)\n        assert l1_promoted_data is not None, \"Data should be promoted to L1\"\n        \n        # Performance analysis\n        avg_l1_set = statistics.mean(l1_set_times)\n        avg_l1_get = statistics.mean(l1_get_times)\n        avg_l2_set = statistics.mean(l2_set_times)\n        avg_l2_get = statistics.mean(l2_get_times)\n        \n        logger.info(\n            f\"Multi-Level Cache Hierarchy Results:\\n\"\n            f\"  L1 Cache - Avg SET: {avg_l1_set:.6f}s, Avg GET: {avg_l1_get:.6f}s\\n\"\n            f\"  L2 Cache - Avg SET: {avg_l2_set:.4f}s, Avg GET: {avg_l2_get:.4f}s\\n\"\n            f\"  Hierarchy Fallback: {hierarchy_time:.4f}s\\n\"\n            f\"  L1 vs L2 GET Speedup: {avg_l2_get / avg_l1_get:.1f}x\"\n        )\n        \n        # Performance assertions\n        assert avg_l1_set <= 0.0001, f\"L1 SET too slow: {avg_l1_set:.6f}s\"\n        assert avg_l1_get <= 0.0001, f\"L1 GET too slow: {avg_l1_get:.6f}s\"\n        assert avg_l2_set <= 0.01, f\"L2 SET too slow: {avg_l2_set:.4f}s\"\n        assert avg_l2_get <= 0.005, f\"L2 GET too slow: {avg_l2_get:.4f}s\"\n        assert hierarchy_time <= 0.01, f\"Hierarchy fallback too slow: {hierarchy_time:.4f}s\"\n        \n        # L1 should be significantly faster than L2\n        assert avg_l2_get / avg_l1_get >= 10, \"L1 cache not providing sufficient speedup\"\n\n    async def test_cache_invalidation_and_consistency(\n        self, cache_service: CacheService, streaming_cache_manager: StreamingCacheManager\n    ):\n        \"\"\"Test cache invalidation strategies and data consistency.\"\"\"\n        \n        logger.info(\"Testing cache invalidation and consistency\")\n        \n        camera_id = \"invalidation_test_camera\"\n        \n        # Initial data\n        initial_data = {\n            \"camera_id\": camera_id,\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"version\": 1,\n            \"vehicle_count\": 15,\n            \"status\": \"active\"\n        }\n        \n        # Set data in both cache levels\n        l1_key = f\"realtime:{camera_id}\"\n        l2_key = f\"realtime:{camera_id}\"\n        \n        streaming_cache_manager.set_l1_cache(l1_key, initial_data)\n        await streaming_cache_manager.set_l2_cache(l2_key, initial_data, ttl=300)\n        \n        # Verify data consistency\n        l1_data = streaming_cache_manager.get_l1_cache(l1_key)\n        l2_data = await streaming_cache_manager.get_l2_cache(l2_key)\n        \n        assert l1_data[\"version\"] == l2_data[\"version\"], \"Initial data inconsistency\"\n        \n        # Test selective invalidation\n        updated_data = {\n            \"camera_id\": camera_id,\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"version\": 2,\n            \"vehicle_count\": 25,\n            \"status\": \"active\"\n        }\n        \n        # Update L2 and invalidate L1\n        await streaming_cache_manager.set_l2_cache(l2_key, updated_data, ttl=300)\n        streaming_cache_manager.invalidate_l1_cache(l1_key)\n        \n        # Verify L1 invalidation\n        l1_after_invalidation = streaming_cache_manager.get_l1_cache(l1_key)\n        assert l1_after_invalidation is None, \"L1 cache should be invalidated\"\n        \n        # Verify L2 still has updated data\n        l2_after_update = await streaming_cache_manager.get_l2_cache(l2_key)\n        assert l2_after_update[\"version\"] == 2, \"L2 should have updated data\"\n        \n        # Test pattern-based invalidation\n        pattern_keys = [f\"pattern_test:{i}\" for i in range(10)]\n        pattern_data = [{\"id\": i, \"data\": f\"test_{i}\"} for i in range(10)]\n        \n        # Set multiple keys with pattern\n        for key, data in zip(pattern_keys, pattern_data):\n            await cache_service.set_json(key, data, ttl=300)\n        \n        # Verify all keys exist\n        for key in pattern_keys:\n            exists = await cache_service.exists(key)\n            assert exists, f\"Key {key} should exist\"\n        \n        # Pattern-based invalidation\n        invalidated_count = await cache_service.delete_pattern(\"pattern_test:*\")\n        assert invalidated_count == len(pattern_keys), f\"Should invalidate {len(pattern_keys)} keys\"\n        \n        # Verify pattern invalidation\n        for key in pattern_keys:\n            exists = await cache_service.exists(key)\n            assert not exists, f\"Key {key} should be invalidated\"\n        \n        # Test TTL-based consistency\n        ttl_test_key = \"ttl_consistency_test\"\n        ttl_data = {\"expires_soon\": True, \"timestamp\": datetime.now(UTC).isoformat()}\n        \n        # Set with short TTL\n        await cache_service.set_json(ttl_test_key, ttl_data, ttl=2)\n        \n        # Verify data exists\n        ttl_retrieved = await cache_service.get_json(ttl_test_key)\n        assert ttl_retrieved is not None, \"TTL data should exist initially\"\n        \n        # Wait for expiration\n        await asyncio.sleep(3)\n        \n        # Verify automatic expiration\n        ttl_after_expiry = await cache_service.get_json(ttl_test_key)\n        assert ttl_after_expiry is None, \"TTL data should expire automatically\"\n        \n        logger.info(\n            \"Cache Invalidation and Consistency Results:\\n\"\n            \"  Selective Invalidation: Passed\\n\"\n            \"  Pattern-based Invalidation: Passed\\n\"\n            \"  TTL-based Expiration: Passed\\n\"\n            \"  Multi-level Consistency: Passed\"\n        )\n\n    async def test_cache_warming_and_preloading(\n        self, streaming_cache_manager: StreamingCacheManager, sample_cache_data: Dict[str, Any]\n    ):\n        \"\"\"Test cache warming and preloading strategies.\"\"\"\n        \n        logger.info(\"Testing cache warming and preloading\")\n        \n        # Test cold cache scenario\n        cold_camera_ids = [f\"cold_cache_camera_{i:02d}\" for i in range(5)]\n        \n        # Measure cold cache performance\n        cold_cache_times = []\n        \n        for camera_id in cold_camera_ids:\n            # Simulate cold cache access\n            start_time = time.time()\n            \n            # Data not in cache, simulate database fetch\n            await asyncio.sleep(0.05)  # Simulate DB query time\n            \n            db_data = sample_cache_data[list(sample_cache_data.keys())[0]][\"realtime\"]\n            db_data[\"camera_id\"] = camera_id\n            \n            # Cache the data\n            await streaming_cache_manager.set_l2_cache(f\"realtime:{camera_id}\", db_data, ttl=300)\n            streaming_cache_manager.set_l1_cache(f\"realtime:{camera_id}\", db_data)\n            \n            cold_time = time.time() - start_time\n            cold_cache_times.append(cold_time)\n        \n        # Test cache warming strategy\n        warm_camera_ids = [f\"warm_cache_camera_{i:02d}\" for i in range(5)]\n        \n        # Pre-warm the cache\n        warming_start = time.time()\n        \n        warming_tasks = []\n        for camera_id in warm_camera_ids:\n            async def warm_camera_cache(cam_id):\n                data = sample_cache_data[list(sample_cache_data.keys())[0]][\"realtime\"]\n                data[\"camera_id\"] = cam_id\n                \n                # Warm both cache levels\n                await streaming_cache_manager.set_l2_cache(f\"realtime:{cam_id}\", data, ttl=300)\n                streaming_cache_manager.set_l1_cache(f\"realtime:{cam_id}\", data)\n                \n                # Also warm historical data\n                historical = sample_cache_data[list(sample_cache_data.keys())[0]][\"historical\"]\n                historical[\"camera_id\"] = cam_id\n                await streaming_cache_manager.set_l2_cache(f\"historical:{cam_id}\", historical, ttl=600)\n            \n            warming_tasks.append(warm_camera_cache(camera_id))\n        \n        await asyncio.gather(*warming_tasks)\n        warming_time = time.time() - warming_start\n        \n        # Measure warm cache performance\n        warm_cache_times = []\n        \n        for camera_id in warm_camera_ids:\n            start_time = time.time()\n            \n            # Access warm cache\n            warm_data = await streaming_cache_manager.get_with_fallback(f\"realtime:{camera_id}\")\n            \n            warm_time = time.time() - start_time\n            warm_cache_times.append(warm_time)\n            \n            assert warm_data is not None, f\"Warm cache miss for {camera_id}\"\n            assert warm_data[\"camera_id\"] == camera_id, \"Warm cache data integrity issue\"\n        \n        # Test predictive preloading\n        prediction_camera_ids = [f\"prediction_camera_{i:02d}\" for i in range(3)]\n        \n        # Simulate access pattern analysis\n        access_patterns = {\n            camera_id: {\n                \"last_access\": datetime.now(UTC) - timedelta(minutes=random.randint(1, 30)),\n                \"access_frequency\": random.uniform(0.1, 1.0),\n                \"priority\": random.choice([\"high\", \"medium\", \"low\"])\n            }\n            for camera_id in prediction_camera_ids\n        }\n        \n        # Predictive preloading based on patterns\n        preload_start = time.time()\n        \n        high_priority_cameras = [\n            cam_id for cam_id, pattern in access_patterns.items()\n            if pattern[\"priority\"] == \"high\" or pattern[\"access_frequency\"] > 0.7\n        ]\n        \n        preload_tasks = []\n        for camera_id in high_priority_cameras:\n            async def preload_camera_data(cam_id):\n                # Preload multiple data types\n                realtime = sample_cache_data[list(sample_cache_data.keys())[0]][\"realtime\"]\n                historical = sample_cache_data[list(sample_cache_data.keys())[0]][\"historical\"]\n                \n                realtime[\"camera_id\"] = cam_id\n                historical[\"camera_id\"] = cam_id\n                \n                await streaming_cache_manager.set_l2_cache(f\"realtime:{cam_id}\", realtime, ttl=300)\n                await streaming_cache_manager.set_l2_cache(f\"historical:{cam_id}\", historical, ttl=600)\n                streaming_cache_manager.set_l1_cache(f\"realtime:{cam_id}\", realtime)\n            \n            preload_tasks.append(preload_camera_data(camera_id))\n        \n        await asyncio.gather(*preload_tasks)\n        preload_time = time.time() - preload_start\n        \n        # Performance analysis\n        avg_cold_time = statistics.mean(cold_cache_times)\n        avg_warm_time = statistics.mean(warm_cache_times)\n        speedup = avg_cold_time / avg_warm_time\n        \n        logger.info(\n            f\"Cache Warming and Preloading Results:\\n\"\n            f\"  Cold Cache Avg Time: {avg_cold_time:.4f}s\\n\"\n            f\"  Warm Cache Avg Time: {avg_warm_time:.4f}s\\n\"\n            f\"  Warm Cache Speedup: {speedup:.1f}x\\n\"\n            f\"  Cache Warming Time: {warming_time:.3f}s for {len(warm_camera_ids)} cameras\\n\"\n            f\"  Predictive Preloading: {preload_time:.3f}s for {len(high_priority_cameras)} cameras\"\n        )\n        \n        # Performance assertions\n        assert avg_warm_time <= 0.01, f\"Warm cache too slow: {avg_warm_time:.4f}s\"\n        assert speedup >= 5.0, f\"Insufficient warm cache speedup: {speedup:.1f}x\"\n        assert warming_time <= 2.0, f\"Cache warming too slow: {warming_time:.3f}s\"\n        assert preload_time <= 1.0, f\"Predictive preloading too slow: {preload_time:.3f}s\"\n\n    async def test_memory_management_and_eviction(\n        self, streaming_cache_manager: StreamingCacheManager\n    ):\n        \"\"\"Test cache memory management and LRU eviction policies.\"\"\"\n        \n        logger.info(\"Testing memory management and eviction policies\")\n        \n        # Test L1 cache size limits and LRU eviction\n        cache_capacity = streaming_cache_manager.l1_cache_size\n        \n        # Fill cache to capacity\n        test_data = []\n        for i in range(cache_capacity):\n            key = f\"memory_test_{i:04d}\"\n            data = {\n                \"id\": i,\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"payload\": f\"data_{i}\" * 100  # Larger payload\n            }\n            \n            streaming_cache_manager.set_l1_cache(key, data)\n            test_data.append((key, data))\n        \n        # Verify cache is full\n        cache_size = streaming_cache_manager.get_l1_cache_size()\n        assert cache_size == cache_capacity, f\"Cache should be full: {cache_size}/{cache_capacity}\"\n        \n        # Add more items to trigger eviction\n        eviction_items = 50\n        evicted_keys = []\n        \n        for i in range(cache_capacity, cache_capacity + eviction_items):\n            key = f\"memory_test_{i:04d}\"\n            data = {\n                \"id\": i,\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"payload\": f\"data_{i}\" * 100\n            }\n            \n            streaming_cache_manager.set_l1_cache(key, data)\n            evicted_keys.append(key)\n        \n        # Verify cache size remains at capacity\n        final_cache_size = streaming_cache_manager.get_l1_cache_size()\n        assert final_cache_size == cache_capacity, f\"Cache size should remain at capacity: {final_cache_size}\"\n        \n        # Verify LRU eviction (oldest items should be evicted)\n        oldest_keys = [f\"memory_test_{i:04d}\" for i in range(eviction_items)]\n        \n        evicted_count = 0\n        for key in oldest_keys:\n            if streaming_cache_manager.get_l1_cache(key) is None:\n                evicted_count += 1\n        \n        # Should have evicted approximately the number of items we added\n        eviction_rate = evicted_count / eviction_items\n        assert eviction_rate >= 0.8, f\"LRU eviction not working properly: {eviction_rate:.1%}\"\n        \n        # Test memory usage estimation\n        memory_usage = streaming_cache_manager.estimate_l1_memory_usage()\n        assert memory_usage > 0, \"Memory usage should be positive\"\n        \n        # Test cache statistics\n        stats = streaming_cache_manager.get_cache_statistics()\n        \n        assert \"l1_cache_size\" in stats\n        assert \"l1_hit_rate\" in stats\n        assert \"l1_miss_rate\" in stats\n        assert \"total_operations\" in stats\n        \n        # Access some items to generate hit/miss statistics\n        hits = 0\n        misses = 0\n        \n        # Access existing items (should be hits)\n        recent_keys = [f\"memory_test_{i:04d}\" for i in range(cache_capacity - 10, cache_capacity)]\n        for key in recent_keys:\n            if streaming_cache_manager.get_l1_cache(key) is not None:\n                hits += 1\n            else:\n                misses += 1\n        \n        # Access non-existent items (should be misses)\n        non_existent_keys = [f\"non_existent_{i}\" for i in range(10)]\n        for key in non_existent_keys:\n            if streaming_cache_manager.get_l1_cache(key) is not None:\n                hits += 1\n            else:\n                misses += 1\n        \n        total_operations = hits + misses\n        hit_rate = hits / total_operations if total_operations > 0 else 0\n        \n        logger.info(\n            f\"Memory Management and Eviction Results:\\n\"\n            f\"  Cache Capacity: {cache_capacity}\\n\"\n            f\"  Final Cache Size: {final_cache_size}\\n\"\n            f\"  Eviction Rate: {eviction_rate:.1%}\\n\"\n            f\"  Memory Usage Estimate: {memory_usage} bytes\\n\"\n            f\"  Hit Rate: {hit_rate:.1%}\\n\"\n            f\"  Cache Statistics: {stats}\"\n        )\n        \n        # Performance assertions\n        assert eviction_rate >= 0.8, \"LRU eviction policy not effective\"\n        assert final_cache_size <= cache_capacity, \"Cache size exceeded capacity\"\n        assert memory_usage > 0, \"Memory usage estimation failed\"\n        \n        # Clear cache for cleanup\n        streaming_cache_manager.clear_l1_cache()\n        cleared_size = streaming_cache_manager.get_l1_cache_size()\n        assert cleared_size == 0, \"Cache should be empty after clear\"\n\n    async def test_cache_resilience_and_failover(\n        self, cache_service: CacheService, redis_client: redis.Redis\n    ):\n        \"\"\"Test cache resilience and failover mechanisms.\"\"\"\n        \n        logger.info(\"Testing cache resilience and failover\")\n        \n        # Test data\n        resilience_data = {\n            \"test_id\": \"resilience_test\",\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"critical_data\": \"important_value\",\n            \"backup_needed\": True\n        }\n        \n        test_key = \"resilience_test_key\"\n        \n        # Store data normally\n        await cache_service.set_json(test_key, resilience_data, ttl=300)\n        \n        # Verify normal operation\n        normal_data = await cache_service.get_json(test_key)\n        assert normal_data is not None, \"Normal cache operation should work\"\n        \n        # Simulate Redis connection issues\n        original_redis = cache_service.redis_client\n        \n        # Test graceful degradation with connection failures\n        class FailingRedisClient:\n            \"\"\"Mock Redis client that simulates connection failures.\"\"\"\n            \n            def __init__(self, failure_rate: float = 0.5):\n                self.failure_rate = failure_rate\n                self.operation_count = 0\n            \n            async def get(self, key):\n                self.operation_count += 1\n                if random.random() < self.failure_rate:\n                    raise redis.ConnectionError(\"Simulated connection failure\")\n                return None  # Simulate cache miss\n            \n            async def setex(self, key, ttl, value):\n                self.operation_count += 1\n                if random.random() < self.failure_rate:\n                    raise redis.ConnectionError(\"Simulated connection failure\")\n                return True\n            \n            async def delete(self, key):\n                self.operation_count += 1\n                if random.random() < self.failure_rate:\n                    raise redis.ConnectionError(\"Simulated connection failure\")\n                return 1\n            \n            async def exists(self, key):\n                self.operation_count += 1\n                if random.random() < self.failure_rate:\n                    raise redis.ConnectionError(\"Simulated connection failure\")\n                return False\n        \n        # Test with failing Redis\n        failing_redis = FailingRedisClient(failure_rate=0.7)\n        cache_service.redis_client = failing_redis\n        \n        # Test resilient operations\n        successful_operations = 0\n        failed_operations = 0\n        \n        for i in range(20):\n            try:\n                # Try to set data\n                await cache_service.set_json(f\"resilience_key_{i}\", resilience_data, ttl=300)\n                successful_operations += 1\n            except Exception:\n                failed_operations += 1\n            \n            try:\n                # Try to get data\n                await cache_service.get_json(f\"resilience_key_{i}\")\n                successful_operations += 1\n            except Exception:\n                failed_operations += 1\n        \n        total_operations = successful_operations + failed_operations\n        success_rate = successful_operations / total_operations if total_operations > 0 else 0\n        \n        # Test recovery after connection restoration\n        cache_service.redis_client = original_redis\n        \n        # Verify recovery\n        recovery_start = time.time()\n        recovery_data = {\"recovery\": True, \"timestamp\": datetime.now(UTC).isoformat()}\n        \n        await cache_service.set_json(\"recovery_test\", recovery_data, ttl=300)\n        retrieved_recovery = await cache_service.get_json(\"recovery_test\")\n        \n        recovery_time = time.time() - recovery_start\n        \n        assert retrieved_recovery is not None, \"Cache should recover after connection restoration\"\n        assert retrieved_recovery[\"recovery\"] is True, \"Recovery data integrity issue\"\n        \n        # Test circuit breaker pattern simulation\n        circuit_breaker_data = []\n        \n        for i in range(10):\n            start_time = time.time()\n            \n            try:\n                # Test cache operation with timeout\n                data = await asyncio.wait_for(\n                    cache_service.get_json(f\"circuit_test_{i}\"),\n                    timeout=0.1\n                )\n                \n                operation_time = time.time() - start_time\n                circuit_breaker_data.append((\"success\", operation_time))\n                \n            except asyncio.TimeoutError:\n                operation_time = time.time() - start_time\n                circuit_breaker_data.append((\"timeout\", operation_time))\n            \n            except Exception as e:\n                operation_time = time.time() - start_time\n                circuit_breaker_data.append((\"error\", operation_time))\n        \n        # Analyze circuit breaker behavior\n        success_count = sum(1 for status, _ in circuit_breaker_data if status == \"success\")\n        timeout_count = sum(1 for status, _ in circuit_breaker_data if status == \"timeout\")\n        error_count = sum(1 for status, _ in circuit_breaker_data if status == \"error\")\n        \n        avg_operation_time = statistics.mean([time for _, time in circuit_breaker_data])\n        \n        logger.info(\n            f\"Cache Resilience and Failover Results:\\n\"\n            f\"  Normal Operation: Passed\\n\"\n            f\"  Resilient Operations Success Rate: {success_rate:.1%}\\n\"\n            f\"  Recovery Time: {recovery_time:.3f}s\\n\"\n            f\"  Circuit Breaker - Success: {success_count}, Timeout: {timeout_count}, Error: {error_count}\\n\"\n            f\"  Avg Operation Time: {avg_operation_time:.3f}s\"\n        )\n        \n        # Resilience assertions\n        assert retrieved_recovery is not None, \"Cache should recover properly\"\n        assert recovery_time <= 0.1, f\"Recovery too slow: {recovery_time:.3f}s\"\n        assert avg_operation_time <= 0.2, f\"Operations too slow under stress: {avg_operation_time:.3f}s\"\n        \n        # Cleanup\n        await cache_service.delete(\"recovery_test\")\n        await cache_service.delete(test_key)