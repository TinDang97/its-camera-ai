"""TimescaleDB Integration Tests.

This test suite validates TimescaleDB integration for time-series data:
- Hypertable creation and partitioning
- Continuous aggregates performance
- Large volume data ingestion
- Time-based queries and analytics
- Data compression and retention
- Real-time materialized views
- Performance optimization
"""

import asyncio
import json
import random
import statistics
import time
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional
from uuid import uuid4

import pytest
import pytest_asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from its_camera_ai.core.logging import get_logger
from its_camera_ai.models.analytics import TrafficData, AggregatedMetrics
from its_camera_ai.repositories.analytics_repository import AnalyticsRepository
from its_camera_ai.services.analytics_dtos import AggregationLevel, TimeWindow

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.database
@pytest.mark.asyncio
class TestTimescaleDBIntegration:
    """Integration tests for TimescaleDB time-series functionality."""

    @pytest_asyncio.fixture
    async def analytics_repository(self, db_session: AsyncSession):
        """Analytics repository with TimescaleDB connection."""
        return AnalyticsRepository(db_session)

    @pytest_asyncio.fixture
    async def sample_traffic_data(self) -> List[Dict[str, Any]]:
        """Generate sample traffic data for testing."""
        camera_ids = [f"timescale_camera_{i:02d}" for i in range(5)]
        data_points = []
        
        # Generate 24 hours of data (every minute)
        start_time = datetime.now(UTC) - timedelta(hours=24)
        
        for minute in range(24 * 60):  # 1440 data points per camera
            timestamp = start_time + timedelta(minutes=minute)
            
            for camera_id in camera_ids:
                # Simulate traffic patterns throughout the day
                hour = timestamp.hour
                
                # Rush hour patterns
                if 7 <= hour <= 9 or 17 <= hour <= 19:
                    base_vehicles = 25 + random.randint(-5, 10)
                    avg_speed = 30 + random.uniform(-10, 10)
                elif 22 <= hour or hour <= 6:
                    base_vehicles = 5 + random.randint(-2, 5)
                    avg_speed = 50 + random.uniform(-5, 15)
                else:
                    base_vehicles = 15 + random.randint(-3, 8)
                    avg_speed = 45 + random.uniform(-8, 8)
                
                # Add some randomness
                vehicle_count = max(0, base_vehicles)
                average_speed = max(20, min(80, avg_speed))
                
                data_point = {
                    \"camera_id\": camera_id,\n                    \"timestamp\": timestamp,\n                    \"vehicle_count\": vehicle_count,\n                    \"average_speed\": average_speed,\n                    \"traffic_density\": min(1.0, vehicle_count / 30.0),\n                    \"congestion_level\": (\n                        \"severe\" if vehicle_count > 25 else\n                        \"heavy\" if vehicle_count > 20 else\n                        \"moderate\" if vehicle_count > 15 else\n                        \"light\" if vehicle_count > 5 else\n                        \"free_flow\"\n                    ),\n                    \"processing_time_ms\": 45.0 + random.uniform(-10, 20),\n                    \"quality_score\": 0.85 + random.uniform(-0.1, 0.1)\n                }\n                data_points.append(data_point)\n        \n        return data_points\n\n    async def test_hypertable_creation_and_partitioning(\n        self, db_session: AsyncSession\n    ):\n        \"\"\"Test TimescaleDB hypertable creation and automatic partitioning.\"\"\"\n        \n        logger.info(\"Testing hypertable creation and partitioning\")\n        \n        # Test hypertable creation\n        create_hypertable_sql = \"\"\"\n        -- Create test table for hypertable\n        CREATE TABLE IF NOT EXISTS test_traffic_data (\n            id SERIAL PRIMARY KEY,\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            traffic_density REAL,\n            congestion_level VARCHAR(20),\n            metadata JSONB\n        );\n        \n        -- Convert to hypertable (if not already)\n        SELECT create_hypertable('test_traffic_data', 'timestamp', \n                                if_not_exists => TRUE,\n                                chunk_time_interval => INTERVAL '1 hour');\n        \"\"\"\n        \n        await db_session.execute(text(create_hypertable_sql))\n        await db_session.commit()\n        \n        # Verify hypertable was created\n        check_hypertable_sql = \"\"\"\n        SELECT hypertable_name, num_dimensions, compression_enabled\n        FROM timescaledb_information.hypertables \n        WHERE hypertable_name = 'test_traffic_data';\n        \"\"\"\n        \n        result = await db_session.execute(text(check_hypertable_sql))\n        hypertable_info = result.fetchone()\n        \n        assert hypertable_info is not None, \"Hypertable was not created\"\n        assert hypertable_info[0] == \"test_traffic_data\", \"Incorrect hypertable name\"\n        assert hypertable_info[1] >= 1, \"Hypertable should have at least 1 dimension\"\n        \n        # Test chunk creation by inserting data across time ranges\n        test_data = [\n            (\"camera_1\", datetime.now(UTC) - timedelta(hours=3), 10, 45.0),\n            (\"camera_1\", datetime.now(UTC) - timedelta(hours=2), 15, 42.0),\n            (\"camera_1\", datetime.now(UTC) - timedelta(hours=1), 20, 38.0),\n            (\"camera_1\", datetime.now(UTC), 25, 35.0),\n        ]\n        \n        insert_sql = \"\"\"\n        INSERT INTO test_traffic_data (camera_id, timestamp, vehicle_count, average_speed)\n        VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed)\n        \"\"\"\n        \n        for camera_id, timestamp, vehicle_count, avg_speed in test_data:\n            await db_session.execute(\n                text(insert_sql),\n                {\n                    \"camera_id\": camera_id,\n                    \"timestamp\": timestamp,\n                    \"vehicle_count\": vehicle_count,\n                    \"average_speed\": avg_speed\n                }\n            )\n        \n        await db_session.commit()\n        \n        # Verify chunks were created\n        check_chunks_sql = \"\"\"\n        SELECT chunk_name, range_start, range_end\n        FROM timescaledb_information.chunks \n        WHERE hypertable_name = 'test_traffic_data'\n        ORDER BY range_start;\n        \"\"\"\n        \n        result = await db_session.execute(text(check_chunks_sql))\n        chunks = result.fetchall()\n        \n        assert len(chunks) >= 1, \"No chunks were created\"\n        \n        logger.info(f\"Hypertable test completed: {len(chunks)} chunks created\")\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP TABLE IF EXISTS test_traffic_data CASCADE;\"))\n        await db_session.commit()\n\n    async def test_large_volume_data_ingestion(\n        self, db_session: AsyncSession, sample_traffic_data: List[Dict[str, Any]]\n    ):\n        \"\"\"Test high-volume data ingestion performance.\"\"\"\n        \n        logger.info(f\"Testing large volume ingestion: {len(sample_traffic_data)} records\")\n        \n        # Create test table\n        setup_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS traffic_ingestion_test (\n            id SERIAL PRIMARY KEY,\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            traffic_density REAL,\n            congestion_level VARCHAR(20),\n            processing_time_ms REAL,\n            quality_score REAL\n        );\n        \n        SELECT create_hypertable('traffic_ingestion_test', 'timestamp', \n                                if_not_exists => TRUE,\n                                chunk_time_interval => INTERVAL '1 hour');\n        \"\"\"\n        \n        await db_session.execute(text(setup_sql))\n        await db_session.commit()\n        \n        # Test batch insertion performance\n        batch_size = 1000\n        insert_times = []\n        total_inserted = 0\n        \n        insert_sql = \"\"\"\n        INSERT INTO traffic_ingestion_test \n        (camera_id, timestamp, vehicle_count, average_speed, traffic_density, \n         congestion_level, processing_time_ms, quality_score)\n        VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed, \n                :traffic_density, :congestion_level, :processing_time_ms, :quality_score)\n        \"\"\"\n        \n        start_time = time.time()\n        \n        for i in range(0, len(sample_traffic_data), batch_size):\n            batch = sample_traffic_data[i:i + batch_size]\n            \n            batch_start = time.time()\n            \n            # Insert batch\n            for record in batch:\n                await db_session.execute(text(insert_sql), record)\n            \n            await db_session.commit()\n            \n            batch_time = time.time() - batch_start\n            insert_times.append(batch_time)\n            total_inserted += len(batch)\n            \n            if (i // batch_size + 1) % 5 == 0:\n                logger.info(f\"Inserted {total_inserted} records...\")\n        \n        total_time = time.time() - start_time\n        \n        # Performance metrics\n        records_per_second = total_inserted / total_time\n        avg_batch_time = statistics.mean(insert_times)\n        max_batch_time = max(insert_times)\n        \n        logger.info(\n            f\"Ingestion Performance Results:\\n\"\n            f\"  Total Records: {total_inserted}\\n\"\n            f\"  Total Time: {total_time:.2f}s\\n\"\n            f\"  Records/Second: {records_per_second:.1f}\\n\"\n            f\"  Avg Batch Time: {avg_batch_time:.3f}s\\n\"\n            f\"  Max Batch Time: {max_batch_time:.3f}s\\n\"\n            f\"  Batch Size: {batch_size}\"\n        )\n        \n        # Performance assertions\n        assert records_per_second >= 1000, f\"Ingestion too slow: {records_per_second:.1f} records/s\"\n        assert avg_batch_time <= 5.0, f\"Average batch time too high: {avg_batch_time:.3f}s\"\n        assert total_time <= 60, f\"Total ingestion time too high: {total_time:.2f}s\"\n        \n        # Verify data integrity\n        count_sql = \"SELECT COUNT(*) FROM traffic_ingestion_test\"\n        result = await db_session.execute(text(count_sql))\n        actual_count = result.scalar()\n        \n        assert actual_count == total_inserted, f\"Data integrity issue: {actual_count}/{total_inserted}\"\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP TABLE IF EXISTS traffic_ingestion_test CASCADE;\"))\n        await db_session.commit()\n\n    async def test_continuous_aggregates_performance(\n        self, db_session: AsyncSession\n    ):\n        \"\"\"Test TimescaleDB continuous aggregates for real-time analytics.\"\"\"\n        \n        logger.info(\"Testing continuous aggregates performance\")\n        \n        # Setup base table and hypertable\n        setup_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS traffic_continuous_agg_test (\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            traffic_density REAL\n        );\n        \n        SELECT create_hypertable('traffic_continuous_agg_test', 'timestamp', \n                                if_not_exists => TRUE,\n                                chunk_time_interval => INTERVAL '1 hour');\n        \"\"\"\n        \n        await db_session.execute(text(setup_sql))\n        await db_session.commit()\n        \n        # Create continuous aggregate\n        continuous_agg_sql = \"\"\"\n        CREATE MATERIALIZED VIEW IF NOT EXISTS traffic_hourly_agg\n        WITH (timescaledb.continuous) AS\n        SELECT \n            camera_id,\n            time_bucket('1 hour', timestamp) AS bucket,\n            AVG(vehicle_count) as avg_vehicle_count,\n            MAX(vehicle_count) as max_vehicle_count,\n            AVG(average_speed) as avg_speed,\n            AVG(traffic_density) as avg_density,\n            COUNT(*) as sample_count\n        FROM traffic_continuous_agg_test\n        GROUP BY camera_id, bucket;\n        \"\"\"\n        \n        await db_session.execute(text(continuous_agg_sql))\n        await db_session.commit()\n        \n        # Insert test data\n        camera_ids = [\"agg_camera_1\", \"agg_camera_2\", \"agg_camera_3\"]\n        test_data = []\n        \n        # Generate 6 hours of data (every 5 minutes)\n        start_time = datetime.now(UTC) - timedelta(hours=6)\n        \n        for minutes in range(0, 6 * 60, 5):  # Every 5 minutes for 6 hours\n            timestamp = start_time + timedelta(minutes=minutes)\n            \n            for camera_id in camera_ids:\n                vehicle_count = 10 + (minutes // 30) % 20  # Varying traffic\n                avg_speed = 40 + random.uniform(-10, 10)\n                density = vehicle_count / 30.0\n                \n                test_data.append({\n                    \"camera_id\": camera_id,\n                    \"timestamp\": timestamp,\n                    \"vehicle_count\": vehicle_count,\n                    \"average_speed\": avg_speed,\n                    \"traffic_density\": density\n                })\n        \n        # Insert data in batches\n        insert_sql = \"\"\"\n        INSERT INTO traffic_continuous_agg_test \n        (camera_id, timestamp, vehicle_count, average_speed, traffic_density)\n        VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed, :traffic_density)\n        \"\"\"\n        \n        batch_size = 100\n        for i in range(0, len(test_data), batch_size):\n            batch = test_data[i:i + batch_size]\n            for record in batch:\n                await db_session.execute(text(insert_sql), record)\n            await db_session.commit()\n        \n        # Refresh continuous aggregate\n        refresh_sql = \"\"\"\n        CALL refresh_continuous_aggregate('traffic_hourly_agg', NULL, NULL);\n        \"\"\"\n        \n        refresh_start = time.time()\n        await db_session.execute(text(refresh_sql))\n        await db_session.commit()\n        refresh_time = time.time() - refresh_start\n        \n        # Test continuous aggregate queries\n        agg_query_sql = \"\"\"\n        SELECT \n            camera_id,\n            bucket,\n            avg_vehicle_count,\n            max_vehicle_count,\n            avg_speed,\n            sample_count\n        FROM traffic_hourly_agg \n        WHERE bucket >= :start_time\n        ORDER BY camera_id, bucket;\n        \"\"\"\n        \n        query_start = time.time()\n        result = await db_session.execute(\n            text(agg_query_sql),\n            {\"start_time\": start_time}\n        )\n        agg_results = result.fetchall()\n        query_time = time.time() - query_start\n        \n        # Validate results\n        assert len(agg_results) > 0, \"No continuous aggregate results found\"\n        \n        # Verify aggregate accuracy\n        for row in agg_results[:3]:  # Check first few rows\n            camera_id, bucket, avg_vehicles, max_vehicles, avg_speed, sample_count = row\n            \n            assert avg_vehicles >= 0, \"Invalid average vehicle count\"\n            assert max_vehicles >= avg_vehicles, \"Max should be >= average\"\n            assert 20 <= avg_speed <= 70, \"Speed out of expected range\"\n            assert sample_count > 0, \"Sample count should be positive\"\n        \n        logger.info(\n            f\"Continuous Aggregates Results:\\n\"\n            f\"  Data Points Inserted: {len(test_data)}\\n\"\n            f\"  Aggregate Records: {len(agg_results)}\\n\"\n            f\"  Refresh Time: {refresh_time:.3f}s\\n\"\n            f\"  Query Time: {query_time:.3f}s\"\n        )\n        \n        # Performance assertions\n        assert refresh_time <= 5.0, f\"Refresh too slow: {refresh_time:.3f}s\"\n        assert query_time <= 1.0, f\"Query too slow: {query_time:.3f}s\"\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP MATERIALIZED VIEW IF EXISTS traffic_hourly_agg CASCADE;\"))\n        await db_session.execute(text(\"DROP TABLE IF EXISTS traffic_continuous_agg_test CASCADE;\"))\n        await db_session.commit()\n\n    async def test_time_based_queries_and_analytics(\n        self, db_session: AsyncSession\n    ):\n        \"\"\"Test complex time-based queries and analytics.\"\"\"\n        \n        logger.info(\"Testing time-based queries and analytics\")\n        \n        # Setup test table\n        setup_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS traffic_analytics_test (\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            congestion_level VARCHAR(20)\n        );\n        \n        SELECT create_hypertable('traffic_analytics_test', 'timestamp', \n                                if_not_exists => TRUE);\n        \"\"\"\n        \n        await db_session.execute(text(setup_sql))\n        await db_session.commit()\n        \n        # Insert test data spanning multiple days\n        camera_ids = [\"analytics_camera_1\", \"analytics_camera_2\"]\n        test_data = []\n        \n        # Generate 7 days of data\n        start_time = datetime.now(UTC) - timedelta(days=7)\n        \n        for day in range(7):\n            for hour in range(24):\n                timestamp = start_time + timedelta(days=day, hours=hour)\n                \n                for camera_id in camera_ids:\n                    # Simulate weekly patterns\n                    is_weekend = day >= 5\n                    vehicle_count = (\n                        15 + random.randint(-5, 10) if is_weekend else\n                        25 + random.randint(-8, 15)\n                    )\n                    \n                    test_data.append({\n                        \"camera_id\": camera_id,\n                        \"timestamp\": timestamp,\n                        \"vehicle_count\": vehicle_count,\n                        \"average_speed\": 40 + random.uniform(-10, 15),\n                        \"congestion_level\": (\n                            \"heavy\" if vehicle_count > 30 else\n                            \"moderate\" if vehicle_count > 20 else\n                            \"light\"\n                        )\n                    })\n        \n        # Insert data\n        insert_sql = \"\"\"\n        INSERT INTO traffic_analytics_test \n        (camera_id, timestamp, vehicle_count, average_speed, congestion_level)\n        VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed, :congestion_level)\n        \"\"\"\n        \n        for record in test_data:\n            await db_session.execute(text(insert_sql), record)\n        await db_session.commit()\n        \n        # Test various time-based queries\n        test_queries = [\n            # 1. Daily averages for last 7 days\n            {\n                \"name\": \"Daily Averages\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    date_trunc('day', timestamp) as day,\n                    AVG(vehicle_count) as avg_vehicles,\n                    AVG(average_speed) as avg_speed\n                FROM traffic_analytics_test\n                WHERE timestamp >= :start_time\n                GROUP BY camera_id, day\n                ORDER BY camera_id, day;\n                \"\"\",\n                \"params\": {\"start_time\": start_time}\n            },\n            # 2. Peak hour analysis\n            {\n                \"name\": \"Peak Hours\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    EXTRACT(hour FROM timestamp) as hour,\n                    AVG(vehicle_count) as avg_vehicles\n                FROM traffic_analytics_test\n                WHERE timestamp >= :start_time\n                GROUP BY camera_id, hour\n                HAVING AVG(vehicle_count) > 20\n                ORDER BY camera_id, avg_vehicles DESC;\n                \"\"\",\n                \"params\": {\"start_time\": start_time}\n            },\n            # 3. Time bucket aggregation\n            {\n                \"name\": \"4-Hour Buckets\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    time_bucket('4 hours', timestamp) as bucket,\n                    AVG(vehicle_count) as avg_vehicles,\n                    COUNT(*) as sample_count\n                FROM traffic_analytics_test\n                WHERE timestamp >= :start_time\n                GROUP BY camera_id, bucket\n                ORDER BY camera_id, bucket;\n                \"\"\",\n                \"params\": {\"start_time\": start_time}\n            },\n            # 4. Moving average\n            {\n                \"name\": \"Moving Average\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    timestamp,\n                    vehicle_count,\n                    AVG(vehicle_count) OVER (\n                        PARTITION BY camera_id \n                        ORDER BY timestamp \n                        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n                    ) as moving_avg\n                FROM traffic_analytics_test\n                WHERE timestamp >= :recent_time\n                ORDER BY camera_id, timestamp\n                LIMIT 20;\n                \"\"\",\n                \"params\": {\"recent_time\": datetime.now(UTC) - timedelta(hours=12)}\n            }\n        ]\n        \n        query_results = {}\n        \n        for query_info in test_queries:\n            query_start = time.time()\n            \n            result = await db_session.execute(\n                text(query_info[\"sql\"]),\n                query_info[\"params\"]\n            )\n            rows = result.fetchall()\n            \n            query_time = time.time() - query_start\n            \n            query_results[query_info[\"name\"]] = {\n                \"rows\": len(rows),\n                \"time\": query_time,\n                \"data\": rows[:5]  # Store first 5 rows for validation\n            }\n            \n            # Performance assertion\n            assert query_time <= 2.0, f\"{query_info['name']} query too slow: {query_time:.3f}s\"\n            assert len(rows) > 0, f\"{query_info['name']} returned no results\"\n        \n        # Log results\n        for name, result in query_results.items():\n            logger.info(f\"{name}: {result['rows']} rows in {result['time']:.3f}s\")\n        \n        # Validate specific query results\n        daily_avg = query_results[\"Daily Averages\"]\n        assert daily_avg[\"rows\"] >= 14, \"Should have data for both cameras over 7 days\"\n        \n        peak_hours = query_results[\"Peak Hours\"]\n        assert peak_hours[\"rows\"] > 0, \"Should identify some peak hours\"\n        \n        time_buckets = query_results[\"4-Hour Buckets\"]\n        assert time_buckets[\"rows\"] >= 42, \"Should have 6 buckets per day * 7 days * 1 camera minimum\"\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP TABLE IF EXISTS traffic_analytics_test CASCADE;\"))\n        await db_session.commit()\n\n    async def test_data_compression_and_retention(\n        self, db_session: AsyncSession\n    ):\n        \"\"\"Test TimescaleDB compression and data retention policies.\"\"\"\n        \n        logger.info(\"Testing data compression and retention\")\n        \n        # Setup table with compression\n        setup_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS traffic_compression_test (\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            metadata JSONB\n        );\n        \n        SELECT create_hypertable('traffic_compression_test', 'timestamp', \n                                chunk_time_interval => INTERVAL '1 day');\n        \"\"\"\n        \n        await db_session.execute(text(setup_sql))\n        await db_session.commit()\n        \n        # Enable compression on older chunks\n        compression_sql = \"\"\"\n        ALTER TABLE traffic_compression_test SET (\n            timescaledb.compress,\n            timescaledb.compress_segmentby = 'camera_id'\n        );\n        \n        SELECT add_compression_policy('traffic_compression_test', INTERVAL '2 days');\n        \"\"\"\n        \n        try:\n            await db_session.execute(text(compression_sql))\n            await db_session.commit()\n        except Exception as e:\n            logger.warning(f\"Compression setup warning (may be expected): {e}\")\n        \n        # Insert historical data\n        historical_data = []\n        \n        # Generate 5 days of historical data\n        start_time = datetime.now(UTC) - timedelta(days=5)\n        \n        for day in range(5):\n            for hour in range(24):\n                timestamp = start_time + timedelta(days=day, hours=hour)\n                \n                for camera_id in [\"compress_camera_1\", \"compress_camera_2\"]:\n                    historical_data.append({\n                        \"camera_id\": camera_id,\n                        \"timestamp\": timestamp,\n                        \"vehicle_count\": random.randint(5, 30),\n                        \"average_speed\": random.uniform(30, 70),\n                        \"metadata\": json.dumps({\n                            \"weather\": random.choice([\"clear\", \"cloudy\", \"rainy\"]),\n                            \"visibility\": random.uniform(0.8, 1.0)\n                        })\n                    })\n        \n        # Insert data\n        insert_sql = \"\"\"\n        INSERT INTO traffic_compression_test \n        (camera_id, timestamp, vehicle_count, average_speed, metadata)\n        VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed, :metadata)\n        \"\"\"\n        \n        insert_start = time.time()\n        for record in historical_data:\n            await db_session.execute(text(insert_sql), record)\n        await db_session.commit()\n        insert_time = time.time() - insert_start\n        \n        # Check table size before compression\n        size_before_sql = \"\"\"\n        SELECT \n            schemaname,\n            tablename,\n            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\n        FROM pg_tables \n        WHERE tablename = 'traffic_compression_test';\n        \"\"\"\n        \n        result = await db_session.execute(text(size_before_sql))\n        size_before = result.fetchone()\n        \n        # Test retention policy\n        retention_sql = \"\"\"\n        SELECT add_retention_policy('traffic_compression_test', INTERVAL '30 days');\n        \"\"\"\n        \n        try:\n            await db_session.execute(text(retention_sql))\n            await db_session.commit()\n        except Exception as e:\n            logger.warning(f\"Retention policy warning (may be expected): {e}\")\n        \n        # Verify data was inserted\n        count_sql = \"SELECT COUNT(*) FROM traffic_compression_test\"\n        result = await db_session.execute(text(count_sql))\n        total_records = result.scalar()\n        \n        # Test query performance on compressed data\n        query_sql = \"\"\"\n        SELECT \n            camera_id,\n            date_trunc('day', timestamp) as day,\n            AVG(vehicle_count) as avg_vehicles,\n            COUNT(*) as record_count\n        FROM traffic_compression_test\n        WHERE timestamp >= :start_time\n        GROUP BY camera_id, day\n        ORDER BY camera_id, day;\n        \"\"\"\n        \n        query_start = time.time()\n        result = await db_session.execute(\n            text(query_sql),\n            {\"start_time\": start_time}\n        )\n        query_results = result.fetchall()\n        query_time = time.time() - query_start\n        \n        logger.info(\n            f\"Compression and Retention Results:\\n\"\n            f\"  Records Inserted: {total_records}\\n\"\n            f\"  Insert Time: {insert_time:.3f}s\\n\"\n            f\"  Query Results: {len(query_results)} rows\\n\"\n            f\"  Query Time: {query_time:.3f}s\\n\"\n            f\"  Table Size: {size_before[2] if size_before else 'Unknown'}\"\n        )\n        \n        # Performance assertions\n        assert total_records == len(historical_data), \"Data insertion incomplete\"\n        assert len(query_results) > 0, \"Query returned no results\"\n        assert query_time <= 2.0, f\"Query on compressed data too slow: {query_time:.3f}s\"\n        \n        # Verify retention policy exists\n        check_policies_sql = \"\"\"\n        SELECT job_id, config\n        FROM timescaledb_information.jobs\n        WHERE proc_name = 'policy_retention';\n        \"\"\"\n        \n        result = await db_session.execute(text(check_policies_sql))\n        policies = result.fetchall()\n        \n        logger.info(f\"Found {len(policies)} retention policies\")\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP TABLE IF EXISTS traffic_compression_test CASCADE;\"))\n        await db_session.commit()\n\n    async def test_real_time_materialized_views(\n        self, db_session: AsyncSession\n    ):\n        \"\"\"Test real-time materialized views for dashboard queries.\"\"\"\n        \n        logger.info(\"Testing real-time materialized views\")\n        \n        # Setup base table\n        setup_sql = \"\"\"\n        CREATE TABLE IF NOT EXISTS traffic_realtime_test (\n            camera_id VARCHAR(50) NOT NULL,\n            timestamp TIMESTAMPTZ NOT NULL,\n            vehicle_count INTEGER DEFAULT 0,\n            average_speed REAL,\n            congestion_level VARCHAR(20)\n        );\n        \n        SELECT create_hypertable('traffic_realtime_test', 'timestamp', \n                                chunk_time_interval => INTERVAL '1 hour');\n        \"\"\"\n        \n        await db_session.execute(text(setup_sql))\n        await db_session.commit()\n        \n        # Create real-time materialized view\n        materialized_view_sql = \"\"\"\n        CREATE MATERIALIZED VIEW IF NOT EXISTS traffic_realtime_summary\n        WITH (timescaledb.continuous) AS\n        SELECT \n            camera_id,\n            time_bucket('15 minutes', timestamp) AS bucket,\n            AVG(vehicle_count) as avg_vehicles,\n            MAX(vehicle_count) as peak_vehicles,\n            AVG(average_speed) as avg_speed,\n            mode() WITHIN GROUP (ORDER BY congestion_level) as dominant_congestion,\n            COUNT(*) as sample_count,\n            MAX(timestamp) as latest_update\n        FROM traffic_realtime_test\n        GROUP BY camera_id, bucket;\n        \n        -- Enable real-time aggregation\n        SELECT add_continuous_aggregate_policy('traffic_realtime_summary',\n            start_offset => INTERVAL '1 hour',\n            end_offset => INTERVAL '15 minutes',\n            schedule_interval => INTERVAL '1 minute');\n        \"\"\"\n        \n        await db_session.execute(text(materialized_view_sql))\n        await db_session.commit()\n        \n        # Simulate real-time data insertion\n        camera_ids = [\"realtime_camera_1\", \"realtime_camera_2\", \"realtime_camera_3\"]\n        \n        # Insert data in real-time simulation\n        for minute in range(60):  # 1 hour of data\n            timestamp = datetime.now(UTC) - timedelta(minutes=60-minute)\n            \n            for camera_id in camera_ids:\n                # Simulate varying traffic\n                base_vehicles = 15 + (minute % 30)\n                vehicle_count = base_vehicles + random.randint(-5, 10)\n                avg_speed = 45 - (vehicle_count * 0.5)  # Inverse relationship\n                \n                congestion = (\n                    \"severe\" if vehicle_count > 35 else\n                    \"heavy\" if vehicle_count > 25 else\n                    \"moderate\" if vehicle_count > 15 else\n                    \"light\"\n                )\n                \n                insert_sql = \"\"\"\n                INSERT INTO traffic_realtime_test \n                (camera_id, timestamp, vehicle_count, average_speed, congestion_level)\n                VALUES (:camera_id, :timestamp, :vehicle_count, :average_speed, :congestion_level)\n                \"\"\"\n                \n                await db_session.execute(text(insert_sql), {\n                    \"camera_id\": camera_id,\n                    \"timestamp\": timestamp,\n                    \"vehicle_count\": vehicle_count,\n                    \"average_speed\": avg_speed,\n                    \"congestion_level\": congestion\n                })\n        \n        await db_session.commit()\n        \n        # Refresh materialized view\n        refresh_sql = \"\"\"\n        CALL refresh_continuous_aggregate('traffic_realtime_summary', NULL, NULL);\n        \"\"\"\n        \n        refresh_start = time.time()\n        await db_session.execute(text(refresh_sql))\n        await db_session.commit()\n        refresh_time = time.time() - refresh_start\n        \n        # Test dashboard-style queries on materialized view\n        dashboard_queries = [\n            # Current status\n            {\n                \"name\": \"Current Status\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    avg_vehicles,\n                    peak_vehicles,\n                    avg_speed,\n                    dominant_congestion,\n                    latest_update\n                FROM traffic_realtime_summary\n                WHERE bucket = (\n                    SELECT MAX(bucket) FROM traffic_realtime_summary\n                )\n                ORDER BY camera_id;\n                \"\"\"\n            },\n            # Trend analysis\n            {\n                \"name\": \"Trend Analysis\",\n                \"sql\": \"\"\"\n                SELECT \n                    camera_id,\n                    bucket,\n                    avg_vehicles,\n                    LAG(avg_vehicles) OVER (\n                        PARTITION BY camera_id ORDER BY bucket\n                    ) as prev_avg_vehicles\n                FROM traffic_realtime_summary\n                WHERE bucket >= :recent_time\n                ORDER BY camera_id, bucket;\n                \"\"\",\n                \"params\": {\"recent_time\": datetime.now(UTC) - timedelta(hours=2)}\n            }\n        ]\n        \n        query_times = []\n        \n        for query_info in dashboard_queries:\n            query_start = time.time()\n            \n            params = query_info.get(\"params\", {})\n            result = await db_session.execute(text(query_info[\"sql\"]), params)\n            rows = result.fetchall()\n            \n            query_time = time.time() - query_start\n            query_times.append(query_time)\n            \n            logger.info(f\"{query_info['name']}: {len(rows)} rows in {query_time:.3f}s\")\n            \n            # Validate results\n            assert len(rows) > 0, f\"{query_info['name']} returned no results\"\n            \n            # Performance assertion for dashboard queries\n            assert query_time <= 0.5, f\"{query_info['name']} too slow for dashboard: {query_time:.3f}s\"\n        \n        avg_query_time = statistics.mean(query_times)\n        \n        logger.info(\n            f\"Real-time Materialized Views Results:\\n\"\n            f\"  Refresh Time: {refresh_time:.3f}s\\n\"\n            f\"  Avg Dashboard Query Time: {avg_query_time:.3f}s\\n\"\n            f\"  Total Cameras: {len(camera_ids)}\"\n        )\n        \n        # Performance assertions\n        assert refresh_time <= 3.0, f\"Materialized view refresh too slow: {refresh_time:.3f}s\"\n        assert avg_query_time <= 0.3, f\"Dashboard queries too slow: {avg_query_time:.3f}s\"\n        \n        # Cleanup\n        await db_session.execute(text(\"DROP MATERIALIZED VIEW IF EXISTS traffic_realtime_summary CASCADE;\"))\n        await db_session.execute(text(\"DROP TABLE IF EXISTS traffic_realtime_test CASCADE;\"))\n        await db_session.commit()