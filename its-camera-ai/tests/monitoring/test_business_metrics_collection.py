"""Business Metrics Collection Tests.

This test suite validates custom business metrics collection for traffic analytics:
- Traffic flow and congestion metrics
- Vehicle detection and classification accuracy
- Incident detection and response times
- System performance and availability KPIs
- Revenue and cost optimization metrics
- User engagement and dashboard usage
- Compliance and regulatory reporting
"""

import asyncio
import json
import random
import statistics
import time
from collections import defaultdict
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

import pytest
import pytest_asyncio
from prometheus_client import CollectorRegistry, Counter, Gauge, Histogram

from its_camera_ai.core.logging import get_logger
from its_camera_ai.services.analytics_dtos import (
    CongestionLevel,
    ProcessingResult,
    RealtimeTrafficMetrics,
)

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.business_metrics
@pytest.mark.asyncio
class TestBusinessMetricsCollection:
    """Tests for business-specific metrics collection and analysis."""

    @pytest_asyncio.fixture
    async def business_metrics_registry(self):
        \"\"\"Business metrics registry with traffic-specific metrics.\"\"\"\n        registry = CollectorRegistry()\n        \n        # Traffic Flow Metrics\n        traffic_metrics = {\n            # Vehicle Flow and Volume\n            \"vehicles_per_hour_total\": Counter(\n                \"its_vehicles_per_hour_total\",\n                \"Total vehicles detected per hour\",\n                [\"camera_id\", \"direction\", \"vehicle_type\", \"time_bucket\"],\n                registry=registry\n            ),\n            \"average_vehicle_speed_kmh\": Gauge(\n                \"its_average_vehicle_speed_kmh\",\n                \"Average vehicle speed in km/h\",\n                [\"camera_id\", \"road_segment\", \"time_period\"],\n                registry=registry\n            ),\n            \"traffic_density_ratio\": Gauge(\n                \"its_traffic_density_ratio\",\n                \"Traffic density ratio (0-1)\",\n                [\"camera_id\", \"lane_id\"],\n                registry=registry\n            ),\n            \n            # Congestion and Flow Analysis\n            \"congestion_duration_minutes\": Histogram(\n                \"its_congestion_duration_minutes\",\n                \"Duration of congestion events in minutes\",\n                [\"camera_id\", \"congestion_level\"],\n                buckets=[1, 5, 10, 15, 30, 60, 120, 240],\n                registry=registry\n            ),\n            \"queue_length_meters\": Gauge(\n                \"its_queue_length_meters\",\n                \"Vehicle queue length in meters\",\n                [\"camera_id\", \"lane_id\"],\n                registry=registry\n            ),\n            \"flow_efficiency_ratio\": Gauge(\n                \"its_flow_efficiency_ratio\",\n                \"Traffic flow efficiency (actual vs optimal)\",\n                [\"camera_id\", \"time_period\"],\n                registry=registry\n            ),\n            \n            # Vehicle Classification and Detection\n            \"vehicle_classification_accuracy\": Gauge(\n                \"its_vehicle_classification_accuracy\",\n                \"Vehicle classification accuracy percentage\",\n                [\"model_version\", \"vehicle_type\"],\n                registry=registry\n            ),\n            \"detection_confidence_score\": Histogram(\n                \"its_detection_confidence_score\",\n                \"ML model detection confidence scores\",\n                [\"camera_id\", \"vehicle_type\"],\n                buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0],\n                registry=registry\n            ),\n            \"false_positive_detections_total\": Counter(\n                \"its_false_positive_detections_total\",\n                \"Total false positive detections\",\n                [\"camera_id\", \"detection_type\", \"model_version\"],\n                registry=registry\n            ),\n            \n            # Incident Detection and Response\n            \"incidents_detected_total\": Counter(\n                \"its_incidents_detected_total\",\n                \"Total incidents detected\",\n                [\"camera_id\", \"incident_type\", \"severity\"],\n                registry=registry\n            ),\n            \"incident_detection_time_seconds\": Histogram(\n                \"its_incident_detection_time_seconds\",\n                \"Time from incident occurrence to detection\",\n                [\"incident_type\"],\n                buckets=[5, 10, 30, 60, 120, 300, 600],\n                registry=registry\n            ),\n            \"incident_response_time_seconds\": Histogram(\n                \"its_incident_response_time_seconds\",\n                \"Time from detection to response initiation\",\n                [\"incident_type\", \"response_team\"],\n                buckets=[30, 60, 120, 300, 600, 1200],\n                registry=registry\n            ),\n            \"incident_resolution_time_minutes\": Histogram(\n                \"its_incident_resolution_time_minutes\",\n                \"Time from detection to resolution\",\n                [\"incident_type\"],\n                buckets=[5, 15, 30, 60, 120, 240, 480],\n                registry=registry\n            ),\n            \n            # Safety and Violations\n            \"traffic_violations_total\": Counter(\n                \"its_traffic_violations_total\",\n                \"Total traffic violations detected\",\n                [\"camera_id\", \"violation_type\", \"severity\"],\n                registry=registry\n            ),\n            \"safety_score\": Gauge(\n                \"its_safety_score\",\n                \"Road safety score (0-100)\",\n                [\"camera_id\", \"time_period\"],\n                registry=registry\n            ),\n            \"near_miss_events_total\": Counter(\n                \"its_near_miss_events_total\",\n                \"Total near-miss events detected\",\n                [\"camera_id\", \"event_type\"],\n                registry=registry\n            ),\n            \n            # System Performance KPIs\n            \"camera_uptime_percentage\": Gauge(\n                \"its_camera_uptime_percentage\",\n                \"Camera uptime percentage\",\n                [\"camera_id\", \"location\"],\n                registry=registry\n            ),\n            \"data_quality_score\": Gauge(\n                \"its_data_quality_score\",\n                \"Data quality score (0-1)\",\n                [\"camera_id\", \"metric_type\"],\n                registry=registry\n            ),\n            \"processing_throughput_fps\": Gauge(\n                \"its_processing_throughput_fps\",\n                \"Processing throughput in frames per second\",\n                [\"service\", \"camera_id\"],\n                registry=registry\n            ),\n            \n            # Business Value and ROI\n            \"cost_per_detection_usd\": Gauge(\n                \"its_cost_per_detection_usd\",\n                \"Cost per vehicle detection in USD\",\n                [\"camera_id\", \"service_tier\"],\n                registry=registry\n            ),\n            \"revenue_impact_usd\": Counter(\n                \"its_revenue_impact_usd\",\n                \"Revenue impact from traffic optimization\",\n                [\"optimization_type\", \"time_period\"],\n                registry=registry\n            ),\n            \"fuel_savings_liters\": Counter(\n                \"its_fuel_savings_liters\",\n                \"Estimated fuel savings from traffic optimization\",\n                [\"camera_id\", \"optimization_method\"],\n                registry=registry\n            )\n        }\n        \n        return registry, traffic_metrics\n\n    @pytest_asyncio.fixture\n    async def sample_traffic_scenarios(self):\n        \"\"\"Generate sample traffic scenarios for metrics testing.\"\"\"\n        scenarios = {\n            \"rush_hour_congestion\": {\n                \"camera_ids\": [\"highway_cam_001\", \"highway_cam_002\"],\n                \"time_period\": \"rush_hour\",\n                \"vehicle_count_range\": (40, 80),\n                \"speed_range\": (15, 35),\n                \"congestion_level\": CongestionLevel.HEAVY,\n                \"incident_probability\": 0.15,\n                \"violation_probability\": 0.08\n            },\n            \"normal_traffic_flow\": {\n                \"camera_ids\": [\"urban_cam_001\", \"urban_cam_002\", \"urban_cam_003\"],\n                \"time_period\": \"normal\",\n                \"vehicle_count_range\": (10, 25),\n                \"speed_range\": (40, 60),\n                \"congestion_level\": CongestionLevel.LIGHT,\n                \"incident_probability\": 0.03,\n                \"violation_probability\": 0.02\n            },\n            \"night_low_traffic\": {\n                \"camera_ids\": [\"suburban_cam_001\", \"suburban_cam_002\"],\n                \"time_period\": \"night\",\n                \"vehicle_count_range\": (2, 8),\n                \"speed_range\": (50, 80),\n                \"congestion_level\": CongestionLevel.FREE_FLOW,\n                \"incident_probability\": 0.01,\n                \"violation_probability\": 0.05\n            },\n            \"construction_zone\": {\n                \"camera_ids\": [\"construction_cam_001\"],\n                \"time_period\": \"construction\",\n                \"vehicle_count_range\": (15, 30),\n                \"speed_range\": (10, 25),\n                \"congestion_level\": CongestionLevel.MODERATE,\n                \"incident_probability\": 0.08,\n                \"violation_probability\": 0.12\n            },\n            \"school_zone_active\": {\n                \"camera_ids\": [\"school_cam_001\", \"school_cam_002\"],\n                \"time_period\": \"school_hours\",\n                \"vehicle_count_range\": (8, 20),\n                \"speed_range\": (15, 30),\n                \"congestion_level\": CongestionLevel.LIGHT,\n                \"incident_probability\": 0.02,\n                \"violation_probability\": 0.15\n            }\n        }\n        \n        return scenarios\n\n    async def test_traffic_flow_metrics_collection(\n        self, business_metrics_registry, sample_traffic_scenarios\n    ):\n        \"\"\"Test traffic flow and volume metrics collection.\"\"\"\n        \n        logger.info(\"Testing traffic flow metrics collection\")\n        \n        registry, metrics = business_metrics_registry\n        \n        # Simulate 24 hours of traffic data collection\n        simulation_hours = 24\n        data_points_per_hour = 60  # One data point per minute\n        \n        collected_metrics = defaultdict(list)\n        \n        for hour in range(simulation_hours):\n            # Determine traffic scenario based on time of day\n            if 7 <= hour <= 9 or 17 <= hour <= 19:  # Rush hours\n                scenario = sample_traffic_scenarios[\"rush_hour_congestion\"]\n            elif 22 <= hour or hour <= 6:  # Night time\n                scenario = sample_traffic_scenarios[\"night_low_traffic\"]\n            elif 8 <= hour <= 15:  # School hours\n                scenario = sample_traffic_scenarios[\"school_zone_active\"]\n            else:  # Normal traffic\n                scenario = sample_traffic_scenarios[\"normal_traffic_flow\"]\n            \n            for minute in range(data_points_per_hour):\n                timestamp = datetime.now(UTC) + timedelta(hours=hour-12, minutes=minute)\n                time_bucket = f\"{hour:02d}:00\"\n                \n                for camera_id in scenario[\"camera_ids\"]:\n                    # Generate vehicle flow data\n                    vehicle_count = random.randint(*scenario[\"vehicle_count_range\"])\n                    avg_speed = random.uniform(*scenario[\"speed_range\"])\n                    \n                    # Vehicle type distribution\n                    vehicle_types = [\"car\", \"truck\", \"bus\", \"motorcycle\"]\n                    type_distribution = [0.75, 0.15, 0.05, 0.05]  # Typical distribution\n                    \n                    for vehicle_type, ratio in zip(vehicle_types, type_distribution):\n                        type_count = int(vehicle_count * ratio)\n                        if type_count > 0:\n                            # Update vehicles per hour metric\n                            metrics[\"vehicles_per_hour_total\"].labels(\n                                camera_id=camera_id,\n                                direction=\"northbound\",\n                                vehicle_type=vehicle_type,\n                                time_bucket=time_bucket\n                            ).inc(type_count)\n                    \n                    # Update speed metrics\n                    metrics[\"average_vehicle_speed_kmh\"].labels(\n                        camera_id=camera_id,\n                        road_segment=f\"segment_{camera_id.split('_')[-1]}\",\n                        time_period=scenario[\"time_period\"]\n                    ).set(avg_speed)\n                    \n                    # Traffic density calculation\n                    max_capacity = 100  # vehicles per hour per lane\n                    density_ratio = min(1.0, vehicle_count / max_capacity)\n                    \n                    for lane_id in [\"lane_1\", \"lane_2\"]:\n                        metrics[\"traffic_density_ratio\"].labels(\n                            camera_id=camera_id,\n                            lane_id=lane_id\n                        ).set(density_ratio * random.uniform(0.8, 1.2))\n                    \n                    # Congestion analysis\n                    if scenario[\"congestion_level\"] in [CongestionLevel.HEAVY, CongestionLevel.SEVERE]:\n                        congestion_duration = random.uniform(5, 45)  # minutes\n                        metrics[\"congestion_duration_minutes\"].labels(\n                            camera_id=camera_id,\n                            congestion_level=scenario[\"congestion_level\"].value\n                        ).observe(congestion_duration)\n                        \n                        # Queue length during congestion\n                        queue_length = random.uniform(50, 200)  # meters\n                        for lane_id in [\"lane_1\", \"lane_2\"]:\n                            metrics[\"queue_length_meters\"].labels(\n                                camera_id=camera_id,\n                                lane_id=lane_id\n                            ).set(queue_length * random.uniform(0.7, 1.3))\n                    \n                    # Flow efficiency\n                    optimal_flow = 80  # vehicles per hour under ideal conditions\n                    actual_flow = vehicle_count\n                    efficiency = min(1.0, actual_flow / optimal_flow) if optimal_flow > 0 else 0\n                    \n                    metrics[\"flow_efficiency_ratio\"].labels(\n                        camera_id=camera_id,\n                        time_period=scenario[\"time_period\"]\n                    ).set(efficiency)\n                    \n                    # Collect data for analysis\n                    collected_metrics[\"hourly_vehicles\"].append({\n                        \"hour\": hour,\n                        \"camera_id\": camera_id,\n                        \"vehicle_count\": vehicle_count,\n                        \"avg_speed\": avg_speed,\n                        \"density_ratio\": density_ratio,\n                        \"scenario\": scenario[\"time_period\"]\n                    })\n        \n        # Analyze collected metrics\n        analysis_results = {\n            \"total_data_points\": len(collected_metrics[\"hourly_vehicles\"]),\n            \"unique_cameras\": len(set(\n                point[\"camera_id\"] for point in collected_metrics[\"hourly_vehicles\"]\n            )),\n            \"peak_hours\": [],\n            \"avg_metrics_by_scenario\": {}\n        }\n        \n        # Find peak traffic hours\n        hourly_totals = defaultdict(int)\n        for point in collected_metrics[\"hourly_vehicles\"]:\n            hourly_totals[point[\"hour\"]] += point[\"vehicle_count\"]\n        \n        sorted_hours = sorted(hourly_totals.items(), key=lambda x: x[1], reverse=True)\n        analysis_results[\"peak_hours\"] = sorted_hours[:3]\n        \n        # Calculate averages by scenario\n        scenario_groups = defaultdict(list)\n        for point in collected_metrics[\"hourly_vehicles\"]:\n            scenario_groups[point[\"scenario\"]].append(point)\n        \n        for scenario, points in scenario_groups.items():\n            analysis_results[\"avg_metrics_by_scenario\"][scenario] = {\n                \"avg_vehicle_count\": statistics.mean(p[\"vehicle_count\"] for p in points),\n                \"avg_speed\": statistics.mean(p[\"avg_speed\"] for p in points),\n                \"avg_density\": statistics.mean(p[\"density_ratio\"] for p in points),\n                \"data_points\": len(points)\n            }\n        \n        logger.info(\n            f\"Traffic Flow Metrics Collection Results:\\n\"\n            f\"  Total Data Points: {analysis_results['total_data_points']}\\n\"\n            f\"  Unique Cameras: {analysis_results['unique_cameras']}\\n\"\n            f\"  Peak Hours: {[f'{h}:00 ({v} vehicles)' for h, v in analysis_results['peak_hours']]}\\n\"\n            f\"  Scenarios Analyzed: {len(analysis_results['avg_metrics_by_scenario'])}\"\n        )\n        \n        # Validate metrics collection\n        assert analysis_results[\"total_data_points\"] > 0\n        assert analysis_results[\"unique_cameras\"] >= 5\n        assert len(analysis_results[\"peak_hours\"]) == 3\n        assert \"rush_hour\" in analysis_results[\"avg_metrics_by_scenario\"]\n        \n        return analysis_results\n\n    async def test_ml_performance_and_accuracy_metrics(\n        self, business_metrics_registry\n    ):\n        \"\"\"Test ML model performance and accuracy metrics.\"\"\"\n        \n        logger.info(\"Testing ML performance and accuracy metrics\")\n        \n        registry, metrics = business_metrics_registry\n        \n        # Model performance testing\n        model_versions = [\"yolo11n_v1.2\", \"yolo11s_v1.1\", \"yolo11m_v1.0\"]\n        vehicle_types = [\"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\"]\n        camera_ids = [f\"accuracy_test_cam_{i:02d}\" for i in range(5)]\n        \n        # Generate ML accuracy metrics\n        accuracy_results = {}\n        \n        for model_version in model_versions:\n            accuracy_results[model_version] = {}\n            \n            for vehicle_type in vehicle_types:\n                # Simulate different accuracy levels per vehicle type\n                base_accuracy = {\n                    \"car\": 0.95,\n                    \"truck\": 0.92,\n                    \"bus\": 0.88,\n                    \"motorcycle\": 0.85,\n                    \"bicycle\": 0.78\n                }[vehicle_type]\n                \n                # Add model-specific variations\n                model_modifier = {\n                    \"yolo11n_v1.2\": 0.0,\n                    \"yolo11s_v1.1\": 0.02,\n                    \"yolo11m_v1.0\": 0.04\n                }[model_version]\n                \n                accuracy = min(1.0, base_accuracy + model_modifier + random.uniform(-0.05, 0.05))\n                accuracy_results[model_version][vehicle_type] = accuracy\n                \n                # Update accuracy metrics\n                metrics[\"vehicle_classification_accuracy\"].labels(\n                    model_version=model_version,\n                    vehicle_type=vehicle_type\n                ).set(accuracy * 100)  # Convert to percentage\n        \n        # Generate detection confidence scores\n        confidence_data = []\n        \n        for _ in range(1000):  # 1000 detections\n            camera_id = random.choice(camera_ids)\n            vehicle_type = random.choice(vehicle_types)\n            \n            # Simulate confidence score distribution\n            # Higher confidence for easier detections (cars)\n            base_confidence = {\n                \"car\": 0.9,\n                \"truck\": 0.85,\n                \"bus\": 0.8,\n                \"motorcycle\": 0.75,\n                \"bicycle\": 0.7\n            }[vehicle_type]\n            \n            confidence = min(1.0, max(0.5, base_confidence + random.normalvariate(0, 0.1)))\n            \n            metrics[\"detection_confidence_score\"].labels(\n                camera_id=camera_id,\n                vehicle_type=vehicle_type\n            ).observe(confidence)\n            \n            confidence_data.append({\n                \"camera_id\": camera_id,\n                \"vehicle_type\": vehicle_type,\n                \"confidence\": confidence\n            })\n        \n        # Generate false positive metrics\n        false_positive_rates = {}\n        \n        for model_version in model_versions:\n            false_positive_rates[model_version] = {}\n            \n            for detection_type in [\"vehicle\", \"person\", \"object\"]:\n                # Simulate false positive events\n                fp_rate = random.uniform(0.01, 0.08)  # 1-8% false positive rate\n                false_positive_rates[model_version][detection_type] = fp_rate\n                \n                # Generate false positive events\n                for camera_id in camera_ids:\n                    fp_count = int(100 * fp_rate)  # Out of 100 detections\n                    if fp_count > 0:\n                        metrics[\"false_positive_detections_total\"].labels(\n                            camera_id=camera_id,\n                            detection_type=detection_type,\n                            model_version=model_version\n                        ).inc(fp_count)\n        \n        # Analyze confidence score distribution\n        confidence_analysis = {\n            \"total_detections\": len(confidence_data),\n            \"avg_confidence_by_type\": {},\n            \"high_confidence_ratio\": 0,\n            \"low_confidence_ratio\": 0\n        }\n        \n        # Calculate average confidence by vehicle type\n        type_groups = defaultdict(list)\n        for detection in confidence_data:\n            type_groups[detection[\"vehicle_type\"]].append(detection[\"confidence\"])\n        \n        for vehicle_type, confidences in type_groups.items():\n            confidence_analysis[\"avg_confidence_by_type\"][vehicle_type] = statistics.mean(confidences)\n        \n        # Calculate confidence ratios\n        high_confidence_count = sum(1 for d in confidence_data if d[\"confidence\"] >= 0.9)\n        low_confidence_count = sum(1 for d in confidence_data if d[\"confidence\"] < 0.7)\n        \n        confidence_analysis[\"high_confidence_ratio\"] = high_confidence_count / len(confidence_data)\n        confidence_analysis[\"low_confidence_ratio\"] = low_confidence_count / len(confidence_data)\n        \n        # Model comparison analysis\n        model_comparison = {}\n        for model_version, accuracies in accuracy_results.items():\n            model_comparison[model_version] = {\n                \"avg_accuracy\": statistics.mean(accuracies.values()),\n                \"min_accuracy\": min(accuracies.values()),\n                \"max_accuracy\": max(accuracies.values()),\n                \"vehicle_types_tested\": len(accuracies)\n            }\n        \n        logger.info(\n            f\"ML Performance and Accuracy Results:\\n\"\n            f\"  Models Tested: {len(model_versions)}\\n\"\n            f\"  Vehicle Types: {len(vehicle_types)}\\n\"\n            f\"  Total Detections Analyzed: {len(confidence_data)}\\n\"\n            f\"  High Confidence Ratio: {confidence_analysis['high_confidence_ratio']:.1%}\\n\"\n            f\"  Low Confidence Ratio: {confidence_analysis['low_confidence_ratio']:.1%}\\n\"\n            f\"  Best Model: {max(model_comparison.keys(), key=lambda k: model_comparison[k]['avg_accuracy'])}\"\n        )\n        \n        # Validate ML metrics\n        assert len(accuracy_results) == len(model_versions)\n        assert all(0.5 <= acc <= 1.0 for model_accs in accuracy_results.values() for acc in model_accs.values())\n        assert confidence_analysis[\"high_confidence_ratio\"] >= 0.5  # At least 50% high confidence\n        assert confidence_analysis[\"low_confidence_ratio\"] <= 0.2   # Less than 20% low confidence\n        \n        return accuracy_results, confidence_analysis, model_comparison\n\n    async def test_incident_detection_and_response_metrics(\n        self, business_metrics_registry\n    ):\n        \"\"\"Test incident detection and response time metrics.\"\"\"\n        \n        logger.info(\"Testing incident detection and response metrics\")\n        \n        registry, metrics = business_metrics_registry\n        \n        # Incident types and their characteristics\n        incident_types = {\n            \"accident\": {\n                \"detection_time_range\": (10, 120),  # seconds\n                \"response_time_range\": (120, 600),  # seconds\n                \"resolution_time_range\": (30, 240),  # minutes\n                \"severity_distribution\": {\"low\": 0.3, \"medium\": 0.5, \"high\": 0.2}\n            },\n            \"breakdown\": {\n                \"detection_time_range\": (30, 300),\n                \"response_time_range\": (300, 1200),\n                \"resolution_time_range\": (15, 120),\n                \"severity_distribution\": {\"low\": 0.6, \"medium\": 0.3, \"high\": 0.1}\n            },\n            \"congestion\": {\n                \"detection_time_range\": (60, 600),\n                \"response_time_range\": (180, 900),\n                \"resolution_time_range\": (45, 180),\n                \"severity_distribution\": {\"low\": 0.4, \"medium\": 0.4, \"high\": 0.2}\n            },\n            \"debris\": {\n                \"detection_time_range\": (5, 60),\n                \"response_time_range\": (60, 300),\n                \"resolution_time_range\": (10, 60),\n                \"severity_distribution\": {\"low\": 0.7, \"medium\": 0.2, \"high\": 0.1}\n            },\n            \"weather_related\": {\n                \"detection_time_range\": (120, 900),\n                \"response_time_range\": (300, 1800),\n                \"resolution_time_range\": (60, 480),\n                \"severity_distribution\": {\"low\": 0.2, \"medium\": 0.5, \"high\": 0.3}\n            }\n        }\n        \n        camera_ids = [f\"incident_cam_{i:02d}\" for i in range(8)]\n        response_teams = [\"traffic_control\", \"emergency_services\", \"maintenance\", \"police\"]\n        \n        # Generate incident events\n        incidents_generated = []\n        \n        for _ in range(100):  # Generate 100 incidents\n            incident_type = random.choice(list(incident_types.keys()))\n            incident_config = incident_types[incident_type]\n            camera_id = random.choice(camera_ids)\n            \n            # Determine severity\n            severity_rand = random.random()\n            severity = \"low\"\n            cumulative_prob = 0\n            for sev, prob in incident_config[\"severity_distribution\"].items():\n                cumulative_prob += prob\n                if severity_rand <= cumulative_prob:\n                    severity = sev\n                    break\n            \n            # Generate timing metrics\n            detection_time = random.uniform(*incident_config[\"detection_time_range\"])\n            response_time = random.uniform(*incident_config[\"response_time_range\"])\n            resolution_time = random.uniform(*incident_config[\"resolution_time_range\"])\n            response_team = random.choice(response_teams)\n            \n            # Record incident metrics\n            metrics[\"incidents_detected_total\"].labels(\n                camera_id=camera_id,\n                incident_type=incident_type,\n                severity=severity\n            ).inc()\n            \n            metrics[\"incident_detection_time_seconds\"].labels(\n                incident_type=incident_type\n            ).observe(detection_time)\n            \n            metrics[\"incident_response_time_seconds\"].labels(\n                incident_type=incident_type,\n                response_team=response_team\n            ).observe(response_time)\n            \n            metrics[\"incident_resolution_time_minutes\"].labels(\n                incident_type=incident_type\n            ).observe(resolution_time)\n            \n            # Store incident data for analysis\n            incident_data = {\n                \"incident_type\": incident_type,\n                \"camera_id\": camera_id,\n                \"severity\": severity,\n                \"detection_time\": detection_time,\n                \"response_time\": response_time,\n                \"resolution_time\": resolution_time,\n                \"response_team\": response_team,\n                \"timestamp\": datetime.now(UTC)\n            }\n            incidents_generated.append(incident_data)\n        \n        # Analyze incident response performance\n        response_analysis = {\n            \"total_incidents\": len(incidents_generated),\n            \"incidents_by_type\": defaultdict(int),\n            \"incidents_by_severity\": defaultdict(int),\n            \"avg_detection_time_by_type\": {},\n            \"avg_response_time_by_team\": {},\n            \"avg_resolution_time_by_type\": {},\n            \"sla_compliance\": {}\n        }\n        \n        # Count by type and severity\n        for incident in incidents_generated:\n            response_analysis[\"incidents_by_type\"][incident[\"incident_type\"]] += 1\n            response_analysis[\"incidents_by_severity\"][incident[\"severity\"]] += 1\n        \n        # Calculate averages by type\n        type_groups = defaultdict(list)\n        for incident in incidents_generated:\n            type_groups[incident[\"incident_type\"]].append(incident)\n        \n        for incident_type, incidents in type_groups.items():\n            response_analysis[\"avg_detection_time_by_type\"][incident_type] = statistics.mean(\n                i[\"detection_time\"] for i in incidents\n            )\n            response_analysis[\"avg_resolution_time_by_type\"][incident_type] = statistics.mean(\n                i[\"resolution_time\"] for i in incidents\n            )\n        \n        # Calculate averages by response team\n        team_groups = defaultdict(list)\n        for incident in incidents_generated:\n            team_groups[incident[\"response_team\"]].append(incident[\"response_time\"])\n        \n        for team, response_times in team_groups.items():\n            response_analysis[\"avg_response_time_by_team\"][team] = statistics.mean(response_times)\n        \n        # SLA compliance analysis\n        sla_thresholds = {\n            \"max_detection_time\": 300,  # 5 minutes\n            \"max_response_time\": 600,   # 10 minutes\n            \"max_resolution_time\": 120  # 2 hours\n        }\n        \n        compliance_counts = {\n            \"detection_sla_met\": 0,\n            \"response_sla_met\": 0,\n            \"resolution_sla_met\": 0\n        }\n        \n        for incident in incidents_generated:\n            if incident[\"detection_time\"] <= sla_thresholds[\"max_detection_time\"]:\n                compliance_counts[\"detection_sla_met\"] += 1\n            \n            if incident[\"response_time\"] <= sla_thresholds[\"max_response_time\"]:\n                compliance_counts[\"response_sla_met\"] += 1\n            \n            if incident[\"resolution_time\"] <= sla_thresholds[\"max_resolution_time\"]:\n                compliance_counts[\"resolution_sla_met\"] += 1\n        \n        total_incidents = len(incidents_generated)\n        response_analysis[\"sla_compliance\"] = {\n            \"detection_compliance\": compliance_counts[\"detection_sla_met\"] / total_incidents,\n            \"response_compliance\": compliance_counts[\"response_sla_met\"] / total_incidents,\n            \"resolution_compliance\": compliance_counts[\"resolution_sla_met\"] / total_incidents\n        }\n        \n        logger.info(\n            f\"Incident Detection and Response Results:\\n\"\n            f\"  Total Incidents: {response_analysis['total_incidents']}\\n\"\n            f\"  Most Common Type: {max(response_analysis['incidents_by_type'], key=response_analysis['incidents_by_type'].get)}\\n\"\n            f\"  Detection SLA Compliance: {response_analysis['sla_compliance']['detection_compliance']:.1%}\\n\"\n            f\"  Response SLA Compliance: {response_analysis['sla_compliance']['response_compliance']:.1%}\\n\"\n            f\"  Resolution SLA Compliance: {response_analysis['sla_compliance']['resolution_compliance']:.1%}\\n\"\n            f\"  Fastest Response Team: {min(response_analysis['avg_response_time_by_team'], key=response_analysis['avg_response_time_by_team'].get)}\"\n        )\n        \n        # Validate incident metrics\n        assert response_analysis[\"total_incidents\"] == 100\n        assert len(response_analysis[\"incidents_by_type\"]) >= 3\n        assert all(comp >= 0.5 for comp in response_analysis[\"sla_compliance\"].values())  # At least 50% compliance\n        \n        return response_analysis, incidents_generated\n\n    async def test_business_value_and_roi_metrics(\n        self, business_metrics_registry\n    ):\n        \"\"\"Test business value and ROI metrics calculation.\"\"\"\n        \n        logger.info(\"Testing business value and ROI metrics\")\n        \n        registry, metrics = business_metrics_registry\n        \n        # Business value metrics\n        camera_ids = [f\"roi_camera_{i:02d}\" for i in range(6)]\n        service_tiers = [\"basic\", \"standard\", \"premium\"]\n        optimization_types = [\"signal_timing\", \"route_optimization\", \"congestion_reduction\", \"incident_response\"]\n        \n        # Cost analysis\n        cost_analysis = {\n            \"operational_costs\": {},\n            \"processing_costs\": {},\n            \"infrastructure_costs\": {}\n        }\n        \n        for camera_id in camera_ids:\n            for service_tier in service_tiers:\n                # Calculate cost per detection based on service tier\n                base_cost = {\n                    \"basic\": 0.05,\n                    \"standard\": 0.08,\n                    \"premium\": 0.12\n                }[service_tier]\n                \n                # Add camera-specific variations\n                camera_modifier = random.uniform(0.8, 1.2)\n                cost_per_detection = base_cost * camera_modifier\n                \n                metrics[\"cost_per_detection_usd\"].labels(\n                    camera_id=camera_id,\n                    service_tier=service_tier\n                ).set(cost_per_detection)\n                \n                cost_analysis[\"operational_costs\"][f\"{camera_id}_{service_tier}\"] = cost_per_detection\n        \n        # Revenue impact calculation\n        revenue_data = []\n        \n        for optimization_type in optimization_types:\n            for time_period in [\"daily\", \"weekly\", \"monthly\"]:\n                # Simulate revenue impact from traffic optimization\n                base_impact = {\n                    \"signal_timing\": {\"daily\": 500, \"weekly\": 3000, \"monthly\": 12000},\n                    \"route_optimization\": {\"daily\": 800, \"weekly\": 5000, \"monthly\": 20000},\n                    \"congestion_reduction\": {\"daily\": 1200, \"weekly\": 7500, \"monthly\": 30000},\n                    \"incident_response\": {\"daily\": 300, \"weekly\": 2000, \"monthly\": 8000}\n                }[optimization_type][time_period]\n                \n                # Add seasonal and random variations\n                impact_variation = random.uniform(0.7, 1.3)\n                revenue_impact = base_impact * impact_variation\n                \n                metrics[\"revenue_impact_usd\"].labels(\n                    optimization_type=optimization_type,\n                    time_period=time_period\n                ).inc(revenue_impact)\n                \n                revenue_data.append({\n                    \"optimization_type\": optimization_type,\n                    \"time_period\": time_period,\n                    \"revenue_impact\": revenue_impact\n                })\n        \n        # Fuel savings calculation\n        fuel_savings_data = []\n        optimization_methods = [\"reduced_idling\", \"optimized_routing\", \"smooth_flow\", \"incident_clearance\"]\n        \n        for camera_id in camera_ids:\n            for method in optimization_methods:\n                # Calculate fuel savings based on optimization method\n                base_savings = {\n                    \"reduced_idling\": 50,\n                    \"optimized_routing\": 75,\n                    \"smooth_flow\": 100,\n                    \"incident_clearance\": 25\n                }[method]\n                \n                # Daily fuel savings in liters\n                daily_savings = base_savings * random.uniform(0.8, 1.5)\n                \n                metrics[\"fuel_savings_liters\"].labels(\n                    camera_id=camera_id,\n                    optimization_method=method\n                ).inc(daily_savings)\n                \n                fuel_savings_data.append({\n                    \"camera_id\": camera_id,\n                    \"method\": method,\n                    \"daily_savings_liters\": daily_savings\n                })\n        \n        # ROI calculation\n        roi_analysis = {\n            \"total_operational_cost\": sum(cost_analysis[\"operational_costs\"].values()) * 1000,  # Scale up\n            \"total_revenue_impact\": sum(r[\"revenue_impact\"] for r in revenue_data if r[\"time_period\"] == \"monthly\"),\n            \"total_fuel_savings_value\": 0,\n            \"roi_percentage\": 0,\n            \"payback_period_months\": 0\n        }\n        \n        # Calculate fuel savings value (assume $1.5 per liter)\n        fuel_price_per_liter = 1.5\n        monthly_fuel_savings_liters = sum(f[\"daily_savings_liters\"] for f in fuel_savings_data) * 30\n        roi_analysis[\"total_fuel_savings_value\"] = monthly_fuel_savings_liters * fuel_price_per_liter\n        \n        # Total monthly benefits\n        total_monthly_benefits = (roi_analysis[\"total_revenue_impact\"] + \n                                roi_analysis[\"total_fuel_savings_value\"])\n        \n        # ROI calculation\n        if roi_analysis[\"total_operational_cost\"] > 0:\n            roi_analysis[\"roi_percentage\"] = (\n                (total_monthly_benefits - roi_analysis[\"total_operational_cost\"]) / \n                roi_analysis[\"total_operational_cost\"]\n            ) * 100\n            \n            roi_analysis[\"payback_period_months\"] = (\n                roi_analysis[\"total_operational_cost\"] / total_monthly_benefits\n            ) if total_monthly_benefits > 0 else float('inf')\n        \n        # Environmental impact metrics\n        environmental_impact = {\n            \"co2_reduction_kg_monthly\": monthly_fuel_savings_liters * 2.3,  # kg CO2 per liter\n            \"fuel_efficiency_improvement\": random.uniform(0.05, 0.15),  # 5-15% improvement\n            \"emission_reduction_percentage\": random.uniform(0.08, 0.20)  # 8-20% reduction\n        }\n        \n        # Service tier performance comparison\n        tier_performance = {}\n        for tier in service_tiers:\n            tier_costs = [cost for key, cost in cost_analysis[\"operational_costs\"].items() if tier in key]\n            tier_performance[tier] = {\n                \"avg_cost_per_detection\": statistics.mean(tier_costs),\n                \"cameras_using_tier\": len(tier_costs),\n                \"cost_efficiency_score\": 1 / statistics.mean(tier_costs) if tier_costs else 0\n            }\n        \n        logger.info(\n            f\"Business Value and ROI Results:\\n\"\n            f\"  Monthly Revenue Impact: ${roi_analysis['total_revenue_impact']:,.2f}\\n\"\n            f\"  Monthly Operational Cost: ${roi_analysis['total_operational_cost']:,.2f}\\n\"\n            f\"  Monthly Fuel Savings Value: ${roi_analysis['total_fuel_savings_value']:,.2f}\\n\"\n            f\"  ROI Percentage: {roi_analysis['roi_percentage']:.1f}%\\n\"\n            f\"  Payback Period: {roi_analysis['payback_period_months']:.1f} months\\n\"\n            f\"  CO2 Reduction: {environmental_impact['co2_reduction_kg_monthly']:,.1f} kg/month\\n\"\n            f\"  Best Service Tier: {max(tier_performance.keys(), key=lambda k: tier_performance[k]['cost_efficiency_score'])}\"\n        )\n        \n        # Validate business metrics\n        assert roi_analysis[\"total_revenue_impact\"] > 0\n        assert roi_analysis[\"total_operational_cost\"] > 0\n        assert roi_analysis[\"roi_percentage\"] > -100  # ROI should be reasonable\n        assert environmental_impact[\"co2_reduction_kg_monthly\"] > 0\n        assert len(tier_performance) == 3\n        \n        return roi_analysis, environmental_impact, tier_performance