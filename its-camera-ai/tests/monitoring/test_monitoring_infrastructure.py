"""Monitoring and Observability Infrastructure Tests.

This test suite validates the monitoring and observability setup:
- Prometheus metrics collection and scraping
- Grafana dashboard functionality
- Custom business metrics tracking
- Alert manager configuration
- Distributed tracing with OpenTelemetry
- Log aggregation and analysis
- SLA monitoring and reporting
"""

import asyncio
import json
import random
import time
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, MagicMock, patch
from uuid import uuid4

import pytest
import pytest_asyncio
import httpx
from prometheus_client import CollectorRegistry, Counter, Histogram, Gauge, generate_latest
from opentelemetry import trace
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.trace import TracerProvider

from its_camera_ai.core.logging import get_logger
from its_camera_ai.core.configs.monitoring import MonitoringConfig
from its_camera_ai.performance.latency_monitor import LatencyMonitor
from its_camera_ai.services.analytics_dtos import ProcessingResult

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.monitoring
@pytest.mark.asyncio
class TestMonitoringInfrastructure:
    """Integration tests for monitoring and observability infrastructure."""

    @pytest_asyncio.fixture
    async def monitoring_config(self):
        \"\"\"Monitoring configuration for tests.\"\"\"\n        return MonitoringConfig(\n            prometheus_enabled=True,\n            prometheus_port=9090,\n            prometheus_path=\"/metrics\",\n            grafana_enabled=True,\n            grafana_port=3000,\n            alert_manager_enabled=True,\n            alert_manager_port=9093,\n            tracing_enabled=True,\n            jaeger_endpoint=\"http://localhost:14268/api/traces\",\n            log_level=\"INFO\",\n            metrics_collection_interval=15,\n            retention_period_days=30\n        )\n\n    @pytest_asyncio.fixture\n    async def prometheus_registry(self):\n        \"\"\"Prometheus metrics registry for testing.\"\"\"\n        registry = CollectorRegistry()\n        \n        # Define custom metrics for ITS Camera AI\n        metrics = {\n            \"frames_processed_total\": Counter(\n                \"its_frames_processed_total\",\n                \"Total number of camera frames processed\",\n                [\"camera_id\", \"status\"],\n                registry=registry\n            ),\n            \"inference_duration_seconds\": Histogram(\n                \"its_inference_duration_seconds\",\n                \"Time spent on ML inference\",\n                [\"model_version\", \"camera_id\"],\n                registry=registry\n            ),\n            \"vehicles_detected_total\": Counter(\n                \"its_vehicles_detected_total\",\n                \"Total number of vehicles detected\",\n                [\"camera_id\", \"vehicle_type\"],\n                registry=registry\n            ),\n            \"active_cameras\": Gauge(\n                \"its_active_cameras\",\n                \"Number of currently active cameras\",\n                registry=registry\n            ),\n            \"processing_queue_size\": Gauge(\n                \"its_processing_queue_size\",\n                \"Current size of processing queue\",\n                [\"queue_type\"],\n                registry=registry\n            ),\n            \"api_requests_total\": Counter(\n                \"its_api_requests_total\",\n                \"Total API requests\",\n                [\"method\", \"endpoint\", \"status_code\"],\n                registry=registry\n            ),\n            \"api_request_duration_seconds\": Histogram(\n                \"its_api_request_duration_seconds\",\n                \"API request duration\",\n                [\"method\", \"endpoint\"],\n                registry=registry\n            ),\n            \"cache_operations_total\": Counter(\n                \"its_cache_operations_total\",\n                \"Total cache operations\",\n                [\"operation\", \"cache_level\", \"status\"],\n                registry=registry\n            ),\n            \"database_connections\": Gauge(\n                \"its_database_connections\",\n                \"Current database connections\",\n                [\"pool_name\"],\n                registry=registry\n            ),\n            \"alert_notifications_total\": Counter(\n                \"its_alert_notifications_total\",\n                \"Total alert notifications sent\",\n                [\"alert_type\", \"severity\", \"channel\"],\n                registry=registry\n            )\n        }\n        \n        return registry, metrics\n\n    @pytest_asyncio.fixture\n    async def latency_monitor(self):\n        \"\"\"Latency monitoring service.\"\"\"\n        return LatencyMonitor(\n            enable_detailed_tracking=True,\n            retention_minutes=60,\n            alert_threshold_ms=1000\n        )\n\n    async def test_prometheus_metrics_collection(\n        self, prometheus_registry, monitoring_config\n    ):\n        \"\"\"Test Prometheus metrics collection and scraping.\"\"\"\n        \n        logger.info(\"Testing Prometheus metrics collection\")\n        \n        registry, metrics = prometheus_registry\n        \n        # Simulate system activity to generate metrics\n        camera_ids = [f\"test_camera_{i:02d}\" for i in range(5)]\n        vehicle_types = [\"car\", \"truck\", \"bus\", \"motorcycle\"]\n        \n        # Generate frame processing metrics\n        for _ in range(100):\n            camera_id = random.choice(camera_ids)\n            status = random.choice([\"success\", \"error\", \"timeout\"])\n            \n            metrics[\"frames_processed_total\"].labels(\n                camera_id=camera_id, status=status\n            ).inc()\n            \n            # Simulate inference timing\n            inference_time = random.uniform(0.02, 0.15)  # 20-150ms\n            metrics[\"inference_duration_seconds\"].labels(\n                model_version=\"yolo11n_v1.2\", camera_id=camera_id\n            ).observe(inference_time)\n            \n            # Vehicle detection metrics\n            if status == \"success\":\n                num_vehicles = random.randint(0, 15)\n                for _ in range(num_vehicles):\n                    vehicle_type = random.choice(vehicle_types)\n                    metrics[\"vehicles_detected_total\"].labels(\n                        camera_id=camera_id, vehicle_type=vehicle_type\n                    ).inc()\n        \n        # Set gauge metrics\n        metrics[\"active_cameras\"].set(len(camera_ids))\n        metrics[\"processing_queue_size\"].labels(queue_type=\"ml_inference\").set(25)\n        metrics[\"processing_queue_size\"].labels(queue_type=\"analytics\").set(12)\n        metrics[\"database_connections\"].labels(pool_name=\"primary\").set(15)\n        \n        # Generate API metrics\n        api_endpoints = [\"/api/v1/analytics/real-time\", \"/api/v1/cameras\", \"/api/v1/incidents\"]\n        methods = [\"GET\", \"POST\", \"PUT\"]\n        status_codes = [\"200\", \"400\", \"404\", \"500\"]\n        \n        for _ in range(50):\n            method = random.choice(methods)\n            endpoint = random.choice(api_endpoints)\n            status_code = random.choice(status_codes)\n            \n            metrics[\"api_requests_total\"].labels(\n                method=method, endpoint=endpoint, status_code=status_code\n            ).inc()\n            \n            # API timing\n            duration = random.uniform(0.01, 0.5)\n            metrics[\"api_request_duration_seconds\"].labels(\n                method=method, endpoint=endpoint\n            ).observe(duration)\n        \n        # Generate cache metrics\n        cache_operations = [\"get\", \"set\", \"delete\", \"invalidate\"]\n        cache_levels = [\"l1\", \"l2\"]\n        cache_statuses = [\"hit\", \"miss\", \"error\"]\n        \n        for _ in range(75):\n            operation = random.choice(cache_operations)\n            level = random.choice(cache_levels)\n            status = random.choice(cache_statuses)\n            \n            metrics[\"cache_operations_total\"].labels(\n                operation=operation, cache_level=level, status=status\n            ).inc()\n        \n        # Generate alert metrics\n        alert_types = [\"traffic_congestion\", \"camera_offline\", \"high_latency\", \"error_rate\"]\n        severities = [\"low\", \"medium\", \"high\", \"critical\"]\n        channels = [\"email\", \"slack\", \"webhook\", \"sms\"]\n        \n        for _ in range(10):\n            alert_type = random.choice(alert_types)\n            severity = random.choice(severities)\n            channel = random.choice(channels)\n            \n            metrics[\"alert_notifications_total\"].labels(\n                alert_type=alert_type, severity=severity, channel=channel\n            ).inc()\n        \n        # Export metrics in Prometheus format\n        metrics_output = generate_latest(registry)\n        \n        # Validate metrics format\n        assert b\"its_frames_processed_total\" in metrics_output\n        assert b\"its_inference_duration_seconds\" in metrics_output\n        assert b\"its_vehicles_detected_total\" in metrics_output\n        assert b\"its_active_cameras\" in metrics_output\n        assert b\"its_api_requests_total\" in metrics_output\n        \n        # Parse and validate specific metrics\n        metrics_text = metrics_output.decode('utf-8')\n        \n        # Check for proper labeling\n        assert 'camera_id=\"test_camera_' in metrics_text\n        assert 'vehicle_type=\"car\"' in metrics_text\n        assert 'status=\"success\"' in metrics_text\n        assert 'model_version=\"yolo11n_v1.2\"' in metrics_text\n        \n        # Validate metric types\n        assert \"# TYPE its_frames_processed_total counter\" in metrics_text\n        assert \"# TYPE its_inference_duration_seconds histogram\" in metrics_text\n        assert \"# TYPE its_active_cameras gauge\" in metrics_text\n        \n        logger.info(\n            f\"Prometheus Metrics Collection Results:\\n\"\n            f\"  Metrics Output Size: {len(metrics_output)} bytes\\n\"\n            f\"  Frame Processing Metrics: Generated\\n\"\n            f\"  API Metrics: Generated\\n\"\n            f\"  Cache Metrics: Generated\\n\"\n            f\"  Alert Metrics: Generated\\n\"\n            f\"  Format Validation: Passed\"\n        )\n        \n        return metrics_output\n\n    async def test_custom_business_metrics_tracking(\n        self, prometheus_registry, latency_monitor\n    ):\n        \"\"\"Test custom business metrics for traffic analytics.\"\"\"\n        \n        logger.info(\"Testing custom business metrics tracking\")\n        \n        registry, base_metrics = prometheus_registry\n        \n        # Define business-specific metrics\n        business_metrics = {\n            \"traffic_violations_total\": Counter(\n                \"its_traffic_violations_total\",\n                \"Total traffic violations detected\",\n                [\"camera_id\", \"violation_type\", \"severity\"],\n                registry=registry\n            ),\n            \"congestion_level_current\": Gauge(\n                \"its_congestion_level_current\",\n                \"Current congestion level (0-5 scale)\",\n                [\"camera_id\", \"road_segment\"],\n                registry=registry\n            ),\n            \"average_speed_kmh\": Gauge(\n                \"its_average_speed_kmh\",\n                \"Average vehicle speed in km/h\",\n                [\"camera_id\", \"time_period\"],\n                registry=registry\n            ),\n            \"incident_detection_time_seconds\": Histogram(\n                \"its_incident_detection_time_seconds\",\n                \"Time from occurrence to detection\",\n                [\"incident_type\"],\n                registry=registry\n            ),\n            \"camera_uptime_ratio\": Gauge(\n                \"its_camera_uptime_ratio\",\n                \"Camera uptime ratio (0-1)\",\n                [\"camera_id\"],\n                registry=registry\n            ),\n            \"false_positive_rate\": Gauge(\n                \"its_false_positive_rate\",\n                \"ML model false positive rate\",\n                [\"model_version\", \"detection_type\"],\n                registry=registry\n            )\n        }\n        \n        # Simulate business metrics collection\n        camera_ids = [f\"business_camera_{i:02d}\" for i in range(3)]\n        violation_types = [\"speeding\", \"wrong_lane\", \"running_red_light\", \"illegal_turn\"]\n        incident_types = [\"accident\", \"breakdown\", \"congestion\", \"debris\"]\n        \n        # Traffic violations\n        for _ in range(30):\n            camera_id = random.choice(camera_ids)\n            violation_type = random.choice(violation_types)\n            severity = random.choice([\"minor\", \"major\", \"critical\"])\n            \n            business_metrics[\"traffic_violations_total\"].labels(\n                camera_id=camera_id, violation_type=violation_type, severity=severity\n            ).inc()\n        \n        # Congestion and speed metrics\n        for camera_id in camera_ids:\n            road_segment = f\"segment_{camera_id.split('_')[-1]}\"\n            \n            # Congestion level (0-5 scale)\n            congestion_level = random.uniform(0, 5)\n            business_metrics[\"congestion_level_current\"].labels(\n                camera_id=camera_id, road_segment=road_segment\n            ).set(congestion_level)\n            \n            # Average speeds for different time periods\n            for time_period in [\"5min\", \"15min\", \"1hour\"]:\n                avg_speed = random.uniform(20, 80)\n                business_metrics[\"average_speed_kmh\"].labels(\n                    camera_id=camera_id, time_period=time_period\n                ).set(avg_speed)\n            \n            # Camera uptime\n            uptime_ratio = random.uniform(0.95, 1.0)\n            business_metrics[\"camera_uptime_ratio\"].labels(\n                camera_id=camera_id\n            ).set(uptime_ratio)\n        \n        # Incident detection timing\n        for _ in range(15):\n            incident_type = random.choice(incident_types)\n            detection_time = random.uniform(5, 120)  # 5 seconds to 2 minutes\n            \n            business_metrics[\"incident_detection_time_seconds\"].labels(\n                incident_type=incident_type\n            ).observe(detection_time)\n        \n        # ML model performance metrics\n        model_versions = [\"yolo11n_v1.2\", \"yolo11s_v1.1\", \"yolo11m_v1.0\"]\n        detection_types = [\"vehicle\", \"person\", \"bicycle\"]\n        \n        for model_version in model_versions:\n            for detection_type in detection_types:\n                false_positive_rate = random.uniform(0.01, 0.15)\n                business_metrics[\"false_positive_rate\"].labels(\n                    model_version=model_version, detection_type=detection_type\n                ).set(false_positive_rate)\n        \n        # Test latency tracking for business operations\n        business_operations = [\n            \"violation_detection\",\n            \"congestion_analysis\",\n            \"incident_classification\",\n            \"speed_calculation\"\n        ]\n        \n        latency_measurements = []\n        \n        for operation in business_operations:\n            for _ in range(10):\n                # Simulate operation timing\n                start_time = time.time()\n                await asyncio.sleep(random.uniform(0.01, 0.1))  # Simulate work\n                end_time = time.time()\n                \n                latency_ms = (end_time - start_time) * 1000\n                latency_measurements.append((operation, latency_ms))\n                \n                # Track in latency monitor\n                latency_monitor.record_latency(operation, latency_ms)\n        \n        # Get latency statistics\n        latency_stats = {}\n        for operation in business_operations:\n            stats = latency_monitor.get_latency_stats(operation)\n            if stats:\n                latency_stats[operation] = stats\n        \n        # Export business metrics\n        business_metrics_output = generate_latest(registry)\n        business_metrics_text = business_metrics_output.decode('utf-8')\n        \n        # Validate business metrics\n        assert \"its_traffic_violations_total\" in business_metrics_text\n        assert \"its_congestion_level_current\" in business_metrics_text\n        assert \"its_average_speed_kmh\" in business_metrics_text\n        assert \"its_incident_detection_time_seconds\" in business_metrics_text\n        assert \"its_camera_uptime_ratio\" in business_metrics_text\n        assert \"its_false_positive_rate\" in business_metrics_text\n        \n        # Check specific business metric values\n        assert 'violation_type=\"speeding\"' in business_metrics_text\n        assert 'incident_type=\"accident\"' in business_metrics_text\n        assert 'model_version=\"yolo11n_v1.2\"' in business_metrics_text\n        \n        logger.info(\n            f\"Custom Business Metrics Results:\\n\"\n            f\"  Traffic Violations: Generated\\n\"\n            f\"  Congestion Metrics: Generated\\n\"\n            f\"  Speed Metrics: Generated\\n\"\n            f\"  Incident Metrics: Generated\\n\"\n            f\"  ML Performance Metrics: Generated\\n\"\n            f\"  Latency Measurements: {len(latency_measurements)}\\n\"\n            f\"  Latency Stats: {len(latency_stats)} operations tracked\"\n        )\n\n    async def test_distributed_tracing_integration(\n        self, monitoring_config\n    ):\n        \"\"\"Test distributed tracing with OpenTelemetry.\"\"\"\n        \n        logger.info(\"Testing distributed tracing integration\")\n        \n        # Setup OpenTelemetry tracing\n        tracer_provider = TracerProvider()\n        trace.set_tracer_provider(tracer_provider)\n        tracer = trace.get_tracer(__name__)\n        \n        # Simulate distributed request flow\n        trace_data = []\n        \n        async def simulate_api_request(request_id: str):\n            \"\"\"Simulate API request with distributed tracing.\"\"\"\n            with tracer.start_as_current_span(\"api_request\") as api_span:\n                api_span.set_attribute(\"request.id\", request_id)\n                api_span.set_attribute(\"request.method\", \"POST\")\n                api_span.set_attribute(\"request.endpoint\", \"/api/v1/analytics/real-time\")\n                \n                # Simulate authentication\n                with tracer.start_as_current_span(\"authentication\") as auth_span:\n                    auth_span.set_attribute(\"auth.method\", \"jwt\")\n                    await asyncio.sleep(0.01)  # Auth time\n                    auth_span.set_attribute(\"auth.success\", True)\n                \n                # Simulate cache lookup\n                with tracer.start_as_current_span(\"cache_lookup\") as cache_span:\n                    cache_span.set_attribute(\"cache.key\", f\"analytics:{request_id}\")\n                    cache_span.set_attribute(\"cache.level\", \"l1\")\n                    await asyncio.sleep(0.001)  # Cache time\n                    cache_hit = random.choice([True, False])\n                    cache_span.set_attribute(\"cache.hit\", cache_hit)\n                    \n                    if not cache_hit:\n                        # Simulate database query\n                        with tracer.start_as_current_span(\"database_query\") as db_span:\n                            db_span.set_attribute(\"db.operation\", \"SELECT\")\n                            db_span.set_attribute(\"db.table\", \"traffic_data\")\n                            await asyncio.sleep(0.05)  # DB time\n                            db_span.set_attribute(\"db.rows_returned\", random.randint(10, 100))\n                        \n                        # Simulate ML inference\n                        with tracer.start_as_current_span(\"ml_inference\") as ml_span:\n                            ml_span.set_attribute(\"ml.model\", \"yolo11n_v1.2\")\n                            ml_span.set_attribute(\"ml.input_size\", \"640x640\")\n                            await asyncio.sleep(0.08)  # ML time\n                            ml_span.set_attribute(\"ml.detections_count\", random.randint(0, 15))\n                \n                # Simulate response generation\n                with tracer.start_as_current_span(\"response_generation\") as resp_span:\n                    resp_span.set_attribute(\"response.format\", \"json\")\n                    await asyncio.sleep(0.005)  # Response time\n                    resp_span.set_attribute(\"response.size_bytes\", random.randint(1000, 5000))\n                \n                api_span.set_attribute(\"response.status_code\", 200)\n                api_span.set_attribute(\"response.success\", True)\n                \n                # Collect trace data\n                trace_info = {\n                    \"request_id\": request_id,\n                    \"trace_id\": format(api_span.get_span_context().trace_id, '032x'),\n                    \"span_id\": format(api_span.get_span_context().span_id, '016x'),\n                    \"spans_created\": 5  # api, auth, cache, db, ml, response\n                }\n                trace_data.append(trace_info)\n        \n        # Simulate multiple concurrent requests\n        request_tasks = []\n        for i in range(10):\n            request_id = f\"req_{i:03d}_{uuid4().hex[:8]}\"\n            task = simulate_api_request(request_id)\n            request_tasks.append(task)\n        \n        # Execute tracing simulation\n        tracing_start = time.time()\n        await asyncio.gather(*request_tasks)\n        tracing_time = time.time() - tracing_start\n        \n        # Validate trace data\n        assert len(trace_data) == 10, \"Should have 10 traces\"\n        \n        # Check trace ID uniqueness\n        trace_ids = [trace[\"trace_id\"] for trace in trace_data]\n        unique_trace_ids = set(trace_ids)\n        assert len(unique_trace_ids) == len(trace_ids), \"All trace IDs should be unique\"\n        \n        # Validate trace ID format (32-character hex)\n        for trace_id in trace_ids:\n            assert len(trace_id) == 32, f\"Invalid trace ID length: {trace_id}\"\n            assert all(c in '0123456789abcdef' for c in trace_id), f\"Invalid trace ID format: {trace_id}\"\n        \n        # Simulate error trace\n        with tracer.start_as_current_span(\"error_request\") as error_span:\n            error_span.set_attribute(\"request.method\", \"GET\")\n            error_span.set_attribute(\"request.endpoint\", \"/api/v1/nonexistent\")\n            \n            # Simulate error\n            error_span.record_exception(Exception(\"Not Found\"))\n            error_span.set_status(trace.Status(trace.StatusCode.ERROR, \"Endpoint not found\"))\n            error_span.set_attribute(\"response.status_code\", 404)\n        \n        logger.info(\n            f\"Distributed Tracing Results:\\n\"\n            f\"  Traces Generated: {len(trace_data)}\\n\"\n            f\"  Unique Trace IDs: {len(unique_trace_ids)}\\n\"\n            f\"  Total Tracing Time: {tracing_time:.3f}s\\n\"\n            f\"  Avg Time per Trace: {tracing_time / len(trace_data):.3f}s\\n\"\n            f\"  Spans per Trace: 5\\n\"\n            f\"  Error Tracing: Tested\"\n        )\n        \n        return trace_data\n\n    async def test_alerting_and_sla_monitoring(\n        self, prometheus_registry, monitoring_config\n    ):\n        \"\"\"Test alerting configuration and SLA monitoring.\"\"\"\n        \n        logger.info(\"Testing alerting and SLA monitoring\")\n        \n        registry, metrics = prometheus_registry\n        \n        # Define SLA metrics\n        sla_metrics = {\n            \"sla_api_availability\": Gauge(\n                \"its_sla_api_availability\",\n                \"API availability percentage\",\n                [\"service\"],\n                registry=registry\n            ),\n            \"sla_response_time_p95\": Gauge(\n                \"its_sla_response_time_p95_seconds\",\n                \"95th percentile response time\",\n                [\"service\", \"endpoint\"],\n                registry=registry\n            ),\n            \"sla_error_rate\": Gauge(\n                \"its_sla_error_rate\",\n                \"Service error rate percentage\",\n                [\"service\"],\n                registry=registry\n            ),\n            \"sla_processing_success_rate\": Gauge(\n                \"its_sla_processing_success_rate\",\n                \"Processing success rate percentage\",\n                [\"service\", \"operation\"],\n                registry=registry\n            )\n        }\n        \n        # Simulate SLA measurements\n        services = [\"api_gateway\", \"ml_inference\", \"analytics\", \"database\"]\n        \n        # API availability (should be > 99.9%)\n        for service in services:\n            availability = random.uniform(99.5, 100.0)\n            sla_metrics[\"sla_api_availability\"].labels(service=service).set(availability)\n        \n        # Response time P95 (should be < 200ms for API, < 100ms for ML)\n        api_endpoints = [\"/real-time\", \"/historical\", \"/incidents\", \"/cameras\"]\n        for service in services:\n            if service == \"api_gateway\":\n                for endpoint in api_endpoints:\n                    p95_time = random.uniform(0.05, 0.25)  # 50-250ms\n                    sla_metrics[\"sla_response_time_p95\"].labels(\n                        service=service, endpoint=endpoint\n                    ).set(p95_time)\n            else:\n                p95_time = random.uniform(0.03, 0.15)  # 30-150ms\n                sla_metrics[\"sla_response_time_p95\"].labels(\n                    service=service, endpoint=\"internal\"\n                ).set(p95_time)\n        \n        # Error rates (should be < 1%)\n        for service in services:\n            error_rate = random.uniform(0.1, 2.0)\n            sla_metrics[\"sla_error_rate\"].labels(service=service).set(error_rate)\n        \n        # Processing success rates (should be > 99%)\n        operations = {\n            \"ml_inference\": [\"vehicle_detection\", \"speed_calculation\", \"classification\"],\n            \"analytics\": [\"aggregation\", \"anomaly_detection\", \"reporting\"],\n            \"database\": [\"insert\", \"query\", \"update\"],\n            \"api_gateway\": [\"authentication\", \"rate_limiting\", \"routing\"]\n        }\n        \n        for service, ops in operations.items():\n            for operation in ops:\n                success_rate = random.uniform(98.0, 99.9)\n                sla_metrics[\"sla_processing_success_rate\"].labels(\n                    service=service, operation=operation\n                ).set(success_rate)\n        \n        # Define alert rules (these would be in Prometheus/Alertmanager config)\n        alert_rules = {\n            \"HighErrorRate\": {\n                \"condition\": \"sla_error_rate > 1.0\",\n                \"severity\": \"warning\",\n                \"threshold\": 1.0,\n                \"for\": \"5m\"\n            },\n            \"CriticalErrorRate\": {\n                \"condition\": \"sla_error_rate > 5.0\",\n                \"severity\": \"critical\",\n                \"threshold\": 5.0,\n                \"for\": \"1m\"\n            },\n            \"HighResponseTime\": {\n                \"condition\": \"sla_response_time_p95 > 0.2\",\n                \"severity\": \"warning\",\n                \"threshold\": 0.2,\n                \"for\": \"3m\"\n            },\n            \"LowAvailability\": {\n                \"condition\": \"sla_api_availability < 99.0\",\n                \"severity\": \"critical\",\n                \"threshold\": 99.0,\n                \"for\": \"2m\"\n            },\n            \"LowProcessingSuccess\": {\n                \"condition\": \"sla_processing_success_rate < 95.0\",\n                \"severity\": \"warning\",\n                \"threshold\": 95.0,\n                \"for\": \"5m\"\n            }\n        }\n        \n        # Evaluate alert conditions against current metrics\n        active_alerts = []\n        \n        # Check error rates\n        for service in services:\n            current_error_rate = random.uniform(0.1, 2.0)\n            if current_error_rate > alert_rules[\"HighErrorRate\"][\"threshold\"]:\n                active_alerts.append({\n                    \"alert\": \"HighErrorRate\",\n                    \"service\": service,\n                    \"value\": current_error_rate,\n                    \"severity\": \"warning\"\n                })\n            \n            if current_error_rate > alert_rules[\"CriticalErrorRate\"][\"threshold\"]:\n                active_alerts.append({\n                    \"alert\": \"CriticalErrorRate\",\n                    \"service\": service,\n                    \"value\": current_error_rate,\n                    \"severity\": \"critical\"\n                })\n        \n        # Check availability\n        for service in services:\n            current_availability = random.uniform(99.5, 100.0)\n            if current_availability < alert_rules[\"LowAvailability\"][\"threshold\"]:\n                active_alerts.append({\n                    \"alert\": \"LowAvailability\",\n                    \"service\": service,\n                    \"value\": current_availability,\n                    \"severity\": \"critical\"\n                })\n        \n        # Simulate alert notifications\n        notification_channels = [\"email\", \"slack\", \"webhook\", \"pagerduty\"]\n        sent_notifications = []\n        \n        for alert in active_alerts:\n            for channel in notification_channels:\n                if alert[\"severity\"] == \"critical\" or channel in [\"email\", \"slack\"]:\n                    notification = {\n                        \"alert\": alert[\"alert\"],\n                        \"service\": alert[\"service\"],\n                        \"severity\": alert[\"severity\"],\n                        \"channel\": channel,\n                        \"timestamp\": datetime.now(UTC).isoformat(),\n                        \"sent\": True\n                    }\n                    sent_notifications.append(notification)\n                    \n                    # Update metrics\n                    metrics[\"alert_notifications_total\"].labels(\n                        alert_type=alert[\"alert\"],\n                        severity=alert[\"severity\"],\n                        channel=channel\n                    ).inc()\n        \n        # Generate SLA report\n        sla_report = {\n            \"timestamp\": datetime.now(UTC).isoformat(),\n            \"period\": \"last_24h\",\n            \"services\": {},\n            \"overall_sla_status\": \"healthy\",\n            \"active_alerts\": len(active_alerts),\n            \"notifications_sent\": len(sent_notifications)\n        }\n        \n        for service in services:\n            sla_report[\"services\"][service] = {\n                \"availability\": random.uniform(99.5, 100.0),\n                \"error_rate\": random.uniform(0.1, 2.0),\n                \"avg_response_time_ms\": random.uniform(50, 200),\n                \"sla_compliance\": random.choice([True, False])\n            }\n        \n        # Calculate overall SLA compliance\n        compliant_services = sum(\n            1 for service_data in sla_report[\"services\"].values()\n            if service_data[\"sla_compliance\"]\n        )\n        overall_compliance = compliant_services / len(services)\n        sla_report[\"overall_compliance_percentage\"] = overall_compliance * 100\n        \n        if overall_compliance < 0.8:\n            sla_report[\"overall_sla_status\"] = \"degraded\"\n        elif overall_compliance < 0.6:\n            sla_report[\"overall_sla_status\"] = \"critical\"\n        \n        logger.info(\n            f\"Alerting and SLA Monitoring Results:\\n\"\n            f\"  Alert Rules Defined: {len(alert_rules)}\\n\"\n            f\"  Active Alerts: {len(active_alerts)}\\n\"\n            f\"  Notifications Sent: {len(sent_notifications)}\\n\"\n            f\"  SLA Compliance: {overall_compliance:.1%}\\n\"\n            f\"  Overall SLA Status: {sla_report['overall_sla_status']}\\n\"\n            f\"  Services Monitored: {len(services)}\"\n        )\n        \n        # Export SLA metrics\n        sla_metrics_output = generate_latest(registry)\n        sla_metrics_text = sla_metrics_output.decode('utf-8')\n        \n        # Validate SLA metrics\n        assert \"its_sla_api_availability\" in sla_metrics_text\n        assert \"its_sla_response_time_p95_seconds\" in sla_metrics_text\n        assert \"its_sla_error_rate\" in sla_metrics_text\n        assert \"its_sla_processing_success_rate\" in sla_metrics_text\n        \n        return sla_report, active_alerts, alert_rules\n\n    async def test_log_aggregation_and_analysis(\n        self, monitoring_config\n    ):\n        \"\"\"Test log aggregation and analysis capabilities.\"\"\"\n        \n        logger.info(\"Testing log aggregation and analysis\")\n        \n        # Simulate structured logging for different components\n        log_entries = []\n        \n        # Generate logs for different services\n        services = [\"api_gateway\", \"ml_inference\", \"analytics\", \"database\", \"cache\"]\n        log_levels = [\"INFO\", \"WARN\", \"ERROR\", \"DEBUG\"]\n        \n        for _ in range(100):\n            service = random.choice(services)\n            level = random.choice(log_levels)\n            \n            # Create structured log entry\n            log_entry = {\n                \"timestamp\": datetime.now(UTC).isoformat(),\n                \"level\": level,\n                \"service\": service,\n                \"message\": f\"Sample {level.lower()} message from {service}\",\n                \"request_id\": f\"req_{uuid4().hex[:8]}\",\n                \"duration_ms\": random.uniform(10, 500),\n                \"metadata\": {\n                    \"component\": service,\n                    \"version\": \"1.2.0\",\n                    \"environment\": \"test\"\n                }\n            }\n            \n            # Add service-specific fields\n            if service == \"ml_inference\":\n                log_entry[\"metadata\"].update({\n                    \"model_version\": \"yolo11n_v1.2\",\n                    \"inference_time_ms\": random.uniform(30, 150),\n                    \"detections_count\": random.randint(0, 15)\n                })\n            elif service == \"api_gateway\":\n                log_entry[\"metadata\"].update({\n                    \"method\": random.choice([\"GET\", \"POST\", \"PUT\", \"DELETE\"]),\n                    \"endpoint\": random.choice([\"/api/v1/analytics\", \"/api/v1/cameras\", \"/api/v1/incidents\"]),\n                    \"status_code\": random.choice([200, 400, 404, 500]),\n                    \"response_size_bytes\": random.randint(100, 5000)\n                })\n            elif service == \"database\":\n                log_entry[\"metadata\"].update({\n                    \"query_type\": random.choice([\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\"]),\n                    \"table\": random.choice([\"traffic_data\", \"cameras\", \"incidents\", \"users\"]),\n                    \"rows_affected\": random.randint(1, 100)\n                })\n            \n            log_entries.append(log_entry)\n        \n        # Analyze log patterns\n        log_analysis = {\n            \"total_entries\": len(log_entries),\n            \"entries_by_level\": {},\n            \"entries_by_service\": {},\n            \"avg_duration_by_service\": {},\n            \"error_patterns\": [],\n            \"performance_outliers\": []\n        }\n        \n        # Group by log level\n        for level in log_levels:\n            level_entries = [log for log in log_entries if log[\"level\"] == level]\n            log_analysis[\"entries_by_level\"][level] = len(level_entries)\n        \n        # Group by service\n        for service in services:\n            service_entries = [log for log in log_entries if log[\"service\"] == service]\n            log_analysis[\"entries_by_service\"][service] = len(service_entries)\n            \n            # Calculate average duration\n            if service_entries:\n                avg_duration = sum(log[\"duration_ms\"] for log in service_entries) / len(service_entries)\n                log_analysis[\"avg_duration_by_service\"][service] = avg_duration\n        \n        # Find error patterns\n        error_entries = [log for log in log_entries if log[\"level\"] == \"ERROR\"]\n        for error_log in error_entries:\n            pattern = {\n                \"service\": error_log[\"service\"],\n                \"timestamp\": error_log[\"timestamp\"],\n                \"message\": error_log[\"message\"],\n                \"duration_ms\": error_log[\"duration_ms\"]\n            }\n            log_analysis[\"error_patterns\"].append(pattern)\n        \n        # Find performance outliers (duration > 400ms)\n        outliers = [log for log in log_entries if log[\"duration_ms\"] > 400]\n        for outlier in outliers:\n            outlier_info = {\n                \"service\": outlier[\"service\"],\n                \"duration_ms\": outlier[\"duration_ms\"],\n                \"request_id\": outlier[\"request_id\"]\n            }\n            log_analysis[\"performance_outliers\"].append(outlier_info)\n        \n        # Simulate log aggregation queries\n        aggregation_queries = [\n            {\n                \"name\": \"Error rate by service\",\n                \"query\": \"SELECT service, COUNT(*) as error_count FROM logs WHERE level='ERROR' GROUP BY service\",\n                \"results\": log_analysis[\"entries_by_level\"].get(\"ERROR\", 0)\n            },\n            {\n                \"name\": \"Average response time\",\n                \"query\": \"SELECT AVG(duration_ms) FROM logs WHERE service='api_gateway'\",\n                \"results\": log_analysis[\"avg_duration_by_service\"].get(\"api_gateway\", 0)\n            },\n            {\n                \"name\": \"High latency requests\",\n                \"query\": \"SELECT * FROM logs WHERE duration_ms > 400 ORDER BY duration_ms DESC\",\n                \"results\": len(log_analysis[\"performance_outliers\"])\n            }\n        ]\n        \n        # Test log retention and cleanup\n        retention_period_days = monitoring_config.retention_period_days\n        old_cutoff = datetime.now(UTC) - timedelta(days=retention_period_days)\n        \n        logs_to_retain = [\n            log for log in log_entries\n            if datetime.fromisoformat(log[\"timestamp\"]) > old_cutoff\n        ]\n        \n        retention_info = {\n            \"total_logs\": len(log_entries),\n            \"logs_to_retain\": len(logs_to_retain),\n            \"logs_to_cleanup\": len(log_entries) - len(logs_to_retain),\n            \"retention_period_days\": retention_period_days\n        }\n        \n        logger.info(\n            f\"Log Aggregation and Analysis Results:\\n\"\n            f\"  Total Log Entries: {log_analysis['total_entries']}\\n\"\n            f\"  Entries by Level: {log_analysis['entries_by_level']}\\n\"\n            f\"  Entries by Service: {log_analysis['entries_by_service']}\\n\"\n            f\"  Error Patterns Found: {len(log_analysis['error_patterns'])}\\n\"\n            f\"  Performance Outliers: {len(log_analysis['performance_outliers'])}\\n\"\n            f\"  Aggregation Queries: {len(aggregation_queries)}\\n\"\n            f\"  Retention: {retention_info['logs_to_retain']}/{retention_info['total_logs']} logs retained\"\n        )\n        \n        # Validate log analysis\n        assert log_analysis[\"total_entries\"] == 100\n        assert sum(log_analysis[\"entries_by_level\"].values()) == 100\n        assert sum(log_analysis[\"entries_by_service\"].values()) == 100\n        assert all(avg >= 0 for avg in log_analysis[\"avg_duration_by_service\"].values())\n        \n        return log_analysis, aggregation_queries, retention_info