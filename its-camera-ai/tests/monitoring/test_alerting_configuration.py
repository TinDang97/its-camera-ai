"""Alerting Configuration Tests for Production Monitoring.

This test suite validates alerting and notification systems:
- Prometheus alert rules configuration
- Alert manager routing and grouping
- Multi-channel notification delivery
- Alert escalation and de-escalation
- SLA violation alerting
- Custom business alert rules
- Alert fatigue prevention
- On-call rotation management
"""

import asyncio
import json
import random
import time
from datetime import UTC, datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, MagicMock, patch
from uuid import uuid4

import pytest
import pytest_asyncio
import yaml

from its_camera_ai.core.logging import get_logger
from its_camera_ai.services.alert_service import AlertService, AlertRule, AlertSeverity
from its_camera_ai.services.analytics_dtos import CongestionLevel

logger = get_logger(__name__)


@pytest.mark.integration
@pytest.mark.alerting
@pytest.mark.asyncio
class TestAlertingConfiguration:
    \"\"\"Tests for alerting configuration and notification systems.\"\"\"\n\n    @pytest_asyncio.fixture\n    async def alert_service(self):\n        \"\"\"Alert service with mock notification channels.\"\"\"\n        service = AlertService(\n            notification_channels=[\n                \"email\", \"slack\", \"webhook\", \"sms\", \"pagerduty\"\n            ],\n            escalation_enabled=True,\n            alert_grouping_enabled=True,\n            rate_limiting_enabled=True\n        )\n        \n        # Mock notification handlers\n        service._notification_handlers = {\n            \"email\": AsyncMock(),\n            \"slack\": AsyncMock(),\n            \"webhook\": AsyncMock(),\n            \"sms\": AsyncMock(),\n            \"pagerduty\": AsyncMock()\n        }\n        \n        return service\n\n    @pytest_asyncio.fixture\n    async def prometheus_alert_rules(self):\n        \"\"\"Prometheus alert rules configuration.\"\"\"\n        return {\n            \"groups\": [\n                {\n                    \"name\": \"its_camera_ai_critical\",\n                    \"rules\": [\n                        {\n                            \"alert\": \"HighErrorRate\",\n                            \"expr\": \"rate(its_api_requests_total{status_code=~'5..'}[5m]) > 0.05\",\n                            \"for\": \"2m\",\n                            \"labels\": {\n                                \"severity\": \"critical\",\n                                \"service\": \"api\",\n                                \"team\": \"sre\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"High error rate detected\",\n                                \"description\": \"API error rate is {{ $value | humanizePercentage }} for {{ $labels.service }}\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/high-error-rate\"\n                            }\n                        },\n                        {\n                            \"alert\": \"DatabaseConnectionFailure\",\n                            \"expr\": \"its_database_connections{pool_name='primary'} == 0\",\n                            \"for\": \"1m\",\n                            \"labels\": {\n                                \"severity\": \"critical\",\n                                \"service\": \"database\",\n                                \"team\": \"sre\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"Database connection pool exhausted\",\n                                \"description\": \"No active database connections available\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/database-connections\"\n                            }\n                        },\n                        {\n                            \"alert\": \"MLInferenceDown\",\n                            \"expr\": \"up{job='ml-inference'} == 0\",\n                            \"for\": \"30s\",\n                            \"labels\": {\n                                \"severity\": \"critical\",\n                                \"service\": \"ml-inference\",\n                                \"team\": \"ml-ops\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"ML inference service is down\",\n                                \"description\": \"ML inference service has been down for more than 30 seconds\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/ml-service-down\"\n                            }\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"its_camera_ai_warning\",\n                    \"rules\": [\n                        {\n                            \"alert\": \"HighLatency\",\n                            \"expr\": \"histogram_quantile(0.95, its_api_request_duration_seconds_bucket) > 0.5\",\n                            \"for\": \"5m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"api\",\n                                \"team\": \"backend\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"High API latency detected\",\n                                \"description\": \"95th percentile latency is {{ $value }}s\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/high-latency\"\n                            }\n                        },\n                        {\n                            \"alert\": \"CacheHitRateLow\",\n                            \"expr\": \"rate(its_cache_operations_total{status='hit'}[10m]) / rate(its_cache_operations_total[10m]) < 0.8\",\n                            \"for\": \"10m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"cache\",\n                                \"team\": \"backend\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"Low cache hit rate\",\n                                \"description\": \"Cache hit rate is {{ $value | humanizePercentage }}\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/cache-performance\"\n                            }\n                        },\n                        {\n                            \"alert\": \"CameraOffline\",\n                            \"expr\": \"its_camera_uptime_percentage < 95\",\n                            \"for\": \"5m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"camera\",\n                                \"team\": \"operations\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"Camera uptime below threshold\",\n                                \"description\": \"Camera {{ $labels.camera_id }} uptime is {{ $value }}%\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/camera-offline\"\n                            }\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"its_business_alerts\",\n                    \"rules\": [\n                        {\n                            \"alert\": \"TrafficCongestionSevere\",\n                            \"expr\": \"its_congestion_level_current >= 4\",\n                            \"for\": \"15m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"traffic-management\",\n                                \"team\": \"traffic-ops\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"Severe traffic congestion detected\",\n                                \"description\": \"Severe congestion at {{ $labels.camera_id }} for over 15 minutes\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/traffic-congestion\"\n                            }\n                        },\n                        {\n                            \"alert\": \"IncidentDetectionDelay\",\n                            \"expr\": \"histogram_quantile(0.95, its_incident_detection_time_seconds_bucket) > 300\",\n                            \"for\": \"5m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"incident-detection\",\n                                \"team\": \"traffic-ops\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"Slow incident detection\",\n                                \"description\": \"95th percentile incident detection time is {{ $value }}s\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/incident-detection\"\n                            }\n                        },\n                        {\n                            \"alert\": \"MLAccuracyDegradation\",\n                            \"expr\": \"its_vehicle_classification_accuracy < 85\",\n                            \"for\": \"30m\",\n                            \"labels\": {\n                                \"severity\": \"warning\",\n                                \"service\": \"ml-inference\",\n                                \"team\": \"ml-ops\"\n                            },\n                            \"annotations\": {\n                                \"summary\": \"ML model accuracy degradation\",\n                                \"description\": \"Vehicle classification accuracy dropped to {{ $value }}% for {{ $labels.vehicle_type }}\",\n                                \"runbook_url\": \"https://docs.example.com/runbooks/ml-accuracy\"\n                            }\n                        }\n                    ]\n                }\n            ]\n        }\n\n    @pytest_asyncio.fixture\n    async def alertmanager_config(self):\n        \"\"\"Alert manager configuration with routing and receivers.\"\"\"\n        return {\n            \"global\": {\n                \"smtp_smarthost\": \"localhost:587\",\n                \"smtp_from\": \"alerts@its-camera-ai.com\",\n                \"slack_api_url\": \"https://hooks.slack.com/services/test\"\n            },\n            \"route\": {\n                \"group_by\": [\"alertname\"],\n                \"group_wait\": \"10s\",\n                \"group_interval\": \"10s\",\n                \"repeat_interval\": \"1h\",\n                \"receiver\": \"web.hook\",\n                \"routes\": [\n                    {\n                        \"match\": {\"severity\": \"critical\"},\n                        \"receiver\": \"critical-alerts\",\n                        \"group_wait\": \"5s\",\n                        \"repeat_interval\": \"15m\"\n                    },\n                    {\n                        \"match\": {\"team\": \"sre\"},\n                        \"receiver\": \"sre-team\",\n                        \"group_wait\": \"10s\"\n                    },\n                    {\n                        \"match\": {\"team\": \"ml-ops\"},\n                        \"receiver\": \"ml-ops-team\"\n                    },\n                    {\n                        \"match\": {\"service\": \"traffic-management\"},\n                        \"receiver\": \"traffic-ops-team\"\n                    }\n                ]\n            },\n            \"receivers\": [\n                {\n                    \"name\": \"web.hook\",\n                    \"webhook_configs\": [\n                        {\n                            \"url\": \"http://localhost:5001/webhook\",\n                            \"send_resolved\": True\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"critical-alerts\",\n                    \"email_configs\": [\n                        {\n                            \"to\": \"sre-oncall@its-camera-ai.com\",\n                            \"subject\": \"[CRITICAL] ITS Camera AI Alert\",\n                            \"body\": \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ .Annotations.description }}{{ end }}\"\n                        }\n                    ],\n                    \"slack_configs\": [\n                        {\n                            \"channel\": \"#alerts-critical\",\n                            \"title\": \"Critical Alert: {{ .GroupLabels.alertname }}\",\n                            \"text\": \"{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ .Annotations.description }}{{ end }}\"\n                        }\n                    ],\n                    \"pagerduty_configs\": [\n                        {\n                            \"routing_key\": \"pagerduty-integration-key\",\n                            \"description\": \"{{ .GroupLabels.alertname }}: {{ .GroupLabels.instance }}\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"sre-team\",\n                    \"email_configs\": [\n                        {\n                            \"to\": \"sre-team@its-camera-ai.com\",\n                            \"subject\": \"[SRE] ITS Camera AI Alert\"\n                        }\n                    ],\n                    \"slack_configs\": [\n                        {\n                            \"channel\": \"#sre-alerts\",\n                            \"title\": \"SRE Alert: {{ .GroupLabels.alertname }}\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"ml-ops-team\",\n                    \"slack_configs\": [\n                        {\n                            \"channel\": \"#ml-ops-alerts\",\n                            \"title\": \"ML Ops Alert: {{ .GroupLabels.alertname }}\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"traffic-ops-team\",\n                    \"email_configs\": [\n                        {\n                            \"to\": \"traffic-ops@its-camera-ai.com\",\n                            \"subject\": \"[TRAFFIC OPS] {{ .GroupLabels.alertname }}\"\n                        }\n                    ]\n                }\n            ],\n            \"inhibit_rules\": [\n                {\n                    \"source_match\": {\"severity\": \"critical\"},\n                    \"target_match\": {\"severity\": \"warning\"},\n                    \"equal\": [\"alertname\", \"instance\"]\n                }\n            ]\n        }\n\n    async def test_alert_rule_evaluation(\n        self, alert_service, prometheus_alert_rules\n    ):\n        \"\"\"Test alert rule evaluation and triggering.\"\"\"\n        \n        logger.info(\"Testing alert rule evaluation\")\n        \n        # Create test alert rules based on Prometheus configuration\n        test_alert_rules = []\n        \n        for group in prometheus_alert_rules[\"groups\"]:\n            for rule in group[\"rules\"]:\n                alert_rule = AlertRule(\n                    name=rule[\"alert\"],\n                    expression=rule[\"expr\"],\n                    duration=rule[\"for\"],\n                    severity=AlertSeverity(rule[\"labels\"][\"severity\"]),\n                    labels=rule[\"labels\"],\n                    annotations=rule[\"annotations\"]\n                )\n                test_alert_rules.append(alert_rule)\n                await alert_service.create_alert_rule(alert_rule)\n        \n        # Simulate metric values that would trigger alerts\n        test_scenarios = [\n            {\n                \"name\": \"High Error Rate\",\n                \"metrics\": {\n                    \"its_api_requests_total\": {\n                        \"status_code=500\": 50,\n                        \"status_code=200\": 950\n                    }\n                },\n                \"expected_alerts\": [\"HighErrorRate\"]\n            },\n            {\n                \"name\": \"Database Connection Failure\",\n                \"metrics\": {\n                    \"its_database_connections\": {\n                        \"pool_name=primary\": 0\n                    }\n                },\n                \"expected_alerts\": [\"DatabaseConnectionFailure\"]\n            },\n            {\n                \"name\": \"High Latency\",\n                \"metrics\": {\n                    \"its_api_request_duration_seconds\": {\n                        \"quantile=0.95\": 0.8\n                    }\n                },\n                \"expected_alerts\": [\"HighLatency\"]\n            },\n            {\n                \"name\": \"Traffic Congestion\",\n                \"metrics\": {\n                    \"its_congestion_level_current\": {\n                        \"camera_id=highway_001\": 4.5\n                    }\n                },\n                \"expected_alerts\": [\"TrafficCongestionSevere\"]\n            },\n            {\n                \"name\": \"ML Accuracy Degradation\",\n                \"metrics\": {\n                    \"its_vehicle_classification_accuracy\": {\n                        \"vehicle_type=car\": 82.5\n                    }\n                },\n                \"expected_alerts\": [\"MLAccuracyDegradation\"]\n            }\n        ]\n        \n        triggered_alerts = []\n        \n        for scenario in test_scenarios:\n            logger.info(f\"Testing scenario: {scenario['name']}\")\n            \n            # Simulate metric evaluation\n            for alert_rule in test_alert_rules:\n                # Simple rule evaluation (in real implementation, this would use Prometheus)\n                rule_triggered = False\n                \n                if alert_rule.name == \"HighErrorRate\":\n                    error_requests = scenario[\"metrics\"].get(\"its_api_requests_total\", {}).get(\"status_code=500\", 0)\n                    total_requests = sum(scenario[\"metrics\"].get(\"its_api_requests_total\", {}).values())\n                    error_rate = error_requests / total_requests if total_requests > 0 else 0\n                    rule_triggered = error_rate > 0.05\n                \n                elif alert_rule.name == \"DatabaseConnectionFailure\":\n                    connections = scenario[\"metrics\"].get(\"its_database_connections\", {}).get(\"pool_name=primary\", 1)\n                    rule_triggered = connections == 0\n                \n                elif alert_rule.name == \"HighLatency\":\n                    latency = scenario[\"metrics\"].get(\"its_api_request_duration_seconds\", {}).get(\"quantile=0.95\", 0)\n                    rule_triggered = latency > 0.5\n                \n                elif alert_rule.name == \"TrafficCongestionSevere\":\n                    congestion = scenario[\"metrics\"].get(\"its_congestion_level_current\", {}).get(\"camera_id=highway_001\", 0)\n                    rule_triggered = congestion >= 4\n                \n                elif alert_rule.name == \"MLAccuracyDegradation\":\n                    accuracy = scenario[\"metrics\"].get(\"its_vehicle_classification_accuracy\", {}).get(\"vehicle_type=car\", 100)\n                    rule_triggered = accuracy < 85\n                \n                if rule_triggered:\n                    alert = {\n                        \"alert_name\": alert_rule.name,\n                        \"severity\": alert_rule.severity.value,\n                        \"labels\": alert_rule.labels,\n                        \"annotations\": alert_rule.annotations,\n                        \"timestamp\": datetime.now(UTC),\n                        \"scenario\": scenario[\"name\"]\n                    }\n                    triggered_alerts.append(alert)\n                    \n                    # Trigger alert through service\n                    await alert_service.trigger_alert(\n                        alert_rule.name,\n                        alert_rule.labels,\n                        alert_rule.annotations\n                    )\n        \n        # Validate alert triggering\n        evaluation_results = {\n            \"scenarios_tested\": len(test_scenarios),\n            \"alert_rules_created\": len(test_alert_rules),\n            \"alerts_triggered\": len(triggered_alerts),\n            \"alerts_by_severity\": {},\n            \"alerts_by_team\": {}\n        }\n        \n        # Group alerts by severity and team\n        for alert in triggered_alerts:\n            severity = alert[\"severity\"]\n            team = alert[\"labels\"].get(\"team\", \"unknown\")\n            \n            evaluation_results[\"alerts_by_severity\"][severity] = (\n                evaluation_results[\"alerts_by_severity\"].get(severity, 0) + 1\n            )\n            evaluation_results[\"alerts_by_team\"][team] = (\n                evaluation_results[\"alerts_by_team\"].get(team, 0) + 1\n            )\n        \n        logger.info(\n            f\"Alert Rule Evaluation Results:\\n\"\n            f\"  Scenarios Tested: {evaluation_results['scenarios_tested']}\\n\"\n            f\"  Alert Rules Created: {evaluation_results['alert_rules_created']}\\n\"\n            f\"  Alerts Triggered: {evaluation_results['alerts_triggered']}\\n\"\n            f\"  Alerts by Severity: {evaluation_results['alerts_by_severity']}\\n\"\n            f\"  Alerts by Team: {evaluation_results['alerts_by_team']}\"\n        )\n        \n        # Validate alert evaluation\n        assert evaluation_results[\"alert_rules_created\"] >= 6  # At least 6 rules from config\n        assert evaluation_results[\"alerts_triggered\"] >= 3   # At least some alerts should trigger\n        assert \"critical\" in evaluation_results[\"alerts_by_severity\"]\n        assert \"warning\" in evaluation_results[\"alerts_by_severity\"]\n        \n        return triggered_alerts, evaluation_results\n\n    async def test_multi_channel_notification_delivery(\n        self, alert_service, alertmanager_config\n    ):\n        \"\"\"Test multi-channel notification delivery.\"\"\"\n        \n        logger.info(\"Testing multi-channel notification delivery\")\n        \n        # Test notification channels\n        notification_channels = [\"email\", \"slack\", \"webhook\", \"sms\", \"pagerduty\"]\n        \n        # Create test alerts with different severities and teams\n        test_alerts = [\n            {\n                \"name\": \"CriticalDatabaseDown\",\n                \"severity\": \"critical\",\n                \"team\": \"sre\",\n                \"service\": \"database\",\n                \"message\": \"Database connection pool exhausted\",\n                \"channels\": [\"email\", \"slack\", \"pagerduty\"]  # Critical alerts go to multiple channels\n            },\n            {\n                \"name\": \"HighAPILatency\",\n                \"severity\": \"warning\",\n                \"team\": \"backend\",\n                \"service\": \"api\",\n                \"message\": \"API response time above threshold\",\n                \"channels\": [\"slack\", \"email\"]  # Warnings go to fewer channels\n            },\n            {\n                \"name\": \"MLModelAccuracyDrop\",\n                \"severity\": \"warning\",\n                \"team\": \"ml-ops\",\n                \"service\": \"ml-inference\",\n                \"message\": \"Vehicle classification accuracy below 85%\",\n                \"channels\": [\"slack\"]\n            },\n            {\n                \"name\": \"TrafficCongestionAlert\",\n                \"severity\": \"info\",\n                \"team\": \"traffic-ops\",\n                \"service\": \"traffic-management\",\n                \"message\": \"Severe congestion detected at intersection\",\n                \"channels\": [\"email\", \"webhook\"]\n            }\n        ]\n        \n        # Track notification attempts\n        notification_attempts = []\n        delivery_results = {\n            \"total_alerts\": len(test_alerts),\n            \"total_notifications\": 0,\n            \"successful_deliveries\": 0,\n            \"failed_deliveries\": 0,\n            \"deliveries_by_channel\": {channel: 0 for channel in notification_channels},\n            \"deliveries_by_severity\": {},\n            \"delivery_times\": []\n        }\n        \n        for alert in test_alerts:\n            logger.info(f\"Processing alert: {alert['name']}\")\n            \n            for channel in alert[\"channels\"]:\n                delivery_start = time.time()\n                \n                try:\n                    # Simulate notification delivery\n                    success = await alert_service.send_notification(\n                        channel=channel,\n                        alert_name=alert[\"name\"],\n                        severity=alert[\"severity\"],\n                        team=alert[\"team\"],\n                        message=alert[\"message\"],\n                        labels={\"service\": alert[\"service\"], \"team\": alert[\"team\"]}\n                    )\n                    \n                    delivery_time = time.time() - delivery_start\n                    \n                    notification_attempt = {\n                        \"alert_name\": alert[\"name\"],\n                        \"channel\": channel,\n                        \"severity\": alert[\"severity\"],\n                        \"team\": alert[\"team\"],\n                        \"success\": success,\n                        \"delivery_time\": delivery_time,\n                        \"timestamp\": datetime.now(UTC)\n                    }\n                    \n                    notification_attempts.append(notification_attempt)\n                    delivery_results[\"total_notifications\"] += 1\n                    delivery_results[\"delivery_times\"].append(delivery_time)\n                    \n                    if success:\n                        delivery_results[\"successful_deliveries\"] += 1\n                        delivery_results[\"deliveries_by_channel\"][channel] += 1\n                        \n                        severity = alert[\"severity\"]\n                        delivery_results[\"deliveries_by_severity\"][severity] = (\n                            delivery_results[\"deliveries_by_severity\"].get(severity, 0) + 1\n                        )\n                    else:\n                        delivery_results[\"failed_deliveries\"] += 1\n                    \n                    # Verify notification handler was called\n                    assert alert_service._notification_handlers[channel].send_notification.called\n                    \n                except Exception as e:\n                    logger.error(f\"Notification delivery failed: {e}\")\n                    delivery_results[\"failed_deliveries\"] += 1\n        \n        # Calculate delivery metrics\n        if delivery_results[\"delivery_times\"]:\n            avg_delivery_time = sum(delivery_results[\"delivery_times\"]) / len(delivery_results[\"delivery_times\"])\n            max_delivery_time = max(delivery_results[\"delivery_times\"])\n        else:\n            avg_delivery_time = 0\n            max_delivery_time = 0\n        \n        delivery_success_rate = (\n            delivery_results[\"successful_deliveries\"] / delivery_results[\"total_notifications\"]\n            if delivery_results[\"total_notifications\"] > 0 else 0\n        )\n        \n        # Test channel-specific formatting\n        channel_formatting_tests = {\n            \"email\": {\n                \"subject_template\": \"[{severity}] ITS Camera AI Alert: {alert_name}\",\n                \"body_template\": \"Alert: {alert_name}\\nSeverity: {severity}\\nTeam: {team}\\nMessage: {message}\"\n            },\n            \"slack\": {\n                \"format\": \"blocks\",\n                \"color_mapping\": {\"critical\": \"danger\", \"warning\": \"warning\", \"info\": \"good\"}\n            },\n            \"webhook\": {\n                \"format\": \"json\",\n                \"fields\": [\"alert_name\", \"severity\", \"team\", \"timestamp\", \"labels\"]\n            }\n        }\n        \n        formatting_test_results = {}\n        for channel, config in channel_formatting_tests.items():\n            if channel in alert_service._notification_handlers:\n                # Test formatting (mock implementation)\n                test_alert = test_alerts[0]\n                formatted_message = await alert_service.format_notification(\n                    channel, test_alert\n                )\n                formatting_test_results[channel] = {\n                    \"formatted\": formatted_message is not None,\n                    \"config_applied\": True\n                }\n        \n        logger.info(\n            f\"Multi-Channel Notification Results:\\n\"\n            f\"  Total Alerts: {delivery_results['total_alerts']}\\n\"\n            f\"  Total Notifications: {delivery_results['total_notifications']}\\n\"\n            f\"  Successful Deliveries: {delivery_results['successful_deliveries']}\\n\"\n            f\"  Failed Deliveries: {delivery_results['failed_deliveries']}\\n\"\n            f\"  Delivery Success Rate: {delivery_success_rate:.1%}\\n\"\n            f\"  Avg Delivery Time: {avg_delivery_time:.3f}s\\n\"\n            f\"  Max Delivery Time: {max_delivery_time:.3f}s\\n\"\n            f\"  Deliveries by Channel: {delivery_results['deliveries_by_channel']}\\n\"\n            f\"  Formatting Tests: {len(formatting_test_results)} channels tested\"\n        )\n        \n        # Validate notification delivery\n        assert delivery_success_rate >= 0.8  # At least 80% success rate\n        assert avg_delivery_time <= 5.0      # Average delivery under 5 seconds\n        assert max_delivery_time <= 10.0     # Max delivery under 10 seconds\n        assert delivery_results[\"deliveries_by_channel\"][\"email\"] > 0  # Email notifications sent\n        assert delivery_results[\"deliveries_by_channel\"][\"slack\"] > 0  # Slack notifications sent\n        \n        return notification_attempts, delivery_results, formatting_test_results\n\n    async def test_alert_escalation_and_deescalation(\n        self, alert_service\n    ):\n        \"\"\"Test alert escalation and de-escalation workflows.\"\"\"\n        \n        logger.info(\"Testing alert escalation and de-escalation\")\n        \n        # Define escalation policies\n        escalation_policies = {\n            \"critical\": {\n                \"initial_channels\": [\"slack\", \"email\"],\n                \"escalation_delay\": 300,  # 5 minutes\n                \"escalation_levels\": [\n                    {\"level\": 1, \"channels\": [\"sms\"], \"delay\": 300},\n                    {\"level\": 2, \"channels\": [\"pagerduty\"], \"delay\": 600},\n                    {\"level\": 3, \"channels\": [\"phone_call\"], \"delay\": 900}\n                ]\n            },\n            \"warning\": {\n                \"initial_channels\": [\"slack\"],\n                \"escalation_delay\": 1800,  # 30 minutes\n                \"escalation_levels\": [\n                    {\"level\": 1, \"channels\": [\"email\"], \"delay\": 1800}\n                ]\n            },\n            \"info\": {\n                \"initial_channels\": [\"webhook\"],\n                \"escalation_delay\": None,  # No escalation for info alerts\n                \"escalation_levels\": []\n            }\n        }\n        \n        # Configure escalation policies in alert service\n        await alert_service.configure_escalation_policies(escalation_policies)\n        \n        # Test escalation scenarios\n        escalation_test_cases = [\n            {\n                \"name\": \"Critical Alert with Full Escalation\",\n                \"alert\": {\n                    \"name\": \"DatabaseConnectionFailure\",\n                    \"severity\": \"critical\",\n                    \"team\": \"sre\",\n                    \"service\": \"database\"\n                },\n                \"acknowledgment_delay\": 1200,  # 20 minutes (triggers escalation)\n                \"expected_escalation_levels\": 2\n            },\n            {\n                \"name\": \"Critical Alert with Quick Acknowledgment\",\n                \"alert\": {\n                    \"name\": \"HighErrorRate\",\n                    \"severity\": \"critical\",\n                    \"team\": \"sre\",\n                    \"service\": \"api\"\n                },\n                \"acknowledgment_delay\": 120,  # 2 minutes (no escalation)\n                \"expected_escalation_levels\": 0\n            },\n            {\n                \"name\": \"Warning Alert with Escalation\",\n                \"alert\": {\n                    \"name\": \"HighLatency\",\n                    \"severity\": \"warning\",\n                    \"team\": \"backend\",\n                    \"service\": \"api\"\n                },\n                \"acknowledgment_delay\": 2400,  # 40 minutes (triggers escalation)\n                \"expected_escalation_levels\": 1\n            }\n        ]\n        \n        escalation_results = []\n        \n        for test_case in escalation_test_cases:\n            logger.info(f\"Testing: {test_case['name']}\")\n            \n            alert = test_case[\"alert\"]\n            \n            # Trigger initial alert\n            alert_id = await alert_service.trigger_alert(\n                alert[\"name\"],\n                {\"team\": alert[\"team\"], \"service\": alert[\"service\"]},\n                {\"summary\": f\"{alert['name']} triggered\", \"description\": \"Test alert\"}\n            )\n            \n            escalation_timeline = [{\n                \"timestamp\": datetime.now(UTC),\n                \"action\": \"alert_triggered\",\n                \"level\": 0,\n                \"channels\": escalation_policies[alert[\"severity\"]][\"initial_channels\"]\n            }]\n            \n            # Simulate escalation process\n            current_time = 0\n            escalation_level = 0\n            acknowledged = False\n            \n            while current_time < test_case[\"acknowledgment_delay\"] and not acknowledged:\n                # Check for escalation\n                policy = escalation_policies[alert[\"severity\"]]\n                \n                if policy[\"escalation_delay\"] and current_time >= policy[\"escalation_delay\"] * (escalation_level + 1):\n                    if escalation_level < len(policy[\"escalation_levels\"]):\n                        escalation_config = policy[\"escalation_levels\"][escalation_level]\n                        \n                        # Trigger escalation\n                        await alert_service.escalate_alert(\n                            alert_id,\n                            escalation_config[\"level\"],\n                            escalation_config[\"channels\"]\n                        )\n                        \n                        escalation_timeline.append({\n                            \"timestamp\": datetime.now(UTC),\n                            \"action\": \"escalated\",\n                            \"level\": escalation_config[\"level\"],\n                            \"channels\": escalation_config[\"channels\"]\n                        })\n                        \n                        escalation_level += 1\n                \n                current_time += 60  # Advance by 1 minute\n            \n            # Simulate acknowledgment or resolution\n            if current_time >= test_case[\"acknowledgment_delay\"]:\n                await alert_service.acknowledge_alert(alert_id, \"operator\", \"Issue acknowledged\")\n                escalation_timeline.append({\n                    \"timestamp\": datetime.now(UTC),\n                    \"action\": \"acknowledged\",\n                    \"level\": escalation_level,\n                    \"operator\": \"operator\"\n                })\n            \n            # Test de-escalation (alert resolution)\n            resolution_delay = random.randint(300, 1800)  # 5-30 minutes\n            await asyncio.sleep(0.1)  # Brief pause for simulation\n            \n            await alert_service.resolve_alert(alert_id, \"system\", \"Issue resolved automatically\")\n            escalation_timeline.append({\n                \"timestamp\": datetime.now(UTC),\n                \"action\": \"resolved\",\n                \"level\": 0,\n                \"resolver\": \"system\"\n            })\n            \n            # Verify escalation behavior\n            actual_escalation_levels = len([\n                event for event in escalation_timeline if event[\"action\"] == \"escalated\"\n            ])\n            \n            escalation_result = {\n                \"test_case\": test_case[\"name\"],\n                \"alert_id\": alert_id,\n                \"expected_escalations\": test_case[\"expected_escalation_levels\"],\n                \"actual_escalations\": actual_escalation_levels,\n                \"timeline\": escalation_timeline,\n                \"escalation_correct\": actual_escalation_levels == test_case[\"expected_escalation_levels\"]\n            }\n            \n            escalation_results.append(escalation_result)\n        \n        # Analyze escalation performance\n        escalation_analysis = {\n            \"total_tests\": len(escalation_test_cases),\n            \"correct_escalations\": sum(1 for r in escalation_results if r[\"escalation_correct\"]),\n            \"escalation_accuracy\": 0,\n            \"avg_escalation_time\": 0,\n            \"policies_tested\": len(escalation_policies)\n        }\n        \n        escalation_analysis[\"escalation_accuracy\"] = (\n            escalation_analysis[\"correct_escalations\"] / escalation_analysis[\"total_tests\"]\n        )\n        \n        # Calculate average escalation time\n        escalation_times = []\n        for result in escalation_results:\n            escalated_events = [e for e in result[\"timeline\"] if e[\"action\"] == \"escalated\"]\n            if escalated_events:\n                first_escalation = escalated_events[0]\n                alert_start = result[\"timeline\"][0]\n                escalation_time = (first_escalation[\"timestamp\"] - alert_start[\"timestamp\"]).total_seconds()\n                escalation_times.append(escalation_time)\n        \n        if escalation_times:\n            escalation_analysis[\"avg_escalation_time\"] = sum(escalation_times) / len(escalation_times)\n        \n        logger.info(\n            f\"Alert Escalation and De-escalation Results:\\n\"\n            f\"  Total Tests: {escalation_analysis['total_tests']}\\n\"\n            f\"  Correct Escalations: {escalation_analysis['correct_escalations']}\\n\"\n            f\"  Escalation Accuracy: {escalation_analysis['escalation_accuracy']:.1%}\\n\"\n            f\"  Avg Escalation Time: {escalation_analysis['avg_escalation_time']:.1f}s\\n\"\n            f\"  Policies Tested: {escalation_analysis['policies_tested']}\"\n        )\n        \n        # Validate escalation behavior\n        assert escalation_analysis[\"escalation_accuracy\"] >= 0.8  # At least 80% accuracy\n        assert escalation_analysis[\"avg_escalation_time\"] <= 900   # Avg escalation within 15 minutes\n        \n        return escalation_results, escalation_analysis\n\n    async def test_alert_fatigue_prevention(\n        self, alert_service\n    ):\n        \"\"\"Test alert fatigue prevention mechanisms.\"\"\"\n        \n        logger.info(\"Testing alert fatigue prevention\")\n        \n        # Configure rate limiting and grouping\n        fatigue_prevention_config = {\n            \"rate_limiting\": {\n                \"enabled\": True,\n                \"max_alerts_per_hour\": 50,\n                \"max_alerts_per_day\": 200,\n                \"burst_threshold\": 10\n            },\n            \"alert_grouping\": {\n                \"enabled\": True,\n                \"group_by\": [\"alertname\", \"service\"],\n                \"group_interval\": 300,  # 5 minutes\n                \"max_group_size\": 20\n            },\n            \"silence_rules\": {\n                \"enabled\": True,\n                \"auto_silence_duration\": 3600,  # 1 hour\n                \"recurring_alert_threshold\": 5\n            },\n            \"notification_throttling\": {\n                \"enabled\": True,\n                \"repeat_interval\": 1800,  # 30 minutes\n                \"max_repeats\": 5\n            }\n        }\n        \n        await alert_service.configure_fatigue_prevention(fatigue_prevention_config)\n        \n        # Simulate alert storm scenarios\n        storm_scenarios = [\n            {\n                \"name\": \"Cascading Service Failures\",\n                \"alerts\": [\n                    {\"name\": \"DatabaseConnectionFailure\", \"service\": \"database\", \"count\": 25},\n                    {\"name\": \"APIGatewayDown\", \"service\": \"api\", \"count\": 30},\n                    {\"name\": \"CacheServiceDown\", \"service\": \"cache\", \"count\": 15}\n                ],\n                \"time_window\": 600  # 10 minutes\n            },\n            {\n                \"name\": \"ML Model Performance Degradation\",\n                \"alerts\": [\n                    {\"name\": \"MLAccuracyDrop\", \"service\": \"ml-inference\", \"count\": 40},\n                    {\"name\": \"HighInferenceLatency\", \"service\": \"ml-inference\", \"count\": 35}\n                ],\n                \"time_window\": 900  # 15 minutes\n            },\n            {\n                \"name\": \"Traffic System Overload\",\n                \"alerts\": [\n                    {\"name\": \"HighTrafficVolume\", \"service\": \"traffic-processing\", \"count\": 60},\n                    {\"name\": \"CongestionAlerts\", \"service\": \"traffic-management\", \"count\": 45}\n                ],\n                \"time_window\": 1200  # 20 minutes\n            }\n        ]\n        \n        prevention_results = []\n        \n        for scenario in storm_scenarios:\n            logger.info(f\"Testing scenario: {scenario['name']}\")\n            \n            scenario_start = time.time()\n            scenario_alerts = []\n            notifications_sent = 0\n            notifications_suppressed = 0\n            groups_created = 0\n            \n            # Generate alert storm\n            for alert_config in scenario[\"alerts\"]:\n                for i in range(alert_config[\"count\"]):\n                    alert_id = f\"{alert_config['name']}_{i:03d}\"\n                    \n                    # Attempt to send alert\n                    try:\n                        result = await alert_service.process_alert_with_prevention(\n                            alert_name=alert_config[\"name\"],\n                            service=alert_config[\"service\"],\n                            severity=\"warning\",\n                            labels={\"service\": alert_config[\"service\"]},\n                            annotations={\"summary\": f\"Alert {i} for {alert_config['name']}\"},\n                            alert_id=alert_id\n                        )\n                        \n                        if result[\"action\"] == \"sent\":\n                            notifications_sent += 1\n                        elif result[\"action\"] == \"suppressed\":\n                            notifications_suppressed += 1\n                        elif result[\"action\"] == \"grouped\":\n                            groups_created += 1\n                        \n                        scenario_alerts.append({\n                            \"alert_id\": alert_id,\n                            \"alert_name\": alert_config[\"name\"],\n                            \"service\": alert_config[\"service\"],\n                            \"action\": result[\"action\"],\n                            \"timestamp\": datetime.now(UTC)\n                        })\n                        \n                    except Exception as e:\n                        logger.error(f\"Error processing alert {alert_id}: {e}\")\n                    \n                    # Small delay to simulate real-time alerts\n                    await asyncio.sleep(0.01)\n            \n            scenario_duration = time.time() - scenario_start\n            total_alerts = sum(alert[\"count\"] for alert in scenario[\"alerts\"])\n            \n            # Calculate prevention effectiveness\n            suppression_rate = notifications_suppressed / total_alerts if total_alerts > 0 else 0\n            grouping_effectiveness = groups_created / total_alerts if total_alerts > 0 else 0\n            alert_rate = total_alerts / (scenario_duration / 60)  # alerts per minute\n            \n            scenario_result = {\n                \"scenario\": scenario[\"name\"],\n                \"total_alerts\": total_alerts,\n                \"notifications_sent\": notifications_sent,\n                \"notifications_suppressed\": notifications_suppressed,\n                \"groups_created\": groups_created,\n                \"suppression_rate\": suppression_rate,\n                \"grouping_effectiveness\": grouping_effectiveness,\n                \"alert_rate_per_minute\": alert_rate,\n                \"scenario_duration\": scenario_duration,\n                \"prevention_effective\": suppression_rate > 0.5 or grouping_effectiveness > 0.3\n            }\n            \n            prevention_results.append(scenario_result)\n        \n        # Test silence rule effectiveness\n        silence_test_results = await self._test_silence_rules(alert_service)\n        \n        # Analyze overall fatigue prevention\n        fatigue_analysis = {\n            \"scenarios_tested\": len(storm_scenarios),\n            \"effective_prevention_scenarios\": sum(1 for r in prevention_results if r[\"prevention_effective\"]),\n            \"avg_suppression_rate\": sum(r[\"suppression_rate\"] for r in prevention_results) / len(prevention_results),\n            \"avg_grouping_effectiveness\": sum(r[\"grouping_effectiveness\"] for r in prevention_results) / len(prevention_results),\n            \"max_alert_rate\": max(r[\"alert_rate_per_minute\"] for r in prevention_results),\n            \"silence_rules_effective\": silence_test_results[\"effective\"],\n            \"prevention_success_rate\": 0\n        }\n        \n        fatigue_analysis[\"prevention_success_rate\"] = (\n            fatigue_analysis[\"effective_prevention_scenarios\"] / fatigue_analysis[\"scenarios_tested\"]\n        )\n        \n        logger.info(\n            f\"Alert Fatigue Prevention Results:\\n\"\n            f\"  Scenarios Tested: {fatigue_analysis['scenarios_tested']}\\n\"\n            f\"  Effective Prevention: {fatigue_analysis['effective_prevention_scenarios']}\\n\"\n            f\"  Prevention Success Rate: {fatigue_analysis['prevention_success_rate']:.1%}\\n\"\n            f\"  Avg Suppression Rate: {fatigue_analysis['avg_suppression_rate']:.1%}\\n\"\n            f\"  Avg Grouping Effectiveness: {fatigue_analysis['avg_grouping_effectiveness']:.1%}\\n\"\n            f\"  Max Alert Rate: {fatigue_analysis['max_alert_rate']:.1f} alerts/min\\n\"\n            f\"  Silence Rules Effective: {fatigue_analysis['silence_rules_effective']}\"\n        )\n        \n        # Validate fatigue prevention\n        assert fatigue_analysis[\"prevention_success_rate\"] >= 0.6  # At least 60% effective\n        assert fatigue_analysis[\"avg_suppression_rate\"] >= 0.3     # At least 30% suppression\n        assert fatigue_analysis[\"silence_rules_effective\"]         # Silence rules working\n        \n        return prevention_results, fatigue_analysis, silence_test_results\n\n    async def _test_silence_rules(self, alert_service) -> Dict[str, Any]:\n        \"\"\"Test automatic silence rule creation and management.\"\"\"\n        \n        # Create recurring alert pattern\n        recurring_alert_name = \"RecurringTestAlert\"\n        silence_threshold = 5\n        \n        silence_results = {\n            \"alerts_sent\": 0,\n            \"silence_triggered\": False,\n            \"silenced_alerts\": 0,\n            \"effective\": False\n        }\n        \n        # Send alerts that should trigger silence rule\n        for i in range(silence_threshold + 3):  # Exceed threshold\n            result = await alert_service.process_alert_with_prevention(\n                alert_name=recurring_alert_name,\n                service=\"test-service\",\n                severity=\"warning\",\n                labels={\"service\": \"test-service\"},\n                annotations={\"summary\": f\"Recurring alert {i}\"},\n                alert_id=f\"recurring_{i}\"\n            )\n            \n            if result[\"action\"] == \"sent\":\n                silence_results[\"alerts_sent\"] += 1\n            elif result[\"action\"] == \"silenced\":\n                silence_results[\"silenced_alerts\"] += 1\n                if not silence_results[\"silence_triggered\"]:\n                    silence_results[\"silence_triggered\"] = True\n        \n        # Check if silence rule was effective\n        silence_results[\"effective\"] = (\n            silence_results[\"silence_triggered\"] and\n            silence_results[\"silenced_alerts\"] > 0 and\n            silence_results[\"alerts_sent\"] <= silence_threshold\n        )\n        \n        return silence_results"